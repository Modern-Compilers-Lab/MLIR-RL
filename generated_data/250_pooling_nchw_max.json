{"linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x512x256x32xf32>, tensor<1x1xf32>) outs (%init: tensor<16x512x256x32xf32>) -> tensor<16x512x256x32xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x512x256x32xf32>, tensor<1x1xf32>) outs (%init: tensor<16x512x256x32xf32>) -> tensor<16x512x256x32xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<16x512x256x32xf32>, %filter: tensor<1x1xf32>, %init: tensor<16x512x256x32xf32>) -> tensor<16x512x256x32xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x512x256x32xf32>, tensor<1x1xf32>) outs (%init: tensor<16x512x256x32xf32>) -> tensor<16x512x256x32xf32>\n  return %ret : tensor<16x512x256x32xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<16x512x256x32xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<16x512x256x32xf32>) -> tensor<16x512x256x32xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<16x512x256x32xf32>\n    %1 = bufferization.to_memref %arg2 : memref<16x512x256x32xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x512x256x32xf32>\n    memref.copy %1, %alloc : memref<16x512x256x32xf32> to memref<16x512x256x32xf32>\n    affine.for %arg3 = 0 to 16 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 256 {\n          affine.for %arg6 = 0 to 32 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<16x512x256x32xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x512x256x32xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x512x256x32xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<16x512x256x32xf32>\n    return %2 : tensor<16x512x256x32xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x512x256x32xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<16x512x256x32xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<16x512x256x32xf32>) -> tensor<16x512x256x32xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<16x512x256x32xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<16x512x256x32xf32>) -> tensor<16x512x256x32xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x512x256x32xf32>, tensor<1x1xf32>) outs (%init: tensor<16x512x256x32xf32>) -> tensor<16x512x256x32xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x512x256x32xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x512x256x32xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 16, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 256, 1], ["%arg6", 0, 32, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 97800172}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x256x256x32xf32>, tensor<5x5xf32>) outs (%init: tensor<16x256x126x14xf32>) -> tensor<16x256x126x14xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x256x256x32xf32>, tensor<5x5xf32>) outs (%init: tensor<16x256x126x14xf32>) -> tensor<16x256x126x14xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<16x256x256x32xf32>, %filter: tensor<5x5xf32>, %init: tensor<16x256x126x14xf32>) -> tensor<16x256x126x14xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x256x256x32xf32>, tensor<5x5xf32>) outs (%init: tensor<16x256x126x14xf32>) -> tensor<16x256x126x14xf32>\n  return %ret : tensor<16x256x126x14xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<16x256x256x32xf32>, %arg1: tensor<5x5xf32>, %arg2: tensor<16x256x126x14xf32>) -> tensor<16x256x126x14xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<16x256x256x32xf32>\n    %1 = bufferization.to_memref %arg2 : memref<16x256x126x14xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x256x126x14xf32>\n    memref.copy %1, %alloc : memref<16x256x126x14xf32> to memref<16x256x126x14xf32>\n    affine.for %arg3 = 0 to 16 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 126 {\n          affine.for %arg6 = 0 to 14 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<16x256x256x32xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x256x126x14xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x256x126x14xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<16x256x126x14xf32>\n    return %2 : tensor<16x256x126x14xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x256x126x14xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<16x256x256x32xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<16x256x256x32xf32>) -> tensor<16x256x256x32xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<5x5xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<5x5xf32>) -> tensor<5x5xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<16x256x126x14xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<16x256x126x14xf32>) -> tensor<16x256x126x14xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x256x256x32xf32>, tensor<5x5xf32>) outs (%init: tensor<16x256x126x14xf32>) -> tensor<16x256x126x14xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x256x126x14xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x256x126x14xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 16, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 126, 1], ["%arg6", 0, 14, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 607403294}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x8x32x64xf32>, tensor<7x7xf32>) outs (%init: tensor<32x8x26x58xf32>) -> tensor<32x8x26x58xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x8x32x64xf32>, tensor<7x7xf32>) outs (%init: tensor<32x8x26x58xf32>) -> tensor<32x8x26x58xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x8x32x64xf32>, %filter: tensor<7x7xf32>, %init: tensor<32x8x26x58xf32>) -> tensor<32x8x26x58xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x8x32x64xf32>, tensor<7x7xf32>) outs (%init: tensor<32x8x26x58xf32>) -> tensor<32x8x26x58xf32>\n  return %ret : tensor<32x8x26x58xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x8x32x64xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<32x8x26x58xf32>) -> tensor<32x8x26x58xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<32x8x32x64xf32>\n    %1 = bufferization.to_memref %arg2 : memref<32x8x26x58xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x8x26x58xf32>\n    memref.copy %1, %alloc : memref<32x8x26x58xf32> to memref<32x8x26x58xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 8 {\n        affine.for %arg5 = 0 to 26 {\n          affine.for %arg6 = 0 to 58 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<32x8x32x64xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x8x26x58xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x8x26x58xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<32x8x26x58xf32>\n    return %2 : tensor<32x8x26x58xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<32x8x26x58xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<32x8x32x64xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<32x8x32x64xf32>) -> tensor<32x8x32x64xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<32x8x26x58xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<32x8x26x58xf32>) -> tensor<32x8x26x58xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x8x32x64xf32>, tensor<7x7xf32>) outs (%init: tensor<32x8x26x58xf32>) -> tensor<32x8x26x58xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<32x8x26x58xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<32x8x26x58xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 8, 1], ["%arg5", 0, 26, 1], ["%arg6", 0, 58, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 75215548}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x256x64x128xf32>, tensor<5x5xf32>) outs (%init: tensor<16x256x60x124xf32>) -> tensor<16x256x60x124xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x256x64x128xf32>, tensor<5x5xf32>) outs (%init: tensor<16x256x60x124xf32>) -> tensor<16x256x60x124xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<16x256x64x128xf32>, %filter: tensor<5x5xf32>, %init: tensor<16x256x60x124xf32>) -> tensor<16x256x60x124xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x256x64x128xf32>, tensor<5x5xf32>) outs (%init: tensor<16x256x60x124xf32>) -> tensor<16x256x60x124xf32>\n  return %ret : tensor<16x256x60x124xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<16x256x64x128xf32>, %arg1: tensor<5x5xf32>, %arg2: tensor<16x256x60x124xf32>) -> tensor<16x256x60x124xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<16x256x64x128xf32>\n    %1 = bufferization.to_memref %arg2 : memref<16x256x60x124xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x256x60x124xf32>\n    memref.copy %1, %alloc : memref<16x256x60x124xf32> to memref<16x256x60x124xf32>\n    affine.for %arg3 = 0 to 16 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 60 {\n          affine.for %arg6 = 0 to 124 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<16x256x64x128xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x256x60x124xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x256x60x124xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<16x256x60x124xf32>\n    return %2 : tensor<16x256x60x124xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x256x60x124xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<16x256x64x128xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<16x256x64x128xf32>) -> tensor<16x256x64x128xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<5x5xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<5x5xf32>) -> tensor<5x5xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<16x256x60x124xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<16x256x60x124xf32>) -> tensor<16x256x60x124xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x256x64x128xf32>, tensor<5x5xf32>) outs (%init: tensor<16x256x60x124xf32>) -> tensor<16x256x60x124xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x256x60x124xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x256x60x124xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 16, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 60, 1], ["%arg6", 0, 124, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 2523121993}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x1024x32x32xf32>, tensor<7x7xf32>) outs (%init: tensor<32x1024x13x13xf32>) -> tensor<32x1024x13x13xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x1024x32x32xf32>, tensor<7x7xf32>) outs (%init: tensor<32x1024x13x13xf32>) -> tensor<32x1024x13x13xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x1024x32x32xf32>, %filter: tensor<7x7xf32>, %init: tensor<32x1024x13x13xf32>) -> tensor<32x1024x13x13xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x1024x32x32xf32>, tensor<7x7xf32>) outs (%init: tensor<32x1024x13x13xf32>) -> tensor<32x1024x13x13xf32>\n  return %ret : tensor<32x1024x13x13xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x1024x32x32xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<32x1024x13x13xf32>) -> tensor<32x1024x13x13xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<32x1024x32x32xf32>\n    %1 = bufferization.to_memref %arg2 : memref<32x1024x13x13xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x1024x13x13xf32>\n    memref.copy %1, %alloc : memref<32x1024x13x13xf32> to memref<32x1024x13x13xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 13 {\n          affine.for %arg6 = 0 to 13 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<32x1024x32x32xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x1024x13x13xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x1024x13x13xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<32x1024x13x13xf32>\n    return %2 : tensor<32x1024x13x13xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<32x1024x13x13xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<32x1024x32x32xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<32x1024x32x32xf32>) -> tensor<32x1024x32x32xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<32x1024x13x13xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<32x1024x13x13xf32>) -> tensor<32x1024x13x13xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x1024x32x32xf32>, tensor<7x7xf32>) outs (%init: tensor<32x1024x13x13xf32>) -> tensor<32x1024x13x13xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<32x1024x13x13xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<32x1024x13x13xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 13, 1], ["%arg6", 0, 13, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 1086473446}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x256x256x128xf32>, tensor<1x1xf32>) outs (%init: tensor<16x256x128x64xf32>) -> tensor<16x256x128x64xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x256x256x128xf32>, tensor<1x1xf32>) outs (%init: tensor<16x256x128x64xf32>) -> tensor<16x256x128x64xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<16x256x256x128xf32>, %filter: tensor<1x1xf32>, %init: tensor<16x256x128x64xf32>) -> tensor<16x256x128x64xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x256x256x128xf32>, tensor<1x1xf32>) outs (%init: tensor<16x256x128x64xf32>) -> tensor<16x256x128x64xf32>\n  return %ret : tensor<16x256x128x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<16x256x256x128xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<16x256x128x64xf32>) -> tensor<16x256x128x64xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<16x256x256x128xf32>\n    %1 = bufferization.to_memref %arg2 : memref<16x256x128x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x256x128x64xf32>\n    memref.copy %1, %alloc : memref<16x256x128x64xf32> to memref<16x256x128x64xf32>\n    affine.for %arg3 = 0 to 16 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 128 {\n          affine.for %arg6 = 0 to 64 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<16x256x256x128xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x256x128x64xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x256x128x64xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<16x256x128x64xf32>\n    return %2 : tensor<16x256x128x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x256x128x64xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<16x256x256x128xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<16x256x256x128xf32>) -> tensor<16x256x256x128xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<16x256x128x64xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<16x256x128x64xf32>) -> tensor<16x256x128x64xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x256x256x128xf32>, tensor<1x1xf32>) outs (%init: tensor<16x256x128x64xf32>) -> tensor<16x256x128x64xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x256x128x64xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x256x128x64xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 16, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 128, 1], ["%arg6", 0, 64, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 67928670}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x16x512x64xf32>, tensor<5x5xf32>) outs (%init: tensor<4x16x508x60xf32>) -> tensor<4x16x508x60xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x16x512x64xf32>, tensor<5x5xf32>) outs (%init: tensor<4x16x508x60xf32>) -> tensor<4x16x508x60xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<4x16x512x64xf32>, %filter: tensor<5x5xf32>, %init: tensor<4x16x508x60xf32>) -> tensor<4x16x508x60xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x16x512x64xf32>, tensor<5x5xf32>) outs (%init: tensor<4x16x508x60xf32>) -> tensor<4x16x508x60xf32>\n  return %ret : tensor<4x16x508x60xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<4x16x512x64xf32>, %arg1: tensor<5x5xf32>, %arg2: tensor<4x16x508x60xf32>) -> tensor<4x16x508x60xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<4x16x512x64xf32>\n    %1 = bufferization.to_memref %arg2 : memref<4x16x508x60xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<4x16x508x60xf32>\n    memref.copy %1, %alloc : memref<4x16x508x60xf32> to memref<4x16x508x60xf32>\n    affine.for %arg3 = 0 to 4 {\n      affine.for %arg4 = 0 to 16 {\n        affine.for %arg5 = 0 to 508 {\n          affine.for %arg6 = 0 to 60 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<4x16x512x64xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x16x508x60xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x16x508x60xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<4x16x508x60xf32>\n    return %2 : tensor<4x16x508x60xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<4x16x508x60xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<4x16x512x64xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<4x16x512x64xf32>) -> tensor<4x16x512x64xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<5x5xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<5x5xf32>) -> tensor<5x5xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<4x16x508x60xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<4x16x508x60xf32>) -> tensor<4x16x508x60xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x16x512x64xf32>, tensor<5x5xf32>) outs (%init: tensor<4x16x508x60xf32>) -> tensor<4x16x508x60xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<4x16x508x60xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<4x16x508x60xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 4, 1], ["%arg4", 0, 16, 1], ["%arg5", 0, 508, 1], ["%arg6", 0, 60, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 161464713}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x512x64x256xf32>, tensor<5x5xf32>) outs (%init: tensor<32x512x30x126xf32>) -> tensor<32x512x30x126xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x512x64x256xf32>, tensor<5x5xf32>) outs (%init: tensor<32x512x30x126xf32>) -> tensor<32x512x30x126xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x512x64x256xf32>, %filter: tensor<5x5xf32>, %init: tensor<32x512x30x126xf32>) -> tensor<32x512x30x126xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x512x64x256xf32>, tensor<5x5xf32>) outs (%init: tensor<32x512x30x126xf32>) -> tensor<32x512x30x126xf32>\n  return %ret : tensor<32x512x30x126xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x512x64x256xf32>, %arg1: tensor<5x5xf32>, %arg2: tensor<32x512x30x126xf32>) -> tensor<32x512x30x126xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<32x512x64x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<32x512x30x126xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x512x30x126xf32>\n    memref.copy %1, %alloc : memref<32x512x30x126xf32> to memref<32x512x30x126xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 30 {\n          affine.for %arg6 = 0 to 126 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<32x512x64x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x512x30x126xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x512x30x126xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<32x512x30x126xf32>\n    return %2 : tensor<32x512x30x126xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<32x512x30x126xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<32x512x64x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<32x512x64x256xf32>) -> tensor<32x512x64x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<5x5xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<5x5xf32>) -> tensor<5x5xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<32x512x30x126xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<32x512x30x126xf32>) -> tensor<32x512x30x126xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x512x64x256xf32>, tensor<5x5xf32>) outs (%init: tensor<32x512x30x126xf32>) -> tensor<32x512x30x126xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<32x512x30x126xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<32x512x30x126xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 30, 1], ["%arg6", 0, 126, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 5162160136}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x128x512x32xf32>, tensor<3x3xf32>) outs (%init: tensor<4x128x255x15xf32>) -> tensor<4x128x255x15xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x128x512x32xf32>, tensor<3x3xf32>) outs (%init: tensor<4x128x255x15xf32>) -> tensor<4x128x255x15xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<4x128x512x32xf32>, %filter: tensor<3x3xf32>, %init: tensor<4x128x255x15xf32>) -> tensor<4x128x255x15xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x128x512x32xf32>, tensor<3x3xf32>) outs (%init: tensor<4x128x255x15xf32>) -> tensor<4x128x255x15xf32>\n  return %ret : tensor<4x128x255x15xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<4x128x512x32xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<4x128x255x15xf32>) -> tensor<4x128x255x15xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<4x128x512x32xf32>\n    %1 = bufferization.to_memref %arg2 : memref<4x128x255x15xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<4x128x255x15xf32>\n    memref.copy %1, %alloc : memref<4x128x255x15xf32> to memref<4x128x255x15xf32>\n    affine.for %arg3 = 0 to 4 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 255 {\n          affine.for %arg6 = 0 to 15 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<4x128x512x32xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x128x255x15xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x128x255x15xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<4x128x255x15xf32>\n    return %2 : tensor<4x128x255x15xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<4x128x255x15xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<4x128x512x32xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<4x128x512x32xf32>) -> tensor<4x128x512x32xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<4x128x255x15xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<4x128x255x15xf32>) -> tensor<4x128x255x15xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x128x512x32xf32>, tensor<3x3xf32>) outs (%init: tensor<4x128x255x15xf32>) -> tensor<4x128x255x15xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<4x128x255x15xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<4x128x255x15xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 4, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 255, 1], ["%arg6", 0, 15, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 43026638}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x8x256x256xf32>, tensor<1x1xf32>) outs (%init: tensor<64x8x256x256xf32>) -> tensor<64x8x256x256xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x8x256x256xf32>, tensor<1x1xf32>) outs (%init: tensor<64x8x256x256xf32>) -> tensor<64x8x256x256xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x8x256x256xf32>, %filter: tensor<1x1xf32>, %init: tensor<64x8x256x256xf32>) -> tensor<64x8x256x256xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x8x256x256xf32>, tensor<1x1xf32>) outs (%init: tensor<64x8x256x256xf32>) -> tensor<64x8x256x256xf32>\n  return %ret : tensor<64x8x256x256xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x8x256x256xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<64x8x256x256xf32>) -> tensor<64x8x256x256xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<64x8x256x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<64x8x256x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x8x256x256xf32>\n    memref.copy %1, %alloc : memref<64x8x256x256xf32> to memref<64x8x256x256xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 8 {\n        affine.for %arg5 = 0 to 256 {\n          affine.for %arg6 = 0 to 256 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<64x8x256x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x8x256x256xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x8x256x256xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<64x8x256x256xf32>\n    return %2 : tensor<64x8x256x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<64x8x256x256xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<64x8x256x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<64x8x256x256xf32>) -> tensor<64x8x256x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<64x8x256x256xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<64x8x256x256xf32>) -> tensor<64x8x256x256xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x8x256x256xf32>, tensor<1x1xf32>) outs (%init: tensor<64x8x256x256xf32>) -> tensor<64x8x256x256xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<64x8x256x256xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<64x8x256x256xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 8, 1], ["%arg5", 0, 256, 1], ["%arg6", 0, 256, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 48797314}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x32x32x64xf32>, tensor<1x1xf32>) outs (%init: tensor<64x32x32x64xf32>) -> tensor<64x32x32x64xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x32x32x64xf32>, tensor<1x1xf32>) outs (%init: tensor<64x32x32x64xf32>) -> tensor<64x32x32x64xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x32x32x64xf32>, %filter: tensor<1x1xf32>, %init: tensor<64x32x32x64xf32>) -> tensor<64x32x32x64xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x32x32x64xf32>, tensor<1x1xf32>) outs (%init: tensor<64x32x32x64xf32>) -> tensor<64x32x32x64xf32>\n  return %ret : tensor<64x32x32x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x32x32x64xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<64x32x32x64xf32>) -> tensor<64x32x32x64xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<64x32x32x64xf32>\n    %1 = bufferization.to_memref %arg2 : memref<64x32x32x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x32x32x64xf32>\n    memref.copy %1, %alloc : memref<64x32x32x64xf32> to memref<64x32x32x64xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 32 {\n          affine.for %arg6 = 0 to 64 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<64x32x32x64xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x32x32x64xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x32x32x64xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<64x32x32x64xf32>\n    return %2 : tensor<64x32x32x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<64x32x32x64xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<64x32x32x64xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<64x32x32x64xf32>) -> tensor<64x32x32x64xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<64x32x32x64xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<64x32x32x64xf32>) -> tensor<64x32x32x64xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x32x32x64xf32>, tensor<1x1xf32>) outs (%init: tensor<64x32x32x64xf32>) -> tensor<64x32x32x64xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<64x32x32x64xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<64x32x32x64xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 32, 1], ["%arg6", 0, 64, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 6272715}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x1024x128x64xf32>, tensor<3x3xf32>) outs (%init: tensor<4x1024x126x62xf32>) -> tensor<4x1024x126x62xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x1024x128x64xf32>, tensor<3x3xf32>) outs (%init: tensor<4x1024x126x62xf32>) -> tensor<4x1024x126x62xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<4x1024x128x64xf32>, %filter: tensor<3x3xf32>, %init: tensor<4x1024x126x62xf32>) -> tensor<4x1024x126x62xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x1024x128x64xf32>, tensor<3x3xf32>) outs (%init: tensor<4x1024x126x62xf32>) -> tensor<4x1024x126x62xf32>\n  return %ret : tensor<4x1024x126x62xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<4x1024x128x64xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<4x1024x126x62xf32>) -> tensor<4x1024x126x62xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<4x1024x128x64xf32>\n    %1 = bufferization.to_memref %arg2 : memref<4x1024x126x62xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<4x1024x126x62xf32>\n    memref.copy %1, %alloc : memref<4x1024x126x62xf32> to memref<4x1024x126x62xf32>\n    affine.for %arg3 = 0 to 4 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 126 {\n          affine.for %arg6 = 0 to 62 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<4x1024x128x64xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x1024x126x62xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x1024x126x62xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<4x1024x126x62xf32>\n    return %2 : tensor<4x1024x126x62xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<4x1024x126x62xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<4x1024x128x64xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<4x1024x128x64xf32>) -> tensor<4x1024x128x64xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<4x1024x126x62xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<4x1024x126x62xf32>) -> tensor<4x1024x126x62xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x1024x128x64xf32>, tensor<3x3xf32>) outs (%init: tensor<4x1024x126x62xf32>) -> tensor<4x1024x126x62xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<4x1024x126x62xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<4x1024x126x62xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 4, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 126, 1], ["%arg6", 0, 62, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 691786732}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x64x32x128xf32>, tensor<1x1xf32>) outs (%init: tensor<4x64x32x128xf32>) -> tensor<4x64x32x128xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x64x32x128xf32>, tensor<1x1xf32>) outs (%init: tensor<4x64x32x128xf32>) -> tensor<4x64x32x128xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<4x64x32x128xf32>, %filter: tensor<1x1xf32>, %init: tensor<4x64x32x128xf32>) -> tensor<4x64x32x128xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x64x32x128xf32>, tensor<1x1xf32>) outs (%init: tensor<4x64x32x128xf32>) -> tensor<4x64x32x128xf32>\n  return %ret : tensor<4x64x32x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<4x64x32x128xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<4x64x32x128xf32>) -> tensor<4x64x32x128xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<4x64x32x128xf32>\n    %1 = bufferization.to_memref %arg2 : memref<4x64x32x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<4x64x32x128xf32>\n    memref.copy %1, %alloc : memref<4x64x32x128xf32> to memref<4x64x32x128xf32>\n    affine.for %arg3 = 0 to 4 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 32 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<4x64x32x128xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x64x32x128xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x64x32x128xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<4x64x32x128xf32>\n    return %2 : tensor<4x64x32x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<4x64x32x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<4x64x32x128xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<4x64x32x128xf32>) -> tensor<4x64x32x128xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<4x64x32x128xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<4x64x32x128xf32>) -> tensor<4x64x32x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x64x32x128xf32>, tensor<1x1xf32>) outs (%init: tensor<4x64x32x128xf32>) -> tensor<4x64x32x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<4x64x32x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<4x64x32x128xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 4, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 32, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 1432851}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x32x32x256xf32>, tensor<1x1xf32>) outs (%init: tensor<256x32x16x128xf32>) -> tensor<256x32x16x128xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x32x32x256xf32>, tensor<1x1xf32>) outs (%init: tensor<256x32x16x128xf32>) -> tensor<256x32x16x128xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x32x32x256xf32>, %filter: tensor<1x1xf32>, %init: tensor<256x32x16x128xf32>) -> tensor<256x32x16x128xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x32x32x256xf32>, tensor<1x1xf32>) outs (%init: tensor<256x32x16x128xf32>) -> tensor<256x32x16x128xf32>\n  return %ret : tensor<256x32x16x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x32x32x256xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<256x32x16x128xf32>) -> tensor<256x32x16x128xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<256x32x32x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<256x32x16x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x32x16x128xf32>\n    memref.copy %1, %alloc : memref<256x32x16x128xf32> to memref<256x32x16x128xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 16 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<256x32x32x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x32x16x128xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x32x16x128xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<256x32x16x128xf32>\n    return %2 : tensor<256x32x16x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x32x16x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x32x32x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x32x32x256xf32>) -> tensor<256x32x32x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x32x16x128xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x32x16x128xf32>) -> tensor<256x32x16x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x32x32x256xf32>, tensor<1x1xf32>) outs (%init: tensor<256x32x16x128xf32>) -> tensor<256x32x16x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x32x16x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x32x16x128xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 16, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 36486524}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x8x512x32xf32>, tensor<5x5xf32>) outs (%init: tensor<4x8x508x28xf32>) -> tensor<4x8x508x28xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x8x512x32xf32>, tensor<5x5xf32>) outs (%init: tensor<4x8x508x28xf32>) -> tensor<4x8x508x28xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<4x8x512x32xf32>, %filter: tensor<5x5xf32>, %init: tensor<4x8x508x28xf32>) -> tensor<4x8x508x28xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x8x512x32xf32>, tensor<5x5xf32>) outs (%init: tensor<4x8x508x28xf32>) -> tensor<4x8x508x28xf32>\n  return %ret : tensor<4x8x508x28xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<4x8x512x32xf32>, %arg1: tensor<5x5xf32>, %arg2: tensor<4x8x508x28xf32>) -> tensor<4x8x508x28xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<4x8x512x32xf32>\n    %1 = bufferization.to_memref %arg2 : memref<4x8x508x28xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<4x8x508x28xf32>\n    memref.copy %1, %alloc : memref<4x8x508x28xf32> to memref<4x8x508x28xf32>\n    affine.for %arg3 = 0 to 4 {\n      affine.for %arg4 = 0 to 8 {\n        affine.for %arg5 = 0 to 508 {\n          affine.for %arg6 = 0 to 28 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<4x8x512x32xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x8x508x28xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x8x508x28xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<4x8x508x28xf32>\n    return %2 : tensor<4x8x508x28xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<4x8x508x28xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<4x8x512x32xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<4x8x512x32xf32>) -> tensor<4x8x512x32xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<5x5xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<5x5xf32>) -> tensor<5x5xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<4x8x508x28xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<4x8x508x28xf32>) -> tensor<4x8x508x28xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x8x512x32xf32>, tensor<5x5xf32>) outs (%init: tensor<4x8x508x28xf32>) -> tensor<4x8x508x28xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<4x8x508x28xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<4x8x508x28xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 4, 1], ["%arg4", 0, 8, 1], ["%arg5", 0, 508, 1], ["%arg6", 0, 28, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 37687610}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x16x256x512xf32>, tensor<1x1xf32>) outs (%init: tensor<16x16x128x256xf32>) -> tensor<16x16x128x256xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x16x256x512xf32>, tensor<1x1xf32>) outs (%init: tensor<16x16x128x256xf32>) -> tensor<16x16x128x256xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<16x16x256x512xf32>, %filter: tensor<1x1xf32>, %init: tensor<16x16x128x256xf32>) -> tensor<16x16x128x256xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x16x256x512xf32>, tensor<1x1xf32>) outs (%init: tensor<16x16x128x256xf32>) -> tensor<16x16x128x256xf32>\n  return %ret : tensor<16x16x128x256xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<16x16x256x512xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<16x16x128x256xf32>) -> tensor<16x16x128x256xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<16x16x256x512xf32>\n    %1 = bufferization.to_memref %arg2 : memref<16x16x128x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x16x128x256xf32>\n    memref.copy %1, %alloc : memref<16x16x128x256xf32> to memref<16x16x128x256xf32>\n    affine.for %arg3 = 0 to 16 {\n      affine.for %arg4 = 0 to 16 {\n        affine.for %arg5 = 0 to 128 {\n          affine.for %arg6 = 0 to 256 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<16x16x256x512xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x16x128x256xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x16x128x256xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<16x16x128x256xf32>\n    return %2 : tensor<16x16x128x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x16x128x256xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<16x16x256x512xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<16x16x256x512xf32>) -> tensor<16x16x256x512xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<16x16x128x256xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<16x16x128x256xf32>) -> tensor<16x16x128x256xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x16x256x512xf32>, tensor<1x1xf32>) outs (%init: tensor<16x16x128x256xf32>) -> tensor<16x16x128x256xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x16x128x256xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x16x128x256xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 16, 1], ["%arg4", 0, 16, 1], ["%arg5", 0, 128, 1], ["%arg6", 0, 256, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 16062931}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x32x512x32xf32>, tensor<5x5xf32>) outs (%init: tensor<64x32x508x28xf32>) -> tensor<64x32x508x28xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x32x512x32xf32>, tensor<5x5xf32>) outs (%init: tensor<64x32x508x28xf32>) -> tensor<64x32x508x28xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x32x512x32xf32>, %filter: tensor<5x5xf32>, %init: tensor<64x32x508x28xf32>) -> tensor<64x32x508x28xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x32x512x32xf32>, tensor<5x5xf32>) outs (%init: tensor<64x32x508x28xf32>) -> tensor<64x32x508x28xf32>\n  return %ret : tensor<64x32x508x28xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x32x512x32xf32>, %arg1: tensor<5x5xf32>, %arg2: tensor<64x32x508x28xf32>) -> tensor<64x32x508x28xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<64x32x512x32xf32>\n    %1 = bufferization.to_memref %arg2 : memref<64x32x508x28xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x32x508x28xf32>\n    memref.copy %1, %alloc : memref<64x32x508x28xf32> to memref<64x32x508x28xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 508 {\n          affine.for %arg6 = 0 to 28 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<64x32x512x32xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x32x508x28xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x32x508x28xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<64x32x508x28xf32>\n    return %2 : tensor<64x32x508x28xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<64x32x508x28xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<64x32x512x32xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<64x32x512x32xf32>) -> tensor<64x32x512x32xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<5x5xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<5x5xf32>) -> tensor<5x5xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<64x32x508x28xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<64x32x508x28xf32>) -> tensor<64x32x508x28xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x32x512x32xf32>, tensor<5x5xf32>) outs (%init: tensor<64x32x508x28xf32>) -> tensor<64x32x508x28xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<64x32x508x28xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<64x32x508x28xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 508, 1], ["%arg6", 0, 28, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 2419316093}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x64x512x64xf32>, tensor<3x3xf32>) outs (%init: tensor<64x64x510x62xf32>) -> tensor<64x64x510x62xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x64x512x64xf32>, tensor<3x3xf32>) outs (%init: tensor<64x64x510x62xf32>) -> tensor<64x64x510x62xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x64x512x64xf32>, %filter: tensor<3x3xf32>, %init: tensor<64x64x510x62xf32>) -> tensor<64x64x510x62xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x64x512x64xf32>, tensor<3x3xf32>) outs (%init: tensor<64x64x510x62xf32>) -> tensor<64x64x510x62xf32>\n  return %ret : tensor<64x64x510x62xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x64x512x64xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<64x64x510x62xf32>) -> tensor<64x64x510x62xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<64x64x512x64xf32>\n    %1 = bufferization.to_memref %arg2 : memref<64x64x510x62xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x64x510x62xf32>\n    memref.copy %1, %alloc : memref<64x64x510x62xf32> to memref<64x64x510x62xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 510 {\n          affine.for %arg6 = 0 to 62 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<64x64x512x64xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x64x510x62xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x64x510x62xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<64x64x510x62xf32>\n    return %2 : tensor<64x64x510x62xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<64x64x510x62xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<64x64x512x64xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<64x64x512x64xf32>) -> tensor<64x64x512x64xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<64x64x510x62xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<64x64x510x62xf32>) -> tensor<64x64x510x62xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x64x512x64xf32>, tensor<3x3xf32>) outs (%init: tensor<64x64x510x62xf32>) -> tensor<64x64x510x62xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<64x64x510x62xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<64x64x510x62xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 510, 1], ["%arg6", 0, 62, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 2800794459}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x16x256x128xf32>, tensor<1x1xf32>) outs (%init: tensor<16x16x256x128xf32>) -> tensor<16x16x256x128xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x16x256x128xf32>, tensor<1x1xf32>) outs (%init: tensor<16x16x256x128xf32>) -> tensor<16x16x256x128xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<16x16x256x128xf32>, %filter: tensor<1x1xf32>, %init: tensor<16x16x256x128xf32>) -> tensor<16x16x256x128xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x16x256x128xf32>, tensor<1x1xf32>) outs (%init: tensor<16x16x256x128xf32>) -> tensor<16x16x256x128xf32>\n  return %ret : tensor<16x16x256x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<16x16x256x128xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<16x16x256x128xf32>) -> tensor<16x16x256x128xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<16x16x256x128xf32>\n    %1 = bufferization.to_memref %arg2 : memref<16x16x256x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x16x256x128xf32>\n    memref.copy %1, %alloc : memref<16x16x256x128xf32> to memref<16x16x256x128xf32>\n    affine.for %arg3 = 0 to 16 {\n      affine.for %arg4 = 0 to 16 {\n        affine.for %arg5 = 0 to 256 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<16x16x256x128xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x16x256x128xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x16x256x128xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<16x16x256x128xf32>\n    return %2 : tensor<16x16x256x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x16x256x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<16x16x256x128xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<16x16x256x128xf32>) -> tensor<16x16x256x128xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<16x16x256x128xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<16x16x256x128xf32>) -> tensor<16x16x256x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x16x256x128xf32>, tensor<1x1xf32>) outs (%init: tensor<16x16x256x128xf32>) -> tensor<16x16x256x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x16x256x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x16x256x128xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 16, 1], ["%arg4", 0, 16, 1], ["%arg5", 0, 256, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 12598877}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x128x128x128xf32>, tensor<7x7xf32>) outs (%init: tensor<4x128x122x122xf32>) -> tensor<4x128x122x122xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x128x128x128xf32>, tensor<7x7xf32>) outs (%init: tensor<4x128x122x122xf32>) -> tensor<4x128x122x122xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<4x128x128x128xf32>, %filter: tensor<7x7xf32>, %init: tensor<4x128x122x122xf32>) -> tensor<4x128x122x122xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x128x128x128xf32>, tensor<7x7xf32>) outs (%init: tensor<4x128x122x122xf32>) -> tensor<4x128x122x122xf32>\n  return %ret : tensor<4x128x122x122xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<4x128x128x128xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<4x128x122x122xf32>) -> tensor<4x128x122x122xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<4x128x128x128xf32>\n    %1 = bufferization.to_memref %arg2 : memref<4x128x122x122xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<4x128x122x122xf32>\n    memref.copy %1, %alloc : memref<4x128x122x122xf32> to memref<4x128x122x122xf32>\n    affine.for %arg3 = 0 to 4 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 122 {\n          affine.for %arg6 = 0 to 122 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<4x128x128x128xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x128x122x122xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x128x122x122xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<4x128x122x122xf32>\n    return %2 : tensor<4x128x122x122xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<4x128x122x122xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<4x128x128x128xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<4x128x128x128xf32>) -> tensor<4x128x128x128xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<4x128x122x122xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<4x128x122x122xf32>) -> tensor<4x128x122x122xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x128x128x128xf32>, tensor<7x7xf32>) outs (%init: tensor<4x128x122x122xf32>) -> tensor<4x128x122x122xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<4x128x122x122xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<4x128x122x122xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 4, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 122, 1], ["%arg6", 0, 122, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 1486880091}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x32x64x32xf32>, tensor<5x5xf32>) outs (%init: tensor<16x32x30x14xf32>) -> tensor<16x32x30x14xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x32x64x32xf32>, tensor<5x5xf32>) outs (%init: tensor<16x32x30x14xf32>) -> tensor<16x32x30x14xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<16x32x64x32xf32>, %filter: tensor<5x5xf32>, %init: tensor<16x32x30x14xf32>) -> tensor<16x32x30x14xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x32x64x32xf32>, tensor<5x5xf32>) outs (%init: tensor<16x32x30x14xf32>) -> tensor<16x32x30x14xf32>\n  return %ret : tensor<16x32x30x14xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<16x32x64x32xf32>, %arg1: tensor<5x5xf32>, %arg2: tensor<16x32x30x14xf32>) -> tensor<16x32x30x14xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<16x32x64x32xf32>\n    %1 = bufferization.to_memref %arg2 : memref<16x32x30x14xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x32x30x14xf32>\n    memref.copy %1, %alloc : memref<16x32x30x14xf32> to memref<16x32x30x14xf32>\n    affine.for %arg3 = 0 to 16 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 30 {\n          affine.for %arg6 = 0 to 14 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<16x32x64x32xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x32x30x14xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x32x30x14xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<16x32x30x14xf32>\n    return %2 : tensor<16x32x30x14xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x32x30x14xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<16x32x64x32xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<16x32x64x32xf32>) -> tensor<16x32x64x32xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<5x5xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<5x5xf32>) -> tensor<5x5xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<16x32x30x14xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<16x32x30x14xf32>) -> tensor<16x32x30x14xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x32x64x32xf32>, tensor<5x5xf32>) outs (%init: tensor<16x32x30x14xf32>) -> tensor<16x32x30x14xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x32x30x14xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x32x30x14xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 16, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 30, 1], ["%arg6", 0, 14, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 17898122}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x8x64x64xf32>, tensor<1x1xf32>) outs (%init: tensor<8x8x64x64xf32>) -> tensor<8x8x64x64xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x8x64x64xf32>, tensor<1x1xf32>) outs (%init: tensor<8x8x64x64xf32>) -> tensor<8x8x64x64xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<8x8x64x64xf32>, %filter: tensor<1x1xf32>, %init: tensor<8x8x64x64xf32>) -> tensor<8x8x64x64xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x8x64x64xf32>, tensor<1x1xf32>) outs (%init: tensor<8x8x64x64xf32>) -> tensor<8x8x64x64xf32>\n  return %ret : tensor<8x8x64x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<8x8x64x64xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<8x8x64x64xf32>) -> tensor<8x8x64x64xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<8x8x64x64xf32>\n    %1 = bufferization.to_memref %arg2 : memref<8x8x64x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<8x8x64x64xf32>\n    memref.copy %1, %alloc : memref<8x8x64x64xf32> to memref<8x8x64x64xf32>\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 8 {\n        affine.for %arg5 = 0 to 64 {\n          affine.for %arg6 = 0 to 64 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<8x8x64x64xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x8x64x64xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x8x64x64xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<8x8x64x64xf32>\n    return %2 : tensor<8x8x64x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<8x8x64x64xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<8x8x64x64xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<8x8x64x64xf32>) -> tensor<8x8x64x64xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<8x8x64x64xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<8x8x64x64xf32>) -> tensor<8x8x64x64xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x8x64x64xf32>, tensor<1x1xf32>) outs (%init: tensor<8x8x64x64xf32>) -> tensor<8x8x64x64xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<8x8x64x64xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<8x8x64x64xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 8, 1], ["%arg4", 0, 8, 1], ["%arg5", 0, 64, 1], ["%arg6", 0, 64, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 378129}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x16x128x256xf32>, tensor<1x1xf32>) outs (%init: tensor<16x16x128x256xf32>) -> tensor<16x16x128x256xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x16x128x256xf32>, tensor<1x1xf32>) outs (%init: tensor<16x16x128x256xf32>) -> tensor<16x16x128x256xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<16x16x128x256xf32>, %filter: tensor<1x1xf32>, %init: tensor<16x16x128x256xf32>) -> tensor<16x16x128x256xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x16x128x256xf32>, tensor<1x1xf32>) outs (%init: tensor<16x16x128x256xf32>) -> tensor<16x16x128x256xf32>\n  return %ret : tensor<16x16x128x256xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<16x16x128x256xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<16x16x128x256xf32>) -> tensor<16x16x128x256xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<16x16x128x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<16x16x128x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x16x128x256xf32>\n    memref.copy %1, %alloc : memref<16x16x128x256xf32> to memref<16x16x128x256xf32>\n    affine.for %arg3 = 0 to 16 {\n      affine.for %arg4 = 0 to 16 {\n        affine.for %arg5 = 0 to 128 {\n          affine.for %arg6 = 0 to 256 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<16x16x128x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x16x128x256xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x16x128x256xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<16x16x128x256xf32>\n    return %2 : tensor<16x16x128x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x16x128x256xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<16x16x128x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<16x16x128x256xf32>) -> tensor<16x16x128x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<16x16x128x256xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<16x16x128x256xf32>) -> tensor<16x16x128x256xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x16x128x256xf32>, tensor<1x1xf32>) outs (%init: tensor<16x16x128x256xf32>) -> tensor<16x16x128x256xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x16x128x256xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x16x128x256xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 16, 1], ["%arg4", 0, 16, 1], ["%arg5", 0, 128, 1], ["%arg6", 0, 256, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 12271711}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x32x32x32xf32>, tensor<5x5xf32>) outs (%init: tensor<64x32x28x28xf32>) -> tensor<64x32x28x28xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x32x32x32xf32>, tensor<5x5xf32>) outs (%init: tensor<64x32x28x28xf32>) -> tensor<64x32x28x28xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x32x32x32xf32>, %filter: tensor<5x5xf32>, %init: tensor<64x32x28x28xf32>) -> tensor<64x32x28x28xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x32x32x32xf32>, tensor<5x5xf32>) outs (%init: tensor<64x32x28x28xf32>) -> tensor<64x32x28x28xf32>\n  return %ret : tensor<64x32x28x28xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x32x32x32xf32>, %arg1: tensor<5x5xf32>, %arg2: tensor<64x32x28x28xf32>) -> tensor<64x32x28x28xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<64x32x32x32xf32>\n    %1 = bufferization.to_memref %arg2 : memref<64x32x28x28xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x32x28x28xf32>\n    memref.copy %1, %alloc : memref<64x32x28x28xf32> to memref<64x32x28x28xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 28 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<64x32x32x32xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x32x28x28xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x32x28x28xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<64x32x28x28xf32>\n    return %2 : tensor<64x32x28x28xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<64x32x28x28xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<64x32x32x32xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<64x32x32x32xf32>) -> tensor<64x32x32x32xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<5x5xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<5x5xf32>) -> tensor<5x5xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<64x32x28x28xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<64x32x28x28xf32>) -> tensor<64x32x28x28xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x32x32x32xf32>, tensor<5x5xf32>) outs (%init: tensor<64x32x28x28xf32>) -> tensor<64x32x28x28xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<64x32x28x28xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<64x32x28x28xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 28, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 133296016}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x16x64x64xf32>, tensor<3x3xf32>) outs (%init: tensor<64x16x62x62xf32>) -> tensor<64x16x62x62xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x16x64x64xf32>, tensor<3x3xf32>) outs (%init: tensor<64x16x62x62xf32>) -> tensor<64x16x62x62xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x16x64x64xf32>, %filter: tensor<3x3xf32>, %init: tensor<64x16x62x62xf32>) -> tensor<64x16x62x62xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x16x64x64xf32>, tensor<3x3xf32>) outs (%init: tensor<64x16x62x62xf32>) -> tensor<64x16x62x62xf32>\n  return %ret : tensor<64x16x62x62xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x16x64x64xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<64x16x62x62xf32>) -> tensor<64x16x62x62xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<64x16x64x64xf32>\n    %1 = bufferization.to_memref %arg2 : memref<64x16x62x62xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x16x62x62xf32>\n    memref.copy %1, %alloc : memref<64x16x62x62xf32> to memref<64x16x62x62xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 16 {\n        affine.for %arg5 = 0 to 62 {\n          affine.for %arg6 = 0 to 62 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<64x16x64x64xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x16x62x62xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x16x62x62xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<64x16x62x62xf32>\n    return %2 : tensor<64x16x62x62xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<64x16x62x62xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<64x16x64x64xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<64x16x64x64xf32>) -> tensor<64x16x64x64xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<64x16x62x62xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<64x16x62x62xf32>) -> tensor<64x16x62x62xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x16x64x64xf32>, tensor<3x3xf32>) outs (%init: tensor<64x16x62x62xf32>) -> tensor<64x16x62x62xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<64x16x62x62xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<64x16x62x62xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 16, 1], ["%arg5", 0, 62, 1], ["%arg6", 0, 62, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 84228186}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x256x512x64xf32>, tensor<7x7xf32>) outs (%init: tensor<8x256x253x29xf32>) -> tensor<8x256x253x29xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x256x512x64xf32>, tensor<7x7xf32>) outs (%init: tensor<8x256x253x29xf32>) -> tensor<8x256x253x29xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<8x256x512x64xf32>, %filter: tensor<7x7xf32>, %init: tensor<8x256x253x29xf32>) -> tensor<8x256x253x29xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x256x512x64xf32>, tensor<7x7xf32>) outs (%init: tensor<8x256x253x29xf32>) -> tensor<8x256x253x29xf32>\n  return %ret : tensor<8x256x253x29xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<8x256x512x64xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<8x256x253x29xf32>) -> tensor<8x256x253x29xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<8x256x512x64xf32>\n    %1 = bufferization.to_memref %arg2 : memref<8x256x253x29xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<8x256x253x29xf32>\n    memref.copy %1, %alloc : memref<8x256x253x29xf32> to memref<8x256x253x29xf32>\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 253 {\n          affine.for %arg6 = 0 to 29 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<8x256x512x64xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x256x253x29xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x256x253x29xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<8x256x253x29xf32>\n    return %2 : tensor<8x256x253x29xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<8x256x253x29xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<8x256x512x64xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<8x256x512x64xf32>) -> tensor<8x256x512x64xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<8x256x253x29xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<8x256x253x29xf32>) -> tensor<8x256x253x29xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x256x512x64xf32>, tensor<7x7xf32>) outs (%init: tensor<8x256x253x29xf32>) -> tensor<8x256x253x29xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<8x256x253x29xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<8x256x253x29xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 8, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 253, 1], ["%arg6", 0, 29, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 2992464175}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x128x128x128xf32>, tensor<1x1xf32>) outs (%init: tensor<4x128x128x128xf32>) -> tensor<4x128x128x128xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x128x128x128xf32>, tensor<1x1xf32>) outs (%init: tensor<4x128x128x128xf32>) -> tensor<4x128x128x128xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<4x128x128x128xf32>, %filter: tensor<1x1xf32>, %init: tensor<4x128x128x128xf32>) -> tensor<4x128x128x128xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x128x128x128xf32>, tensor<1x1xf32>) outs (%init: tensor<4x128x128x128xf32>) -> tensor<4x128x128x128xf32>\n  return %ret : tensor<4x128x128x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<4x128x128x128xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<4x128x128x128xf32>) -> tensor<4x128x128x128xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<4x128x128x128xf32>\n    %1 = bufferization.to_memref %arg2 : memref<4x128x128x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<4x128x128x128xf32>\n    memref.copy %1, %alloc : memref<4x128x128x128xf32> to memref<4x128x128x128xf32>\n    affine.for %arg3 = 0 to 4 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 128 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<4x128x128x128xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x128x128x128xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x128x128x128xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<4x128x128x128xf32>\n    return %2 : tensor<4x128x128x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<4x128x128x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<4x128x128x128xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<4x128x128x128xf32>) -> tensor<4x128x128x128xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<4x128x128x128xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<4x128x128x128xf32>) -> tensor<4x128x128x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x128x128x128xf32>, tensor<1x1xf32>) outs (%init: tensor<4x128x128x128xf32>) -> tensor<4x128x128x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<4x128x128x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<4x128x128x128xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 4, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 128, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 12556089}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x1024x128x64xf32>, tensor<7x7xf32>) outs (%init: tensor<16x1024x61x29xf32>) -> tensor<16x1024x61x29xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x1024x128x64xf32>, tensor<7x7xf32>) outs (%init: tensor<16x1024x61x29xf32>) -> tensor<16x1024x61x29xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<16x1024x128x64xf32>, %filter: tensor<7x7xf32>, %init: tensor<16x1024x61x29xf32>) -> tensor<16x1024x61x29xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x1024x128x64xf32>, tensor<7x7xf32>) outs (%init: tensor<16x1024x61x29xf32>) -> tensor<16x1024x61x29xf32>\n  return %ret : tensor<16x1024x61x29xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<16x1024x128x64xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<16x1024x61x29xf32>) -> tensor<16x1024x61x29xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<16x1024x128x64xf32>\n    %1 = bufferization.to_memref %arg2 : memref<16x1024x61x29xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x1024x61x29xf32>\n    memref.copy %1, %alloc : memref<16x1024x61x29xf32> to memref<16x1024x61x29xf32>\n    affine.for %arg3 = 0 to 16 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 61 {\n          affine.for %arg6 = 0 to 29 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<16x1024x128x64xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x1024x61x29xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x1024x61x29xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<16x1024x61x29xf32>\n    return %2 : tensor<16x1024x61x29xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x1024x61x29xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<16x1024x128x64xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<16x1024x128x64xf32>) -> tensor<16x1024x128x64xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<16x1024x61x29xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<16x1024x61x29xf32>) -> tensor<16x1024x61x29xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x1024x128x64xf32>, tensor<7x7xf32>) outs (%init: tensor<16x1024x61x29xf32>) -> tensor<16x1024x61x29xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x1024x61x29xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x1024x61x29xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 16, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 61, 1], ["%arg6", 0, 29, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 5773040422}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x256x256x128xf32>, tensor<7x7xf32>) outs (%init: tensor<8x256x125x61xf32>) -> tensor<8x256x125x61xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x256x256x128xf32>, tensor<7x7xf32>) outs (%init: tensor<8x256x125x61xf32>) -> tensor<8x256x125x61xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<8x256x256x128xf32>, %filter: tensor<7x7xf32>, %init: tensor<8x256x125x61xf32>) -> tensor<8x256x125x61xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x256x256x128xf32>, tensor<7x7xf32>) outs (%init: tensor<8x256x125x61xf32>) -> tensor<8x256x125x61xf32>\n  return %ret : tensor<8x256x125x61xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<8x256x256x128xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<8x256x125x61xf32>) -> tensor<8x256x125x61xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<8x256x256x128xf32>\n    %1 = bufferization.to_memref %arg2 : memref<8x256x125x61xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<8x256x125x61xf32>\n    memref.copy %1, %alloc : memref<8x256x125x61xf32> to memref<8x256x125x61xf32>\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 125 {\n          affine.for %arg6 = 0 to 61 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<8x256x256x128xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x256x125x61xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x256x125x61xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<8x256x125x61xf32>\n    return %2 : tensor<8x256x125x61xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<8x256x125x61xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<8x256x256x128xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<8x256x256x128xf32>) -> tensor<8x256x256x128xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<8x256x125x61xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<8x256x125x61xf32>) -> tensor<8x256x125x61xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x256x256x128xf32>, tensor<7x7xf32>) outs (%init: tensor<8x256x125x61xf32>) -> tensor<8x256x125x61xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<8x256x125x61xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<8x256x125x61xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 8, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 125, 1], ["%arg6", 0, 61, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 3047373360}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x512x512x256xf32>, tensor<3x3xf32>) outs (%init: tensor<16x512x255x127xf32>) -> tensor<16x512x255x127xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x512x512x256xf32>, tensor<3x3xf32>) outs (%init: tensor<16x512x255x127xf32>) -> tensor<16x512x255x127xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<16x512x512x256xf32>, %filter: tensor<3x3xf32>, %init: tensor<16x512x255x127xf32>) -> tensor<16x512x255x127xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x512x512x256xf32>, tensor<3x3xf32>) outs (%init: tensor<16x512x255x127xf32>) -> tensor<16x512x255x127xf32>\n  return %ret : tensor<16x512x255x127xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<16x512x512x256xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<16x512x255x127xf32>) -> tensor<16x512x255x127xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<16x512x512x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<16x512x255x127xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x512x255x127xf32>\n    memref.copy %1, %alloc : memref<16x512x255x127xf32> to memref<16x512x255x127xf32>\n    affine.for %arg3 = 0 to 16 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 255 {\n          affine.for %arg6 = 0 to 127 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<16x512x512x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x512x255x127xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x512x255x127xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<16x512x255x127xf32>\n    return %2 : tensor<16x512x255x127xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x512x255x127xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<16x512x512x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<16x512x512x256xf32>) -> tensor<16x512x512x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<16x512x255x127xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<16x512x255x127xf32>) -> tensor<16x512x255x127xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x512x512x256xf32>, tensor<3x3xf32>) outs (%init: tensor<16x512x255x127xf32>) -> tensor<16x512x255x127xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x512x255x127xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x512x255x127xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 16, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 255, 1], ["%arg6", 0, 127, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 5749336352}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x64x32x32xf32>, tensor<3x3xf32>) outs (%init: tensor<4x64x30x30xf32>) -> tensor<4x64x30x30xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x64x32x32xf32>, tensor<3x3xf32>) outs (%init: tensor<4x64x30x30xf32>) -> tensor<4x64x30x30xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<4x64x32x32xf32>, %filter: tensor<3x3xf32>, %init: tensor<4x64x30x30xf32>) -> tensor<4x64x30x30xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x64x32x32xf32>, tensor<3x3xf32>) outs (%init: tensor<4x64x30x30xf32>) -> tensor<4x64x30x30xf32>\n  return %ret : tensor<4x64x30x30xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<4x64x32x32xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<4x64x30x30xf32>) -> tensor<4x64x30x30xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<4x64x32x32xf32>\n    %1 = bufferization.to_memref %arg2 : memref<4x64x30x30xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<4x64x30x30xf32>\n    memref.copy %1, %alloc : memref<4x64x30x30xf32> to memref<4x64x30x30xf32>\n    affine.for %arg3 = 0 to 4 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 30 {\n          affine.for %arg6 = 0 to 30 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<4x64x32x32xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x64x30x30xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x64x30x30xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<4x64x30x30xf32>\n    return %2 : tensor<4x64x30x30xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<4x64x30x30xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<4x64x32x32xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<4x64x32x32xf32>) -> tensor<4x64x32x32xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<4x64x30x30xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<4x64x30x30xf32>) -> tensor<4x64x30x30xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x64x32x32xf32>, tensor<3x3xf32>) outs (%init: tensor<4x64x30x30xf32>) -> tensor<4x64x30x30xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<4x64x30x30xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<4x64x30x30xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 4, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 30, 1], ["%arg6", 0, 30, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 4955585}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x64x32x32xf32>, tensor<1x1xf32>) outs (%init: tensor<256x64x16x16xf32>) -> tensor<256x64x16x16xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x64x32x32xf32>, tensor<1x1xf32>) outs (%init: tensor<256x64x16x16xf32>) -> tensor<256x64x16x16xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x64x32x32xf32>, %filter: tensor<1x1xf32>, %init: tensor<256x64x16x16xf32>) -> tensor<256x64x16x16xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x64x32x32xf32>, tensor<1x1xf32>) outs (%init: tensor<256x64x16x16xf32>) -> tensor<256x64x16x16xf32>\n  return %ret : tensor<256x64x16x16xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x64x32x32xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<256x64x16x16xf32>) -> tensor<256x64x16x16xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<256x64x32x32xf32>\n    %1 = bufferization.to_memref %arg2 : memref<256x64x16x16xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x64x16x16xf32>\n    memref.copy %1, %alloc : memref<256x64x16x16xf32> to memref<256x64x16x16xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 16 {\n          affine.for %arg6 = 0 to 16 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<256x64x32x32xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x64x16x16xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x64x16x16xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<256x64x16x16xf32>\n    return %2 : tensor<256x64x16x16xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x64x16x16xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x64x32x32xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x64x32x32xf32>) -> tensor<256x64x32x32xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x64x16x16xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x64x16x16xf32>) -> tensor<256x64x16x16xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x64x32x32xf32>, tensor<1x1xf32>) outs (%init: tensor<256x64x16x16xf32>) -> tensor<256x64x16x16xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x64x16x16xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x64x16x16xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 16, 1], ["%arg6", 0, 16, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 9604866}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x16x256x64xf32>, tensor<5x5xf32>) outs (%init: tensor<256x16x126x30xf32>) -> tensor<256x16x126x30xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x16x256x64xf32>, tensor<5x5xf32>) outs (%init: tensor<256x16x126x30xf32>) -> tensor<256x16x126x30xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x16x256x64xf32>, %filter: tensor<5x5xf32>, %init: tensor<256x16x126x30xf32>) -> tensor<256x16x126x30xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x16x256x64xf32>, tensor<5x5xf32>) outs (%init: tensor<256x16x126x30xf32>) -> tensor<256x16x126x30xf32>\n  return %ret : tensor<256x16x126x30xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x16x256x64xf32>, %arg1: tensor<5x5xf32>, %arg2: tensor<256x16x126x30xf32>) -> tensor<256x16x126x30xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<256x16x256x64xf32>\n    %1 = bufferization.to_memref %arg2 : memref<256x16x126x30xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x16x126x30xf32>\n    memref.copy %1, %alloc : memref<256x16x126x30xf32> to memref<256x16x126x30xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 16 {\n        affine.for %arg5 = 0 to 126 {\n          affine.for %arg6 = 0 to 30 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<256x16x256x64xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x16x126x30xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x16x126x30xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<256x16x126x30xf32>\n    return %2 : tensor<256x16x126x30xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x16x126x30xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x16x256x64xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x16x256x64xf32>) -> tensor<256x16x256x64xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<5x5xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<5x5xf32>) -> tensor<5x5xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x16x126x30xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x16x126x30xf32>) -> tensor<256x16x126x30xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x16x256x64xf32>, tensor<5x5xf32>) outs (%init: tensor<256x16x126x30xf32>) -> tensor<256x16x126x30xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x16x126x30xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x16x126x30xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 16, 1], ["%arg5", 0, 126, 1], ["%arg6", 0, 30, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 1294474198}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x512x32x32xf32>, tensor<7x7xf32>) outs (%init: tensor<8x512x13x13xf32>) -> tensor<8x512x13x13xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x512x32x32xf32>, tensor<7x7xf32>) outs (%init: tensor<8x512x13x13xf32>) -> tensor<8x512x13x13xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<8x512x32x32xf32>, %filter: tensor<7x7xf32>, %init: tensor<8x512x13x13xf32>) -> tensor<8x512x13x13xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x512x32x32xf32>, tensor<7x7xf32>) outs (%init: tensor<8x512x13x13xf32>) -> tensor<8x512x13x13xf32>\n  return %ret : tensor<8x512x13x13xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<8x512x32x32xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<8x512x13x13xf32>) -> tensor<8x512x13x13xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<8x512x32x32xf32>\n    %1 = bufferization.to_memref %arg2 : memref<8x512x13x13xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<8x512x13x13xf32>\n    memref.copy %1, %alloc : memref<8x512x13x13xf32> to memref<8x512x13x13xf32>\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 13 {\n          affine.for %arg6 = 0 to 13 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<8x512x32x32xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x512x13x13xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x512x13x13xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<8x512x13x13xf32>\n    return %2 : tensor<8x512x13x13xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<8x512x13x13xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<8x512x32x32xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<8x512x32x32xf32>) -> tensor<8x512x32x32xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<8x512x13x13xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<8x512x13x13xf32>) -> tensor<8x512x13x13xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x512x32x32xf32>, tensor<7x7xf32>) outs (%init: tensor<8x512x13x13xf32>) -> tensor<8x512x13x13xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<8x512x13x13xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<8x512x13x13xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 8, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 13, 1], ["%arg6", 0, 13, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 135380268}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x1024x32x32xf32>, tensor<1x1xf32>) outs (%init: tensor<64x1024x32x32xf32>) -> tensor<64x1024x32x32xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x1024x32x32xf32>, tensor<1x1xf32>) outs (%init: tensor<64x1024x32x32xf32>) -> tensor<64x1024x32x32xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x1024x32x32xf32>, %filter: tensor<1x1xf32>, %init: tensor<64x1024x32x32xf32>) -> tensor<64x1024x32x32xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x1024x32x32xf32>, tensor<1x1xf32>) outs (%init: tensor<64x1024x32x32xf32>) -> tensor<64x1024x32x32xf32>\n  return %ret : tensor<64x1024x32x32xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x1024x32x32xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<64x1024x32x32xf32>) -> tensor<64x1024x32x32xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<64x1024x32x32xf32>\n    %1 = bufferization.to_memref %arg2 : memref<64x1024x32x32xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x1024x32x32xf32>\n    memref.copy %1, %alloc : memref<64x1024x32x32xf32> to memref<64x1024x32x32xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 32 {\n          affine.for %arg6 = 0 to 32 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<64x1024x32x32xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x1024x32x32xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x1024x32x32xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<64x1024x32x32xf32>\n    return %2 : tensor<64x1024x32x32xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<64x1024x32x32xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<64x1024x32x32xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<64x1024x32x32xf32>) -> tensor<64x1024x32x32xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<64x1024x32x32xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<64x1024x32x32xf32>) -> tensor<64x1024x32x32xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x1024x32x32xf32>, tensor<1x1xf32>) outs (%init: tensor<64x1024x32x32xf32>) -> tensor<64x1024x32x32xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<64x1024x32x32xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<64x1024x32x32xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 32, 1], ["%arg6", 0, 32, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 98307184}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x256x64x256xf32>, tensor<3x3xf32>) outs (%init: tensor<4x256x62x254xf32>) -> tensor<4x256x62x254xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x256x64x256xf32>, tensor<3x3xf32>) outs (%init: tensor<4x256x62x254xf32>) -> tensor<4x256x62x254xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<4x256x64x256xf32>, %filter: tensor<3x3xf32>, %init: tensor<4x256x62x254xf32>) -> tensor<4x256x62x254xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x256x64x256xf32>, tensor<3x3xf32>) outs (%init: tensor<4x256x62x254xf32>) -> tensor<4x256x62x254xf32>\n  return %ret : tensor<4x256x62x254xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<4x256x64x256xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<4x256x62x254xf32>) -> tensor<4x256x62x254xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<4x256x64x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<4x256x62x254xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<4x256x62x254xf32>\n    memref.copy %1, %alloc : memref<4x256x62x254xf32> to memref<4x256x62x254xf32>\n    affine.for %arg3 = 0 to 4 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 62 {\n          affine.for %arg6 = 0 to 254 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<4x256x64x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x256x62x254xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x256x62x254xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<4x256x62x254xf32>\n    return %2 : tensor<4x256x62x254xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<4x256x62x254xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<4x256x64x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<4x256x64x256xf32>) -> tensor<4x256x64x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<4x256x62x254xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<4x256x62x254xf32>) -> tensor<4x256x62x254xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x256x64x256xf32>, tensor<3x3xf32>) outs (%init: tensor<4x256x62x254xf32>) -> tensor<4x256x62x254xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<4x256x62x254xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<4x256x62x254xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 4, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 62, 1], ["%arg6", 0, 254, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 346328275}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x256x64x256xf32>, tensor<3x3xf32>) outs (%init: tensor<16x256x62x254xf32>) -> tensor<16x256x62x254xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x256x64x256xf32>, tensor<3x3xf32>) outs (%init: tensor<16x256x62x254xf32>) -> tensor<16x256x62x254xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<16x256x64x256xf32>, %filter: tensor<3x3xf32>, %init: tensor<16x256x62x254xf32>) -> tensor<16x256x62x254xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x256x64x256xf32>, tensor<3x3xf32>) outs (%init: tensor<16x256x62x254xf32>) -> tensor<16x256x62x254xf32>\n  return %ret : tensor<16x256x62x254xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<16x256x64x256xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<16x256x62x254xf32>) -> tensor<16x256x62x254xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<16x256x64x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<16x256x62x254xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x256x62x254xf32>\n    memref.copy %1, %alloc : memref<16x256x62x254xf32> to memref<16x256x62x254xf32>\n    affine.for %arg3 = 0 to 16 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 62 {\n          affine.for %arg6 = 0 to 254 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<16x256x64x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x256x62x254xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x256x62x254xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<16x256x62x254xf32>\n    return %2 : tensor<16x256x62x254xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x256x62x254xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<16x256x64x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<16x256x64x256xf32>) -> tensor<16x256x64x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<16x256x62x254xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<16x256x62x254xf32>) -> tensor<16x256x62x254xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x256x64x256xf32>, tensor<3x3xf32>) outs (%init: tensor<16x256x62x254xf32>) -> tensor<16x256x62x254xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x256x62x254xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x256x62x254xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 16, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 62, 1], ["%arg6", 0, 254, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 1387006563}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x64x256x32xf32>, tensor<3x3xf32>) outs (%init: tensor<64x64x254x30xf32>) -> tensor<64x64x254x30xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x64x256x32xf32>, tensor<3x3xf32>) outs (%init: tensor<64x64x254x30xf32>) -> tensor<64x64x254x30xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x64x256x32xf32>, %filter: tensor<3x3xf32>, %init: tensor<64x64x254x30xf32>) -> tensor<64x64x254x30xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x64x256x32xf32>, tensor<3x3xf32>) outs (%init: tensor<64x64x254x30xf32>) -> tensor<64x64x254x30xf32>\n  return %ret : tensor<64x64x254x30xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x64x256x32xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<64x64x254x30xf32>) -> tensor<64x64x254x30xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<64x64x256x32xf32>\n    %1 = bufferization.to_memref %arg2 : memref<64x64x254x30xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x64x254x30xf32>\n    memref.copy %1, %alloc : memref<64x64x254x30xf32> to memref<64x64x254x30xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 254 {\n          affine.for %arg6 = 0 to 30 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<64x64x256x32xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x64x254x30xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x64x254x30xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<64x64x254x30xf32>\n    return %2 : tensor<64x64x254x30xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<64x64x254x30xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<64x64x256x32xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<64x64x256x32xf32>) -> tensor<64x64x256x32xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<64x64x254x30xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<64x64x254x30xf32>) -> tensor<64x64x254x30xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x64x256x32xf32>, tensor<3x3xf32>) outs (%init: tensor<64x64x254x30xf32>) -> tensor<64x64x254x30xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<64x64x254x30xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<64x64x254x30xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 254, 1], ["%arg6", 0, 30, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 680254346}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x16x32x256xf32>, tensor<5x5xf32>) outs (%init: tensor<4x16x14x126xf32>) -> tensor<4x16x14x126xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x16x32x256xf32>, tensor<5x5xf32>) outs (%init: tensor<4x16x14x126xf32>) -> tensor<4x16x14x126xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<4x16x32x256xf32>, %filter: tensor<5x5xf32>, %init: tensor<4x16x14x126xf32>) -> tensor<4x16x14x126xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x16x32x256xf32>, tensor<5x5xf32>) outs (%init: tensor<4x16x14x126xf32>) -> tensor<4x16x14x126xf32>\n  return %ret : tensor<4x16x14x126xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<4x16x32x256xf32>, %arg1: tensor<5x5xf32>, %arg2: tensor<4x16x14x126xf32>) -> tensor<4x16x14x126xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<4x16x32x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<4x16x14x126xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<4x16x14x126xf32>\n    memref.copy %1, %alloc : memref<4x16x14x126xf32> to memref<4x16x14x126xf32>\n    affine.for %arg3 = 0 to 4 {\n      affine.for %arg4 = 0 to 16 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 126 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<4x16x32x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x16x14x126xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x16x14x126xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<4x16x14x126xf32>\n    return %2 : tensor<4x16x14x126xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<4x16x14x126xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<4x16x32x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<4x16x32x256xf32>) -> tensor<4x16x32x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<5x5xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<5x5xf32>) -> tensor<5x5xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<4x16x14x126xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<4x16x14x126xf32>) -> tensor<4x16x14x126xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x16x32x256xf32>, tensor<5x5xf32>) outs (%init: tensor<4x16x14x126xf32>) -> tensor<4x16x14x126xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<4x16x14x126xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<4x16x14x126xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 4, 1], ["%arg4", 0, 16, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 126, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 9336876}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x256x512x64xf32>, tensor<3x3xf32>) outs (%init: tensor<8x256x255x31xf32>) -> tensor<8x256x255x31xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x256x512x64xf32>, tensor<3x3xf32>) outs (%init: tensor<8x256x255x31xf32>) -> tensor<8x256x255x31xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<8x256x512x64xf32>, %filter: tensor<3x3xf32>, %init: tensor<8x256x255x31xf32>) -> tensor<8x256x255x31xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x256x512x64xf32>, tensor<3x3xf32>) outs (%init: tensor<8x256x255x31xf32>) -> tensor<8x256x255x31xf32>\n  return %ret : tensor<8x256x255x31xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<8x256x512x64xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<8x256x255x31xf32>) -> tensor<8x256x255x31xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<8x256x512x64xf32>\n    %1 = bufferization.to_memref %arg2 : memref<8x256x255x31xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<8x256x255x31xf32>\n    memref.copy %1, %alloc : memref<8x256x255x31xf32> to memref<8x256x255x31xf32>\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 255 {\n          affine.for %arg6 = 0 to 31 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<8x256x512x64xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x256x255x31xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x256x255x31xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<8x256x255x31xf32>\n    return %2 : tensor<8x256x255x31xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<8x256x255x31xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<8x256x512x64xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<8x256x512x64xf32>) -> tensor<8x256x512x64xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<8x256x255x31xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<8x256x255x31xf32>) -> tensor<8x256x255x31xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x256x512x64xf32>, tensor<3x3xf32>) outs (%init: tensor<8x256x255x31xf32>) -> tensor<8x256x255x31xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<8x256x255x31xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<8x256x255x31xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 8, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 255, 1], ["%arg6", 0, 31, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 351488847}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x64x32x512xf32>, tensor<5x5xf32>) outs (%init: tensor<16x64x14x254xf32>) -> tensor<16x64x14x254xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x64x32x512xf32>, tensor<5x5xf32>) outs (%init: tensor<16x64x14x254xf32>) -> tensor<16x64x14x254xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<16x64x32x512xf32>, %filter: tensor<5x5xf32>, %init: tensor<16x64x14x254xf32>) -> tensor<16x64x14x254xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x64x32x512xf32>, tensor<5x5xf32>) outs (%init: tensor<16x64x14x254xf32>) -> tensor<16x64x14x254xf32>\n  return %ret : tensor<16x64x14x254xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<16x64x32x512xf32>, %arg1: tensor<5x5xf32>, %arg2: tensor<16x64x14x254xf32>) -> tensor<16x64x14x254xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<16x64x32x512xf32>\n    %1 = bufferization.to_memref %arg2 : memref<16x64x14x254xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x64x14x254xf32>\n    memref.copy %1, %alloc : memref<16x64x14x254xf32> to memref<16x64x14x254xf32>\n    affine.for %arg3 = 0 to 16 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 254 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<16x64x32x512xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x64x14x254xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x64x14x254xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<16x64x14x254xf32>\n    return %2 : tensor<16x64x14x254xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x64x14x254xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<16x64x32x512xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<16x64x32x512xf32>) -> tensor<16x64x32x512xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<5x5xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<5x5xf32>) -> tensor<5x5xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<16x64x14x254xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<16x64x14x254xf32>) -> tensor<16x64x14x254xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x64x32x512xf32>, tensor<5x5xf32>) outs (%init: tensor<16x64x14x254xf32>) -> tensor<16x64x14x254xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x64x14x254xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x64x14x254xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 16, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 254, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 303088026}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x16x64x64xf32>, tensor<7x7xf32>) outs (%init: tensor<4x16x58x58xf32>) -> tensor<4x16x58x58xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x16x64x64xf32>, tensor<7x7xf32>) outs (%init: tensor<4x16x58x58xf32>) -> tensor<4x16x58x58xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<4x16x64x64xf32>, %filter: tensor<7x7xf32>, %init: tensor<4x16x58x58xf32>) -> tensor<4x16x58x58xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x16x64x64xf32>, tensor<7x7xf32>) outs (%init: tensor<4x16x58x58xf32>) -> tensor<4x16x58x58xf32>\n  return %ret : tensor<4x16x58x58xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<4x16x64x64xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<4x16x58x58xf32>) -> tensor<4x16x58x58xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<4x16x64x64xf32>\n    %1 = bufferization.to_memref %arg2 : memref<4x16x58x58xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<4x16x58x58xf32>\n    memref.copy %1, %alloc : memref<4x16x58x58xf32> to memref<4x16x58x58xf32>\n    affine.for %arg3 = 0 to 4 {\n      affine.for %arg4 = 0 to 16 {\n        affine.for %arg5 = 0 to 58 {\n          affine.for %arg6 = 0 to 58 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<4x16x64x64xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x16x58x58xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x16x58x58xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<4x16x58x58xf32>\n    return %2 : tensor<4x16x58x58xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<4x16x58x58xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<4x16x64x64xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<4x16x64x64xf32>) -> tensor<4x16x64x64xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<4x16x58x58xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<4x16x58x58xf32>) -> tensor<4x16x58x58xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x16x64x64xf32>, tensor<7x7xf32>) outs (%init: tensor<4x16x58x58xf32>) -> tensor<4x16x58x58xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<4x16x58x58xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<4x16x58x58xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 4, 1], ["%arg4", 0, 16, 1], ["%arg5", 0, 58, 1], ["%arg6", 0, 58, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 41954144}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x32x512x256xf32>, tensor<5x5xf32>) outs (%init: tensor<4x32x508x252xf32>) -> tensor<4x32x508x252xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x32x512x256xf32>, tensor<5x5xf32>) outs (%init: tensor<4x32x508x252xf32>) -> tensor<4x32x508x252xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<4x32x512x256xf32>, %filter: tensor<5x5xf32>, %init: tensor<4x32x508x252xf32>) -> tensor<4x32x508x252xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x32x512x256xf32>, tensor<5x5xf32>) outs (%init: tensor<4x32x508x252xf32>) -> tensor<4x32x508x252xf32>\n  return %ret : tensor<4x32x508x252xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<4x32x512x256xf32>, %arg1: tensor<5x5xf32>, %arg2: tensor<4x32x508x252xf32>) -> tensor<4x32x508x252xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<4x32x512x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<4x32x508x252xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<4x32x508x252xf32>\n    memref.copy %1, %alloc : memref<4x32x508x252xf32> to memref<4x32x508x252xf32>\n    affine.for %arg3 = 0 to 4 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 508 {\n          affine.for %arg6 = 0 to 252 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<4x32x512x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x32x508x252xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x32x508x252xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<4x32x508x252xf32>\n    return %2 : tensor<4x32x508x252xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<4x32x508x252xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<4x32x512x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<4x32x512x256xf32>) -> tensor<4x32x512x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<5x5xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<5x5xf32>) -> tensor<5x5xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<4x32x508x252xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<4x32x508x252xf32>) -> tensor<4x32x508x252xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x32x512x256xf32>, tensor<5x5xf32>) outs (%init: tensor<4x32x508x252xf32>) -> tensor<4x32x508x252xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<4x32x508x252xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<4x32x508x252xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 4, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 508, 1], ["%arg6", 0, 252, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 1356257050}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x512x128x64xf32>, tensor<1x1xf32>) outs (%init: tensor<32x512x128x64xf32>) -> tensor<32x512x128x64xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x512x128x64xf32>, tensor<1x1xf32>) outs (%init: tensor<32x512x128x64xf32>) -> tensor<32x512x128x64xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x512x128x64xf32>, %filter: tensor<1x1xf32>, %init: tensor<32x512x128x64xf32>) -> tensor<32x512x128x64xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x512x128x64xf32>, tensor<1x1xf32>) outs (%init: tensor<32x512x128x64xf32>) -> tensor<32x512x128x64xf32>\n  return %ret : tensor<32x512x128x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x512x128x64xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<32x512x128x64xf32>) -> tensor<32x512x128x64xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<32x512x128x64xf32>\n    %1 = bufferization.to_memref %arg2 : memref<32x512x128x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x512x128x64xf32>\n    memref.copy %1, %alloc : memref<32x512x128x64xf32> to memref<32x512x128x64xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 128 {\n          affine.for %arg6 = 0 to 64 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<32x512x128x64xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x512x128x64xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x512x128x64xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<32x512x128x64xf32>\n    return %2 : tensor<32x512x128x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<32x512x128x64xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<32x512x128x64xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<32x512x128x64xf32>) -> tensor<32x512x128x64xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<32x512x128x64xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<32x512x128x64xf32>) -> tensor<32x512x128x64xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x512x128x64xf32>, tensor<1x1xf32>) outs (%init: tensor<32x512x128x64xf32>) -> tensor<32x512x128x64xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<32x512x128x64xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<32x512x128x64xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 128, 1], ["%arg6", 0, 64, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 209023745}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x64x64x512xf32>, tensor<5x5xf32>) outs (%init: tensor<16x64x60x508xf32>) -> tensor<16x64x60x508xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x64x64x512xf32>, tensor<5x5xf32>) outs (%init: tensor<16x64x60x508xf32>) -> tensor<16x64x60x508xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<16x64x64x512xf32>, %filter: tensor<5x5xf32>, %init: tensor<16x64x60x508xf32>) -> tensor<16x64x60x508xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x64x64x512xf32>, tensor<5x5xf32>) outs (%init: tensor<16x64x60x508xf32>) -> tensor<16x64x60x508xf32>\n  return %ret : tensor<16x64x60x508xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<16x64x64x512xf32>, %arg1: tensor<5x5xf32>, %arg2: tensor<16x64x60x508xf32>) -> tensor<16x64x60x508xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<16x64x64x512xf32>\n    %1 = bufferization.to_memref %arg2 : memref<16x64x60x508xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x64x60x508xf32>\n    memref.copy %1, %alloc : memref<16x64x60x508xf32> to memref<16x64x60x508xf32>\n    affine.for %arg3 = 0 to 16 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 60 {\n          affine.for %arg6 = 0 to 508 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<16x64x64x512xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x64x60x508xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x64x60x508xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<16x64x60x508xf32>\n    return %2 : tensor<16x64x60x508xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x64x60x508xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<16x64x64x512xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<16x64x64x512xf32>) -> tensor<16x64x64x512xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<5x5xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<5x5xf32>) -> tensor<5x5xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<16x64x60x508xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<16x64x60x508xf32>) -> tensor<16x64x60x508xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x64x64x512xf32>, tensor<5x5xf32>) outs (%init: tensor<16x64x60x508xf32>) -> tensor<16x64x60x508xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x64x60x508xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x64x60x508xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 16, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 60, 1], ["%arg6", 0, 508, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 2581833864}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x512x64x512xf32>, tensor<1x1xf32>) outs (%init: tensor<16x512x32x256xf32>) -> tensor<16x512x32x256xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x512x64x512xf32>, tensor<1x1xf32>) outs (%init: tensor<16x512x32x256xf32>) -> tensor<16x512x32x256xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<16x512x64x512xf32>, %filter: tensor<1x1xf32>, %init: tensor<16x512x32x256xf32>) -> tensor<16x512x32x256xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x512x64x512xf32>, tensor<1x1xf32>) outs (%init: tensor<16x512x32x256xf32>) -> tensor<16x512x32x256xf32>\n  return %ret : tensor<16x512x32x256xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<16x512x64x512xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<16x512x32x256xf32>) -> tensor<16x512x32x256xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<16x512x64x512xf32>\n    %1 = bufferization.to_memref %arg2 : memref<16x512x32x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x512x32x256xf32>\n    memref.copy %1, %alloc : memref<16x512x32x256xf32> to memref<16x512x32x256xf32>\n    affine.for %arg3 = 0 to 16 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 32 {\n          affine.for %arg6 = 0 to 256 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<16x512x64x512xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x512x32x256xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x512x32x256xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<16x512x32x256xf32>\n    return %2 : tensor<16x512x32x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x512x32x256xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<16x512x64x512xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<16x512x64x512xf32>) -> tensor<16x512x64x512xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<16x512x32x256xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<16x512x32x256xf32>) -> tensor<16x512x32x256xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x512x64x512xf32>, tensor<1x1xf32>) outs (%init: tensor<16x512x32x256xf32>) -> tensor<16x512x32x256xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x512x32x256xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x512x32x256xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 16, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 32, 1], ["%arg6", 0, 256, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 127702162}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x32x512x128xf32>, tensor<7x7xf32>) outs (%init: tensor<32x32x506x122xf32>) -> tensor<32x32x506x122xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x32x512x128xf32>, tensor<7x7xf32>) outs (%init: tensor<32x32x506x122xf32>) -> tensor<32x32x506x122xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x32x512x128xf32>, %filter: tensor<7x7xf32>, %init: tensor<32x32x506x122xf32>) -> tensor<32x32x506x122xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x32x512x128xf32>, tensor<7x7xf32>) outs (%init: tensor<32x32x506x122xf32>) -> tensor<32x32x506x122xf32>\n  return %ret : tensor<32x32x506x122xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x32x512x128xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<32x32x506x122xf32>) -> tensor<32x32x506x122xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<32x32x512x128xf32>\n    %1 = bufferization.to_memref %arg2 : memref<32x32x506x122xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x32x506x122xf32>\n    memref.copy %1, %alloc : memref<32x32x506x122xf32> to memref<32x32x506x122xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 506 {\n          affine.for %arg6 = 0 to 122 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<32x32x512x128xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x32x506x122xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x32x506x122xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<32x32x506x122xf32>\n    return %2 : tensor<32x32x506x122xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<32x32x506x122xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<32x32x512x128xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<32x32x512x128xf32>) -> tensor<32x32x512x128xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<32x32x506x122xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<32x32x506x122xf32>) -> tensor<32x32x506x122xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x32x512x128xf32>, tensor<7x7xf32>) outs (%init: tensor<32x32x506x122xf32>) -> tensor<32x32x506x122xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<32x32x506x122xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<32x32x506x122xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 506, 1], ["%arg6", 0, 122, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 12331776210}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x16x256x512xf32>, tensor<5x5xf32>) outs (%init: tensor<32x16x126x254xf32>) -> tensor<32x16x126x254xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x16x256x512xf32>, tensor<5x5xf32>) outs (%init: tensor<32x16x126x254xf32>) -> tensor<32x16x126x254xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x16x256x512xf32>, %filter: tensor<5x5xf32>, %init: tensor<32x16x126x254xf32>) -> tensor<32x16x126x254xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x16x256x512xf32>, tensor<5x5xf32>) outs (%init: tensor<32x16x126x254xf32>) -> tensor<32x16x126x254xf32>\n  return %ret : tensor<32x16x126x254xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x16x256x512xf32>, %arg1: tensor<5x5xf32>, %arg2: tensor<32x16x126x254xf32>) -> tensor<32x16x126x254xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<32x16x256x512xf32>\n    %1 = bufferization.to_memref %arg2 : memref<32x16x126x254xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x16x126x254xf32>\n    memref.copy %1, %alloc : memref<32x16x126x254xf32> to memref<32x16x126x254xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 16 {\n        affine.for %arg5 = 0 to 126 {\n          affine.for %arg6 = 0 to 254 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<32x16x256x512xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x16x126x254xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x16x126x254xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<32x16x126x254xf32>\n    return %2 : tensor<32x16x126x254xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<32x16x126x254xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<32x16x256x512xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<32x16x256x512xf32>) -> tensor<32x16x256x512xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<5x5xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<5x5xf32>) -> tensor<5x5xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<32x16x126x254xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<32x16x126x254xf32>) -> tensor<32x16x126x254xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x16x256x512xf32>, tensor<5x5xf32>) outs (%init: tensor<32x16x126x254xf32>) -> tensor<32x16x126x254xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<32x16x126x254xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<32x16x126x254xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 16, 1], ["%arg5", 0, 126, 1], ["%arg6", 0, 254, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 1365335472}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x128x32x64xf32>, tensor<3x3xf32>) outs (%init: tensor<8x128x30x62xf32>) -> tensor<8x128x30x62xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x128x32x64xf32>, tensor<3x3xf32>) outs (%init: tensor<8x128x30x62xf32>) -> tensor<8x128x30x62xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<8x128x32x64xf32>, %filter: tensor<3x3xf32>, %init: tensor<8x128x30x62xf32>) -> tensor<8x128x30x62xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x128x32x64xf32>, tensor<3x3xf32>) outs (%init: tensor<8x128x30x62xf32>) -> tensor<8x128x30x62xf32>\n  return %ret : tensor<8x128x30x62xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<8x128x32x64xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<8x128x30x62xf32>) -> tensor<8x128x30x62xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<8x128x32x64xf32>\n    %1 = bufferization.to_memref %arg2 : memref<8x128x30x62xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<8x128x30x62xf32>\n    memref.copy %1, %alloc : memref<8x128x30x62xf32> to memref<8x128x30x62xf32>\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 30 {\n          affine.for %arg6 = 0 to 62 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<8x128x32x64xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x128x30x62xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x128x30x62xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<8x128x30x62xf32>\n    return %2 : tensor<8x128x30x62xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<8x128x30x62xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<8x128x32x64xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<8x128x32x64xf32>) -> tensor<8x128x32x64xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<8x128x30x62xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<8x128x30x62xf32>) -> tensor<8x128x30x62xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x128x32x64xf32>, tensor<3x3xf32>) outs (%init: tensor<8x128x30x62xf32>) -> tensor<8x128x30x62xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<8x128x30x62xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<8x128x30x62xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 8, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 30, 1], ["%arg6", 0, 62, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 40660111}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x32x512x256xf32>, tensor<7x7xf32>) outs (%init: tensor<16x32x253x125xf32>) -> tensor<16x32x253x125xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x32x512x256xf32>, tensor<7x7xf32>) outs (%init: tensor<16x32x253x125xf32>) -> tensor<16x32x253x125xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<16x32x512x256xf32>, %filter: tensor<7x7xf32>, %init: tensor<16x32x253x125xf32>) -> tensor<16x32x253x125xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x32x512x256xf32>, tensor<7x7xf32>) outs (%init: tensor<16x32x253x125xf32>) -> tensor<16x32x253x125xf32>\n  return %ret : tensor<16x32x253x125xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<16x32x512x256xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<16x32x253x125xf32>) -> tensor<16x32x253x125xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<16x32x512x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<16x32x253x125xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x32x253x125xf32>\n    memref.copy %1, %alloc : memref<16x32x253x125xf32> to memref<16x32x253x125xf32>\n    affine.for %arg3 = 0 to 16 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 253 {\n          affine.for %arg6 = 0 to 125 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<16x32x512x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x32x253x125xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x32x253x125xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<16x32x253x125xf32>\n    return %2 : tensor<16x32x253x125xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x32x253x125xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<16x32x512x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<16x32x512x256xf32>) -> tensor<16x32x512x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<16x32x253x125xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<16x32x253x125xf32>) -> tensor<16x32x253x125xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x32x512x256xf32>, tensor<7x7xf32>) outs (%init: tensor<16x32x253x125xf32>) -> tensor<16x32x253x125xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x32x253x125xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x32x253x125xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 16, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 253, 1], ["%arg6", 0, 125, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 3163074312}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x64x512x256xf32>, tensor<5x5xf32>) outs (%init: tensor<8x64x508x252xf32>) -> tensor<8x64x508x252xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x64x512x256xf32>, tensor<5x5xf32>) outs (%init: tensor<8x64x508x252xf32>) -> tensor<8x64x508x252xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<8x64x512x256xf32>, %filter: tensor<5x5xf32>, %init: tensor<8x64x508x252xf32>) -> tensor<8x64x508x252xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x64x512x256xf32>, tensor<5x5xf32>) outs (%init: tensor<8x64x508x252xf32>) -> tensor<8x64x508x252xf32>\n  return %ret : tensor<8x64x508x252xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<8x64x512x256xf32>, %arg1: tensor<5x5xf32>, %arg2: tensor<8x64x508x252xf32>) -> tensor<8x64x508x252xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<8x64x512x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<8x64x508x252xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<8x64x508x252xf32>\n    memref.copy %1, %alloc : memref<8x64x508x252xf32> to memref<8x64x508x252xf32>\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 508 {\n          affine.for %arg6 = 0 to 252 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<8x64x512x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x64x508x252xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x64x508x252xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<8x64x508x252xf32>\n    return %2 : tensor<8x64x508x252xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<8x64x508x252xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<8x64x512x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<8x64x512x256xf32>) -> tensor<8x64x512x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<5x5xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<5x5xf32>) -> tensor<5x5xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<8x64x508x252xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<8x64x508x252xf32>) -> tensor<8x64x508x252xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x64x512x256xf32>, tensor<5x5xf32>) outs (%init: tensor<8x64x508x252xf32>) -> tensor<8x64x508x252xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<8x64x508x252xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<8x64x508x252xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 8, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 508, 1], ["%arg6", 0, 252, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 5428519206}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x16x512x64xf32>, tensor<1x1xf32>) outs (%init: tensor<256x16x256x32xf32>) -> tensor<256x16x256x32xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x16x512x64xf32>, tensor<1x1xf32>) outs (%init: tensor<256x16x256x32xf32>) -> tensor<256x16x256x32xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x16x512x64xf32>, %filter: tensor<1x1xf32>, %init: tensor<256x16x256x32xf32>) -> tensor<256x16x256x32xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x16x512x64xf32>, tensor<1x1xf32>) outs (%init: tensor<256x16x256x32xf32>) -> tensor<256x16x256x32xf32>\n  return %ret : tensor<256x16x256x32xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x16x512x64xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<256x16x256x32xf32>) -> tensor<256x16x256x32xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<256x16x512x64xf32>\n    %1 = bufferization.to_memref %arg2 : memref<256x16x256x32xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x16x256x32xf32>\n    memref.copy %1, %alloc : memref<256x16x256x32xf32> to memref<256x16x256x32xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 16 {\n        affine.for %arg5 = 0 to 256 {\n          affine.for %arg6 = 0 to 32 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<256x16x512x64xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x16x256x32xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x16x256x32xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<256x16x256x32xf32>\n    return %2 : tensor<256x16x256x32xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x16x256x32xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x16x512x64xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x16x512x64xf32>) -> tensor<256x16x512x64xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x16x256x32xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x16x256x32xf32>) -> tensor<256x16x256x32xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x16x512x64xf32>, tensor<1x1xf32>) outs (%init: tensor<256x16x256x32xf32>) -> tensor<256x16x256x32xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x16x256x32xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x16x256x32xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 16, 1], ["%arg5", 0, 256, 1], ["%arg6", 0, 32, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 68918452}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x128x128x128xf32>, tensor<3x3xf32>) outs (%init: tensor<8x128x63x63xf32>) -> tensor<8x128x63x63xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x128x128x128xf32>, tensor<3x3xf32>) outs (%init: tensor<8x128x63x63xf32>) -> tensor<8x128x63x63xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<8x128x128x128xf32>, %filter: tensor<3x3xf32>, %init: tensor<8x128x63x63xf32>) -> tensor<8x128x63x63xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x128x128x128xf32>, tensor<3x3xf32>) outs (%init: tensor<8x128x63x63xf32>) -> tensor<8x128x63x63xf32>\n  return %ret : tensor<8x128x63x63xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<8x128x128x128xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<8x128x63x63xf32>) -> tensor<8x128x63x63xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<8x128x128x128xf32>\n    %1 = bufferization.to_memref %arg2 : memref<8x128x63x63xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<8x128x63x63xf32>\n    memref.copy %1, %alloc : memref<8x128x63x63xf32> to memref<8x128x63x63xf32>\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 63 {\n          affine.for %arg6 = 0 to 63 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<8x128x128x128xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x128x63x63xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x128x63x63xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<8x128x63x63xf32>\n    return %2 : tensor<8x128x63x63xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<8x128x63x63xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<8x128x128x128xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<8x128x128x128xf32>) -> tensor<8x128x128x128xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<8x128x63x63xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<8x128x63x63xf32>) -> tensor<8x128x63x63xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x128x128x128xf32>, tensor<3x3xf32>) outs (%init: tensor<8x128x63x63xf32>) -> tensor<8x128x63x63xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<8x128x63x63xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<8x128x63x63xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 8, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 63, 1], ["%arg6", 0, 63, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 87711099}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x64x32x256xf32>, tensor<7x7xf32>) outs (%init: tensor<16x64x13x125xf32>) -> tensor<16x64x13x125xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x64x32x256xf32>, tensor<7x7xf32>) outs (%init: tensor<16x64x13x125xf32>) -> tensor<16x64x13x125xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<16x64x32x256xf32>, %filter: tensor<7x7xf32>, %init: tensor<16x64x13x125xf32>) -> tensor<16x64x13x125xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x64x32x256xf32>, tensor<7x7xf32>) outs (%init: tensor<16x64x13x125xf32>) -> tensor<16x64x13x125xf32>\n  return %ret : tensor<16x64x13x125xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<16x64x32x256xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<16x64x13x125xf32>) -> tensor<16x64x13x125xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<16x64x32x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<16x64x13x125xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x64x13x125xf32>\n    memref.copy %1, %alloc : memref<16x64x13x125xf32> to memref<16x64x13x125xf32>\n    affine.for %arg3 = 0 to 16 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 13 {\n          affine.for %arg6 = 0 to 125 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<16x64x32x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x64x13x125xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x64x13x125xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<16x64x13x125xf32>\n    return %2 : tensor<16x64x13x125xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x64x13x125xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<16x64x32x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<16x64x32x256xf32>) -> tensor<16x64x32x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<16x64x13x125xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<16x64x13x125xf32>) -> tensor<16x64x13x125xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x64x32x256xf32>, tensor<7x7xf32>) outs (%init: tensor<16x64x13x125xf32>) -> tensor<16x64x13x125xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x64x13x125xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x64x13x125xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 16, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 13, 1], ["%arg6", 0, 125, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 325092167}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x1024x128x32xf32>, tensor<3x3xf32>) outs (%init: tensor<8x1024x126x30xf32>) -> tensor<8x1024x126x30xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x1024x128x32xf32>, tensor<3x3xf32>) outs (%init: tensor<8x1024x126x30xf32>) -> tensor<8x1024x126x30xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<8x1024x128x32xf32>, %filter: tensor<3x3xf32>, %init: tensor<8x1024x126x30xf32>) -> tensor<8x1024x126x30xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x1024x128x32xf32>, tensor<3x3xf32>) outs (%init: tensor<8x1024x126x30xf32>) -> tensor<8x1024x126x30xf32>\n  return %ret : tensor<8x1024x126x30xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<8x1024x128x32xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<8x1024x126x30xf32>) -> tensor<8x1024x126x30xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<8x1024x128x32xf32>\n    %1 = bufferization.to_memref %arg2 : memref<8x1024x126x30xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<8x1024x126x30xf32>\n    memref.copy %1, %alloc : memref<8x1024x126x30xf32> to memref<8x1024x126x30xf32>\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 126 {\n          affine.for %arg6 = 0 to 30 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<8x1024x128x32xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x1024x126x30xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x1024x126x30xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<8x1024x126x30xf32>\n    return %2 : tensor<8x1024x126x30xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<8x1024x126x30xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<8x1024x128x32xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<8x1024x128x32xf32>) -> tensor<8x1024x128x32xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<8x1024x126x30xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<8x1024x126x30xf32>) -> tensor<8x1024x126x30xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x1024x128x32xf32>, tensor<3x3xf32>) outs (%init: tensor<8x1024x126x30xf32>) -> tensor<8x1024x126x30xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<8x1024x126x30xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<8x1024x126x30xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 8, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 126, 1], ["%arg6", 0, 30, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 676644209}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x512x64x32xf32>, tensor<1x1xf32>) outs (%init: tensor<32x512x64x32xf32>) -> tensor<32x512x64x32xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x512x64x32xf32>, tensor<1x1xf32>) outs (%init: tensor<32x512x64x32xf32>) -> tensor<32x512x64x32xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x512x64x32xf32>, %filter: tensor<1x1xf32>, %init: tensor<32x512x64x32xf32>) -> tensor<32x512x64x32xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x512x64x32xf32>, tensor<1x1xf32>) outs (%init: tensor<32x512x64x32xf32>) -> tensor<32x512x64x32xf32>\n  return %ret : tensor<32x512x64x32xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x512x64x32xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<32x512x64x32xf32>) -> tensor<32x512x64x32xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<32x512x64x32xf32>\n    %1 = bufferization.to_memref %arg2 : memref<32x512x64x32xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x512x64x32xf32>\n    memref.copy %1, %alloc : memref<32x512x64x32xf32> to memref<32x512x64x32xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 64 {\n          affine.for %arg6 = 0 to 32 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<32x512x64x32xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x512x64x32xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x512x64x32xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<32x512x64x32xf32>\n    return %2 : tensor<32x512x64x32xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<32x512x64x32xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<32x512x64x32xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<32x512x64x32xf32>) -> tensor<32x512x64x32xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<32x512x64x32xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<32x512x64x32xf32>) -> tensor<32x512x64x32xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x512x64x32xf32>, tensor<1x1xf32>) outs (%init: tensor<32x512x64x32xf32>) -> tensor<32x512x64x32xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<32x512x64x32xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<32x512x64x32xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 64, 1], ["%arg6", 0, 32, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 48909727}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x64x32x64xf32>, tensor<3x3xf32>) outs (%init: tensor<256x64x30x62xf32>) -> tensor<256x64x30x62xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x64x32x64xf32>, tensor<3x3xf32>) outs (%init: tensor<256x64x30x62xf32>) -> tensor<256x64x30x62xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x64x32x64xf32>, %filter: tensor<3x3xf32>, %init: tensor<256x64x30x62xf32>) -> tensor<256x64x30x62xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x64x32x64xf32>, tensor<3x3xf32>) outs (%init: tensor<256x64x30x62xf32>) -> tensor<256x64x30x62xf32>\n  return %ret : tensor<256x64x30x62xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x64x32x64xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<256x64x30x62xf32>) -> tensor<256x64x30x62xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<256x64x32x64xf32>\n    %1 = bufferization.to_memref %arg2 : memref<256x64x30x62xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x64x30x62xf32>\n    memref.copy %1, %alloc : memref<256x64x30x62xf32> to memref<256x64x30x62xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 30 {\n          affine.for %arg6 = 0 to 62 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<256x64x32x64xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x64x30x62xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x64x30x62xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<256x64x30x62xf32>\n    return %2 : tensor<256x64x30x62xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x64x30x62xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x64x32x64xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x64x32x64xf32>) -> tensor<256x64x32x64xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x64x30x62xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x64x30x62xf32>) -> tensor<256x64x30x62xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x64x32x64xf32>, tensor<3x3xf32>) outs (%init: tensor<256x64x30x62xf32>) -> tensor<256x64x30x62xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x64x30x62xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x64x30x62xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 30, 1], ["%arg6", 0, 62, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 660998262}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x8x32x128xf32>, tensor<5x5xf32>) outs (%init: tensor<8x8x14x62xf32>) -> tensor<8x8x14x62xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x8x32x128xf32>, tensor<5x5xf32>) outs (%init: tensor<8x8x14x62xf32>) -> tensor<8x8x14x62xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<8x8x32x128xf32>, %filter: tensor<5x5xf32>, %init: tensor<8x8x14x62xf32>) -> tensor<8x8x14x62xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x8x32x128xf32>, tensor<5x5xf32>) outs (%init: tensor<8x8x14x62xf32>) -> tensor<8x8x14x62xf32>\n  return %ret : tensor<8x8x14x62xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<8x8x32x128xf32>, %arg1: tensor<5x5xf32>, %arg2: tensor<8x8x14x62xf32>) -> tensor<8x8x14x62xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<8x8x32x128xf32>\n    %1 = bufferization.to_memref %arg2 : memref<8x8x14x62xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<8x8x14x62xf32>\n    memref.copy %1, %alloc : memref<8x8x14x62xf32> to memref<8x8x14x62xf32>\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 8 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 62 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<8x8x32x128xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x8x14x62xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x8x14x62xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<8x8x14x62xf32>\n    return %2 : tensor<8x8x14x62xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<8x8x14x62xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<8x8x32x128xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<8x8x32x128xf32>) -> tensor<8x8x32x128xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<5x5xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<5x5xf32>) -> tensor<5x5xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<8x8x14x62xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<8x8x14x62xf32>) -> tensor<8x8x14x62xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x8x32x128xf32>, tensor<5x5xf32>) outs (%init: tensor<8x8x14x62xf32>) -> tensor<8x8x14x62xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<8x8x14x62xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<8x8x14x62xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 8, 1], ["%arg4", 0, 8, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 62, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 4597559}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x32x32x128xf32>, tensor<3x3xf32>) outs (%init: tensor<8x32x30x126xf32>) -> tensor<8x32x30x126xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x32x32x128xf32>, tensor<3x3xf32>) outs (%init: tensor<8x32x30x126xf32>) -> tensor<8x32x30x126xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<8x32x32x128xf32>, %filter: tensor<3x3xf32>, %init: tensor<8x32x30x126xf32>) -> tensor<8x32x30x126xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x32x32x128xf32>, tensor<3x3xf32>) outs (%init: tensor<8x32x30x126xf32>) -> tensor<8x32x30x126xf32>\n  return %ret : tensor<8x32x30x126xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<8x32x32x128xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<8x32x30x126xf32>) -> tensor<8x32x30x126xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<8x32x32x128xf32>\n    %1 = bufferization.to_memref %arg2 : memref<8x32x30x126xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<8x32x30x126xf32>\n    memref.copy %1, %alloc : memref<8x32x30x126xf32> to memref<8x32x30x126xf32>\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 30 {\n          affine.for %arg6 = 0 to 126 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<8x32x32x128xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x32x30x126xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x32x30x126xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<8x32x30x126xf32>\n    return %2 : tensor<8x32x30x126xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<8x32x30x126xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<8x32x32x128xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<8x32x32x128xf32>) -> tensor<8x32x32x128xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<8x32x30x126xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<8x32x30x126xf32>) -> tensor<8x32x30x126xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x32x32x128xf32>, tensor<3x3xf32>) outs (%init: tensor<8x32x30x126xf32>) -> tensor<8x32x30x126xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<8x32x30x126xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<8x32x30x126xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 8, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 30, 1], ["%arg6", 0, 126, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 20574534}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x8x32x32xf32>, tensor<5x5xf32>) outs (%init: tensor<4x8x14x14xf32>) -> tensor<4x8x14x14xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x8x32x32xf32>, tensor<5x5xf32>) outs (%init: tensor<4x8x14x14xf32>) -> tensor<4x8x14x14xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<4x8x32x32xf32>, %filter: tensor<5x5xf32>, %init: tensor<4x8x14x14xf32>) -> tensor<4x8x14x14xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x8x32x32xf32>, tensor<5x5xf32>) outs (%init: tensor<4x8x14x14xf32>) -> tensor<4x8x14x14xf32>\n  return %ret : tensor<4x8x14x14xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<4x8x32x32xf32>, %arg1: tensor<5x5xf32>, %arg2: tensor<4x8x14x14xf32>) -> tensor<4x8x14x14xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<4x8x32x32xf32>\n    %1 = bufferization.to_memref %arg2 : memref<4x8x14x14xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<4x8x14x14xf32>\n    memref.copy %1, %alloc : memref<4x8x14x14xf32> to memref<4x8x14x14xf32>\n    affine.for %arg3 = 0 to 4 {\n      affine.for %arg4 = 0 to 8 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 14 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<4x8x32x32xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x8x14x14xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x8x14x14xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<4x8x14x14xf32>\n    return %2 : tensor<4x8x14x14xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<4x8x14x14xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<4x8x32x32xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<4x8x32x32xf32>) -> tensor<4x8x32x32xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<5x5xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<5x5xf32>) -> tensor<5x5xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<4x8x14x14xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<4x8x14x14xf32>) -> tensor<4x8x14x14xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x8x32x32xf32>, tensor<5x5xf32>) outs (%init: tensor<4x8x14x14xf32>) -> tensor<4x8x14x14xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<4x8x14x14xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<4x8x14x14xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 4, 1], ["%arg4", 0, 8, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 14, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 538965}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x32x256x256xf32>, tensor<1x1xf32>) outs (%init: tensor<256x32x128x128xf32>) -> tensor<256x32x128x128xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x32x256x256xf32>, tensor<1x1xf32>) outs (%init: tensor<256x32x128x128xf32>) -> tensor<256x32x128x128xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x32x256x256xf32>, %filter: tensor<1x1xf32>, %init: tensor<256x32x128x128xf32>) -> tensor<256x32x128x128xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x32x256x256xf32>, tensor<1x1xf32>) outs (%init: tensor<256x32x128x128xf32>) -> tensor<256x32x128x128xf32>\n  return %ret : tensor<256x32x128x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x32x256x256xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<256x32x128x128xf32>) -> tensor<256x32x128x128xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<256x32x256x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<256x32x128x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x32x128x128xf32>\n    memref.copy %1, %alloc : memref<256x32x128x128xf32> to memref<256x32x128x128xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 128 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<256x32x256x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x32x128x128xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x32x128x128xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<256x32x128x128xf32>\n    return %2 : tensor<256x32x128x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x32x128x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x32x256x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x32x256x256xf32>) -> tensor<256x32x256x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x32x128x128xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x32x128x128xf32>) -> tensor<256x32x128x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x32x256x256xf32>, tensor<1x1xf32>) outs (%init: tensor<256x32x128x128xf32>) -> tensor<256x32x128x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x32x128x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x32x128x128xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 128, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 286568263}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x64x256x64xf32>, tensor<3x3xf32>) outs (%init: tensor<32x64x254x62xf32>) -> tensor<32x64x254x62xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x64x256x64xf32>, tensor<3x3xf32>) outs (%init: tensor<32x64x254x62xf32>) -> tensor<32x64x254x62xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x64x256x64xf32>, %filter: tensor<3x3xf32>, %init: tensor<32x64x254x62xf32>) -> tensor<32x64x254x62xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x64x256x64xf32>, tensor<3x3xf32>) outs (%init: tensor<32x64x254x62xf32>) -> tensor<32x64x254x62xf32>\n  return %ret : tensor<32x64x254x62xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x64x256x64xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<32x64x254x62xf32>) -> tensor<32x64x254x62xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<32x64x256x64xf32>\n    %1 = bufferization.to_memref %arg2 : memref<32x64x254x62xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x64x254x62xf32>\n    memref.copy %1, %alloc : memref<32x64x254x62xf32> to memref<32x64x254x62xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 254 {\n          affine.for %arg6 = 0 to 62 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<32x64x256x64xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x64x254x62xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x64x254x62xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<32x64x254x62xf32>\n    return %2 : tensor<32x64x254x62xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<32x64x254x62xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<32x64x256x64xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<32x64x256x64xf32>) -> tensor<32x64x256x64xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<32x64x254x62xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<32x64x254x62xf32>) -> tensor<32x64x254x62xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x64x256x64xf32>, tensor<3x3xf32>) outs (%init: tensor<32x64x254x62xf32>) -> tensor<32x64x254x62xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<32x64x254x62xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<32x64x254x62xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 254, 1], ["%arg6", 0, 62, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 696988010}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x128x128x128xf32>, tensor<3x3xf32>) outs (%init: tensor<16x128x63x63xf32>) -> tensor<16x128x63x63xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x128x128x128xf32>, tensor<3x3xf32>) outs (%init: tensor<16x128x63x63xf32>) -> tensor<16x128x63x63xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<16x128x128x128xf32>, %filter: tensor<3x3xf32>, %init: tensor<16x128x63x63xf32>) -> tensor<16x128x63x63xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x128x128x128xf32>, tensor<3x3xf32>) outs (%init: tensor<16x128x63x63xf32>) -> tensor<16x128x63x63xf32>\n  return %ret : tensor<16x128x63x63xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<16x128x128x128xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<16x128x63x63xf32>) -> tensor<16x128x63x63xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<16x128x128x128xf32>\n    %1 = bufferization.to_memref %arg2 : memref<16x128x63x63xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x128x63x63xf32>\n    memref.copy %1, %alloc : memref<16x128x63x63xf32> to memref<16x128x63x63xf32>\n    affine.for %arg3 = 0 to 16 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 63 {\n          affine.for %arg6 = 0 to 63 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<16x128x128x128xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x128x63x63xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x128x63x63xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<16x128x63x63xf32>\n    return %2 : tensor<16x128x63x63xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x128x63x63xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<16x128x128x128xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<16x128x128x128xf32>) -> tensor<16x128x128x128xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<16x128x63x63xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<16x128x63x63xf32>) -> tensor<16x128x63x63xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x128x128x128xf32>, tensor<3x3xf32>) outs (%init: tensor<16x128x63x63xf32>) -> tensor<16x128x63x63xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x128x63x63xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x128x63x63xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 16, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 63, 1], ["%arg6", 0, 63, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 174983310}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x32x64x256xf32>, tensor<7x7xf32>) outs (%init: tensor<64x32x29x125xf32>) -> tensor<64x32x29x125xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x32x64x256xf32>, tensor<7x7xf32>) outs (%init: tensor<64x32x29x125xf32>) -> tensor<64x32x29x125xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x32x64x256xf32>, %filter: tensor<7x7xf32>, %init: tensor<64x32x29x125xf32>) -> tensor<64x32x29x125xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x32x64x256xf32>, tensor<7x7xf32>) outs (%init: tensor<64x32x29x125xf32>) -> tensor<64x32x29x125xf32>\n  return %ret : tensor<64x32x29x125xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x32x64x256xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<64x32x29x125xf32>) -> tensor<64x32x29x125xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<64x32x64x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<64x32x29x125xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x32x29x125xf32>\n    memref.copy %1, %alloc : memref<64x32x29x125xf32> to memref<64x32x29x125xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 29 {\n          affine.for %arg6 = 0 to 125 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<64x32x64x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x32x29x125xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x32x29x125xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<64x32x29x125xf32>\n    return %2 : tensor<64x32x29x125xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<64x32x29x125xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<64x32x64x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<64x32x64x256xf32>) -> tensor<64x32x64x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<64x32x29x125xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<64x32x29x125xf32>) -> tensor<64x32x29x125xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x32x64x256xf32>, tensor<7x7xf32>) outs (%init: tensor<64x32x29x125xf32>) -> tensor<64x32x29x125xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<64x32x29x125xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<64x32x29x125xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 29, 1], ["%arg6", 0, 125, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 1448373796}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x1024x64x128xf32>, tensor<5x5xf32>) outs (%init: tensor<4x1024x30x62xf32>) -> tensor<4x1024x30x62xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x1024x64x128xf32>, tensor<5x5xf32>) outs (%init: tensor<4x1024x30x62xf32>) -> tensor<4x1024x30x62xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<4x1024x64x128xf32>, %filter: tensor<5x5xf32>, %init: tensor<4x1024x30x62xf32>) -> tensor<4x1024x30x62xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x1024x64x128xf32>, tensor<5x5xf32>) outs (%init: tensor<4x1024x30x62xf32>) -> tensor<4x1024x30x62xf32>\n  return %ret : tensor<4x1024x30x62xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<4x1024x64x128xf32>, %arg1: tensor<5x5xf32>, %arg2: tensor<4x1024x30x62xf32>) -> tensor<4x1024x30x62xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<4x1024x64x128xf32>\n    %1 = bufferization.to_memref %arg2 : memref<4x1024x30x62xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<4x1024x30x62xf32>\n    memref.copy %1, %alloc : memref<4x1024x30x62xf32> to memref<4x1024x30x62xf32>\n    affine.for %arg3 = 0 to 4 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 30 {\n          affine.for %arg6 = 0 to 62 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<4x1024x64x128xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x1024x30x62xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x1024x30x62xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<4x1024x30x62xf32>\n    return %2 : tensor<4x1024x30x62xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<4x1024x30x62xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<4x1024x64x128xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<4x1024x64x128xf32>) -> tensor<4x1024x64x128xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<5x5xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<5x5xf32>) -> tensor<5x5xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<4x1024x30x62xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<4x1024x30x62xf32>) -> tensor<4x1024x30x62xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x1024x64x128xf32>, tensor<5x5xf32>) outs (%init: tensor<4x1024x30x62xf32>) -> tensor<4x1024x30x62xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<4x1024x30x62xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<4x1024x30x62xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 4, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 30, 1], ["%arg6", 0, 62, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 634494399}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x256x512x128xf32>, tensor<1x1xf32>) outs (%init: tensor<64x256x256x64xf32>) -> tensor<64x256x256x64xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x256x512x128xf32>, tensor<1x1xf32>) outs (%init: tensor<64x256x256x64xf32>) -> tensor<64x256x256x64xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x256x512x128xf32>, %filter: tensor<1x1xf32>, %init: tensor<64x256x256x64xf32>) -> tensor<64x256x256x64xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x256x512x128xf32>, tensor<1x1xf32>) outs (%init: tensor<64x256x256x64xf32>) -> tensor<64x256x256x64xf32>\n  return %ret : tensor<64x256x256x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x256x512x128xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<64x256x256x64xf32>) -> tensor<64x256x256x64xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<64x256x512x128xf32>\n    %1 = bufferization.to_memref %arg2 : memref<64x256x256x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x256x256x64xf32>\n    memref.copy %1, %alloc : memref<64x256x256x64xf32> to memref<64x256x256x64xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 256 {\n          affine.for %arg6 = 0 to 64 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<64x256x512x128xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x256x256x64xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x256x256x64xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<64x256x256x64xf32>\n    return %2 : tensor<64x256x256x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<64x256x256x64xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<64x256x512x128xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<64x256x512x128xf32>) -> tensor<64x256x512x128xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<64x256x256x64xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<64x256x256x64xf32>) -> tensor<64x256x256x64xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x256x512x128xf32>, tensor<1x1xf32>) outs (%init: tensor<64x256x256x64xf32>) -> tensor<64x256x256x64xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<64x256x256x64xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<64x256x256x64xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 256, 1], ["%arg6", 0, 64, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 544960718}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x64x32x128xf32>, tensor<5x5xf32>) outs (%init: tensor<64x64x28x124xf32>) -> tensor<64x64x28x124xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x64x32x128xf32>, tensor<5x5xf32>) outs (%init: tensor<64x64x28x124xf32>) -> tensor<64x64x28x124xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x64x32x128xf32>, %filter: tensor<5x5xf32>, %init: tensor<64x64x28x124xf32>) -> tensor<64x64x28x124xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x64x32x128xf32>, tensor<5x5xf32>) outs (%init: tensor<64x64x28x124xf32>) -> tensor<64x64x28x124xf32>\n  return %ret : tensor<64x64x28x124xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x64x32x128xf32>, %arg1: tensor<5x5xf32>, %arg2: tensor<64x64x28x124xf32>) -> tensor<64x64x28x124xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<64x64x32x128xf32>\n    %1 = bufferization.to_memref %arg2 : memref<64x64x28x124xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x64x28x124xf32>\n    memref.copy %1, %alloc : memref<64x64x28x124xf32> to memref<64x64x28x124xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 124 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<64x64x32x128xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x64x28x124xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x64x28x124xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<64x64x28x124xf32>\n    return %2 : tensor<64x64x28x124xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<64x64x28x124xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<64x64x32x128xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<64x64x32x128xf32>) -> tensor<64x64x32x128xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<5x5xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<5x5xf32>) -> tensor<5x5xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<64x64x28x124xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<64x64x28x124xf32>) -> tensor<64x64x28x124xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x64x32x128xf32>, tensor<5x5xf32>) outs (%init: tensor<64x64x28x124xf32>) -> tensor<64x64x28x124xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<64x64x28x124xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<64x64x28x124xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 124, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 1176950019}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x128x256x64xf32>, tensor<7x7xf32>) outs (%init: tensor<4x128x250x58xf32>) -> tensor<4x128x250x58xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x128x256x64xf32>, tensor<7x7xf32>) outs (%init: tensor<4x128x250x58xf32>) -> tensor<4x128x250x58xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<4x128x256x64xf32>, %filter: tensor<7x7xf32>, %init: tensor<4x128x250x58xf32>) -> tensor<4x128x250x58xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x128x256x64xf32>, tensor<7x7xf32>) outs (%init: tensor<4x128x250x58xf32>) -> tensor<4x128x250x58xf32>\n  return %ret : tensor<4x128x250x58xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<4x128x256x64xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<4x128x250x58xf32>) -> tensor<4x128x250x58xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<4x128x256x64xf32>\n    %1 = bufferization.to_memref %arg2 : memref<4x128x250x58xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<4x128x250x58xf32>\n    memref.copy %1, %alloc : memref<4x128x250x58xf32> to memref<4x128x250x58xf32>\n    affine.for %arg3 = 0 to 4 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 250 {\n          affine.for %arg6 = 0 to 58 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<4x128x256x64xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x128x250x58xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x128x250x58xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<4x128x250x58xf32>\n    return %2 : tensor<4x128x250x58xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<4x128x250x58xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<4x128x256x64xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<4x128x256x64xf32>) -> tensor<4x128x256x64xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<4x128x250x58xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<4x128x250x58xf32>) -> tensor<4x128x250x58xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x128x256x64xf32>, tensor<7x7xf32>) outs (%init: tensor<4x128x250x58xf32>) -> tensor<4x128x250x58xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<4x128x250x58xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<4x128x250x58xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 4, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 250, 1], ["%arg6", 0, 58, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 1448115305}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x128x512x512xf32>, tensor<1x1xf32>) outs (%init: tensor<32x128x256x256xf32>) -> tensor<32x128x256x256xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x128x512x512xf32>, tensor<1x1xf32>) outs (%init: tensor<32x128x256x256xf32>) -> tensor<32x128x256x256xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x128x512x512xf32>, %filter: tensor<1x1xf32>, %init: tensor<32x128x256x256xf32>) -> tensor<32x128x256x256xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x128x512x512xf32>, tensor<1x1xf32>) outs (%init: tensor<32x128x256x256xf32>) -> tensor<32x128x256x256xf32>\n  return %ret : tensor<32x128x256x256xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x128x512x512xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<32x128x256x256xf32>) -> tensor<32x128x256x256xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<32x128x512x512xf32>\n    %1 = bufferization.to_memref %arg2 : memref<32x128x256x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x128x256x256xf32>\n    memref.copy %1, %alloc : memref<32x128x256x256xf32> to memref<32x128x256x256xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 256 {\n          affine.for %arg6 = 0 to 256 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<32x128x512x512xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x128x256x256xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x128x256x256xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<32x128x256x256xf32>\n    return %2 : tensor<32x128x256x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<32x128x256x256xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<32x128x512x512xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<32x128x512x512xf32>) -> tensor<32x128x512x512xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<32x128x256x256xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<32x128x256x256xf32>) -> tensor<32x128x256x256xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x128x512x512xf32>, tensor<1x1xf32>) outs (%init: tensor<32x128x256x256xf32>) -> tensor<32x128x256x256xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<32x128x256x256xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<32x128x256x256xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 256, 1], ["%arg6", 0, 256, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 513769444}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x256x32x256xf32>, tensor<7x7xf32>) outs (%init: tensor<8x256x13x125xf32>) -> tensor<8x256x13x125xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x256x32x256xf32>, tensor<7x7xf32>) outs (%init: tensor<8x256x13x125xf32>) -> tensor<8x256x13x125xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<8x256x32x256xf32>, %filter: tensor<7x7xf32>, %init: tensor<8x256x13x125xf32>) -> tensor<8x256x13x125xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x256x32x256xf32>, tensor<7x7xf32>) outs (%init: tensor<8x256x13x125xf32>) -> tensor<8x256x13x125xf32>\n  return %ret : tensor<8x256x13x125xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<8x256x32x256xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<8x256x13x125xf32>) -> tensor<8x256x13x125xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<8x256x32x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<8x256x13x125xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<8x256x13x125xf32>\n    memref.copy %1, %alloc : memref<8x256x13x125xf32> to memref<8x256x13x125xf32>\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 13 {\n          affine.for %arg6 = 0 to 125 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<8x256x32x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x256x13x125xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x256x13x125xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<8x256x13x125xf32>\n    return %2 : tensor<8x256x13x125xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<8x256x13x125xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<8x256x32x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<8x256x32x256xf32>) -> tensor<8x256x32x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<8x256x13x125xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<8x256x13x125xf32>) -> tensor<8x256x13x125xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x256x32x256xf32>, tensor<7x7xf32>) outs (%init: tensor<8x256x13x125xf32>) -> tensor<8x256x13x125xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<8x256x13x125xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<8x256x13x125xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 8, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 13, 1], ["%arg6", 0, 125, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 649836038}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x1024x512x32xf32>, tensor<7x7xf32>) outs (%init: tensor<4x1024x253x13xf32>) -> tensor<4x1024x253x13xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x1024x512x32xf32>, tensor<7x7xf32>) outs (%init: tensor<4x1024x253x13xf32>) -> tensor<4x1024x253x13xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<4x1024x512x32xf32>, %filter: tensor<7x7xf32>, %init: tensor<4x1024x253x13xf32>) -> tensor<4x1024x253x13xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x1024x512x32xf32>, tensor<7x7xf32>) outs (%init: tensor<4x1024x253x13xf32>) -> tensor<4x1024x253x13xf32>\n  return %ret : tensor<4x1024x253x13xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<4x1024x512x32xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<4x1024x253x13xf32>) -> tensor<4x1024x253x13xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<4x1024x512x32xf32>\n    %1 = bufferization.to_memref %arg2 : memref<4x1024x253x13xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<4x1024x253x13xf32>\n    memref.copy %1, %alloc : memref<4x1024x253x13xf32> to memref<4x1024x253x13xf32>\n    affine.for %arg3 = 0 to 4 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 253 {\n          affine.for %arg6 = 0 to 13 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<4x1024x512x32xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x1024x253x13xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x1024x253x13xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<4x1024x253x13xf32>\n    return %2 : tensor<4x1024x253x13xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<4x1024x253x13xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<4x1024x512x32xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<4x1024x512x32xf32>) -> tensor<4x1024x512x32xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<4x1024x253x13xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<4x1024x253x13xf32>) -> tensor<4x1024x253x13xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x1024x512x32xf32>, tensor<7x7xf32>) outs (%init: tensor<4x1024x253x13xf32>) -> tensor<4x1024x253x13xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<4x1024x253x13xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<4x1024x253x13xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 4, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 253, 1], ["%arg6", 0, 13, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 2644853217}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x512x128x256xf32>, tensor<3x3xf32>) outs (%init: tensor<8x512x63x127xf32>) -> tensor<8x512x63x127xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x512x128x256xf32>, tensor<3x3xf32>) outs (%init: tensor<8x512x63x127xf32>) -> tensor<8x512x63x127xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<8x512x128x256xf32>, %filter: tensor<3x3xf32>, %init: tensor<8x512x63x127xf32>) -> tensor<8x512x63x127xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x512x128x256xf32>, tensor<3x3xf32>) outs (%init: tensor<8x512x63x127xf32>) -> tensor<8x512x63x127xf32>\n  return %ret : tensor<8x512x63x127xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<8x512x128x256xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<8x512x63x127xf32>) -> tensor<8x512x63x127xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<8x512x128x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<8x512x63x127xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<8x512x63x127xf32>\n    memref.copy %1, %alloc : memref<8x512x63x127xf32> to memref<8x512x63x127xf32>\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 63 {\n          affine.for %arg6 = 0 to 127 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<8x512x128x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x512x63x127xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x512x63x127xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<8x512x63x127xf32>\n    return %2 : tensor<8x512x63x127xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<8x512x63x127xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<8x512x128x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<8x512x128x256xf32>) -> tensor<8x512x128x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<8x512x63x127xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<8x512x63x127xf32>) -> tensor<8x512x63x127xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x512x128x256xf32>, tensor<3x3xf32>) outs (%init: tensor<8x512x63x127xf32>) -> tensor<8x512x63x127xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<8x512x63x127xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<8x512x63x127xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 8, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 63, 1], ["%arg6", 0, 127, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 711258936}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x16x128x128xf32>, tensor<5x5xf32>) outs (%init: tensor<8x16x62x62xf32>) -> tensor<8x16x62x62xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x16x128x128xf32>, tensor<5x5xf32>) outs (%init: tensor<8x16x62x62xf32>) -> tensor<8x16x62x62xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<8x16x128x128xf32>, %filter: tensor<5x5xf32>, %init: tensor<8x16x62x62xf32>) -> tensor<8x16x62x62xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x16x128x128xf32>, tensor<5x5xf32>) outs (%init: tensor<8x16x62x62xf32>) -> tensor<8x16x62x62xf32>\n  return %ret : tensor<8x16x62x62xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<8x16x128x128xf32>, %arg1: tensor<5x5xf32>, %arg2: tensor<8x16x62x62xf32>) -> tensor<8x16x62x62xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<8x16x128x128xf32>\n    %1 = bufferization.to_memref %arg2 : memref<8x16x62x62xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<8x16x62x62xf32>\n    memref.copy %1, %alloc : memref<8x16x62x62xf32> to memref<8x16x62x62xf32>\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 16 {\n        affine.for %arg5 = 0 to 62 {\n          affine.for %arg6 = 0 to 62 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<8x16x128x128xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x16x62x62xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x16x62x62xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<8x16x62x62xf32>\n    return %2 : tensor<8x16x62x62xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<8x16x62x62xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<8x16x128x128xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<8x16x128x128xf32>) -> tensor<8x16x128x128xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<5x5xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<5x5xf32>) -> tensor<5x5xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<8x16x62x62xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<8x16x62x62xf32>) -> tensor<8x16x62x62xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x16x128x128xf32>, tensor<5x5xf32>) outs (%init: tensor<8x16x62x62xf32>) -> tensor<8x16x62x62xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<8x16x62x62xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<8x16x62x62xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 8, 1], ["%arg4", 0, 16, 1], ["%arg5", 0, 62, 1], ["%arg6", 0, 62, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 40703225}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x32x256x128xf32>, tensor<1x1xf32>) outs (%init: tensor<16x32x128x64xf32>) -> tensor<16x32x128x64xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x32x256x128xf32>, tensor<1x1xf32>) outs (%init: tensor<16x32x128x64xf32>) -> tensor<16x32x128x64xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<16x32x256x128xf32>, %filter: tensor<1x1xf32>, %init: tensor<16x32x128x64xf32>) -> tensor<16x32x128x64xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x32x256x128xf32>, tensor<1x1xf32>) outs (%init: tensor<16x32x128x64xf32>) -> tensor<16x32x128x64xf32>\n  return %ret : tensor<16x32x128x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<16x32x256x128xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<16x32x128x64xf32>) -> tensor<16x32x128x64xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<16x32x256x128xf32>\n    %1 = bufferization.to_memref %arg2 : memref<16x32x128x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x32x128x64xf32>\n    memref.copy %1, %alloc : memref<16x32x128x64xf32> to memref<16x32x128x64xf32>\n    affine.for %arg3 = 0 to 16 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 128 {\n          affine.for %arg6 = 0 to 64 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<16x32x256x128xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x32x128x64xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x32x128x64xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<16x32x128x64xf32>\n    return %2 : tensor<16x32x128x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x32x128x64xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<16x32x256x128xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<16x32x256x128xf32>) -> tensor<16x32x256x128xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<16x32x128x64xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<16x32x128x64xf32>) -> tensor<16x32x128x64xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x32x256x128xf32>, tensor<1x1xf32>) outs (%init: tensor<16x32x128x64xf32>) -> tensor<16x32x128x64xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x32x128x64xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x32x128x64xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 16, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 128, 1], ["%arg6", 0, 64, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 8436691}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x256x512x64xf32>, tensor<3x3xf32>) outs (%init: tensor<32x256x255x31xf32>) -> tensor<32x256x255x31xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x256x512x64xf32>, tensor<3x3xf32>) outs (%init: tensor<32x256x255x31xf32>) -> tensor<32x256x255x31xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x256x512x64xf32>, %filter: tensor<3x3xf32>, %init: tensor<32x256x255x31xf32>) -> tensor<32x256x255x31xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x256x512x64xf32>, tensor<3x3xf32>) outs (%init: tensor<32x256x255x31xf32>) -> tensor<32x256x255x31xf32>\n  return %ret : tensor<32x256x255x31xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x256x512x64xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<32x256x255x31xf32>) -> tensor<32x256x255x31xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<32x256x512x64xf32>\n    %1 = bufferization.to_memref %arg2 : memref<32x256x255x31xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x256x255x31xf32>\n    memref.copy %1, %alloc : memref<32x256x255x31xf32> to memref<32x256x255x31xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 255 {\n          affine.for %arg6 = 0 to 31 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<32x256x512x64xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x256x255x31xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x256x255x31xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<32x256x255x31xf32>\n    return %2 : tensor<32x256x255x31xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<32x256x255x31xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<32x256x512x64xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<32x256x512x64xf32>) -> tensor<32x256x512x64xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<32x256x255x31xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<32x256x255x31xf32>) -> tensor<32x256x255x31xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x256x512x64xf32>, tensor<3x3xf32>) outs (%init: tensor<32x256x255x31xf32>) -> tensor<32x256x255x31xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<32x256x255x31xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<32x256x255x31xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 255, 1], ["%arg6", 0, 31, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 1405302904}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x16x128x512xf32>, tensor<5x5xf32>) outs (%init: tensor<4x16x124x508xf32>) -> tensor<4x16x124x508xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x16x128x512xf32>, tensor<5x5xf32>) outs (%init: tensor<4x16x124x508xf32>) -> tensor<4x16x124x508xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<4x16x128x512xf32>, %filter: tensor<5x5xf32>, %init: tensor<4x16x124x508xf32>) -> tensor<4x16x124x508xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x16x128x512xf32>, tensor<5x5xf32>) outs (%init: tensor<4x16x124x508xf32>) -> tensor<4x16x124x508xf32>\n  return %ret : tensor<4x16x124x508xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<4x16x128x512xf32>, %arg1: tensor<5x5xf32>, %arg2: tensor<4x16x124x508xf32>) -> tensor<4x16x124x508xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<4x16x128x512xf32>\n    %1 = bufferization.to_memref %arg2 : memref<4x16x124x508xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<4x16x124x508xf32>\n    memref.copy %1, %alloc : memref<4x16x124x508xf32> to memref<4x16x124x508xf32>\n    affine.for %arg3 = 0 to 4 {\n      affine.for %arg4 = 0 to 16 {\n        affine.for %arg5 = 0 to 124 {\n          affine.for %arg6 = 0 to 508 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<4x16x128x512xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x16x124x508xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x16x124x508xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<4x16x124x508xf32>\n    return %2 : tensor<4x16x124x508xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<4x16x124x508xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<4x16x128x512xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<4x16x128x512xf32>) -> tensor<4x16x128x512xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<5x5xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<5x5xf32>) -> tensor<5x5xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<4x16x124x508xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<4x16x124x508xf32>) -> tensor<4x16x124x508xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x16x128x512xf32>, tensor<5x5xf32>) outs (%init: tensor<4x16x124x508xf32>) -> tensor<4x16x124x508xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<4x16x124x508xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<4x16x124x508xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 4, 1], ["%arg4", 0, 16, 1], ["%arg5", 0, 124, 1], ["%arg6", 0, 508, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 333126840}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x512x128x256xf32>, tensor<5x5xf32>) outs (%init: tensor<8x512x62x126xf32>) -> tensor<8x512x62x126xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x512x128x256xf32>, tensor<5x5xf32>) outs (%init: tensor<8x512x62x126xf32>) -> tensor<8x512x62x126xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<8x512x128x256xf32>, %filter: tensor<5x5xf32>, %init: tensor<8x512x62x126xf32>) -> tensor<8x512x62x126xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x512x128x256xf32>, tensor<5x5xf32>) outs (%init: tensor<8x512x62x126xf32>) -> tensor<8x512x62x126xf32>\n  return %ret : tensor<8x512x62x126xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<8x512x128x256xf32>, %arg1: tensor<5x5xf32>, %arg2: tensor<8x512x62x126xf32>) -> tensor<8x512x62x126xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<8x512x128x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<8x512x62x126xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<8x512x62x126xf32>\n    memref.copy %1, %alloc : memref<8x512x62x126xf32> to memref<8x512x62x126xf32>\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 62 {\n          affine.for %arg6 = 0 to 126 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<8x512x128x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x512x62x126xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x512x62x126xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<8x512x62x126xf32>\n    return %2 : tensor<8x512x62x126xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<8x512x62x126xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<8x512x128x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<8x512x128x256xf32>) -> tensor<8x512x128x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<5x5xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<5x5xf32>) -> tensor<5x5xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<8x512x62x126xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<8x512x62x126xf32>) -> tensor<8x512x62x126xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x512x128x256xf32>, tensor<5x5xf32>) outs (%init: tensor<8x512x62x126xf32>) -> tensor<8x512x62x126xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<8x512x62x126xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<8x512x62x126xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 8, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 62, 1], ["%arg6", 0, 126, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 2669604112}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x8x32x512xf32>, tensor<1x1xf32>) outs (%init: tensor<64x8x16x256xf32>) -> tensor<64x8x16x256xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x8x32x512xf32>, tensor<1x1xf32>) outs (%init: tensor<64x8x16x256xf32>) -> tensor<64x8x16x256xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x8x32x512xf32>, %filter: tensor<1x1xf32>, %init: tensor<64x8x16x256xf32>) -> tensor<64x8x16x256xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x8x32x512xf32>, tensor<1x1xf32>) outs (%init: tensor<64x8x16x256xf32>) -> tensor<64x8x16x256xf32>\n  return %ret : tensor<64x8x16x256xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x8x32x512xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<64x8x16x256xf32>) -> tensor<64x8x16x256xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<64x8x32x512xf32>\n    %1 = bufferization.to_memref %arg2 : memref<64x8x16x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x8x16x256xf32>\n    memref.copy %1, %alloc : memref<64x8x16x256xf32> to memref<64x8x16x256xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 8 {\n        affine.for %arg5 = 0 to 16 {\n          affine.for %arg6 = 0 to 256 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<64x8x32x512xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x8x16x256xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x8x16x256xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<64x8x16x256xf32>\n    return %2 : tensor<64x8x16x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<64x8x16x256xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<64x8x32x512xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<64x8x32x512xf32>) -> tensor<64x8x32x512xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<64x8x16x256xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<64x8x16x256xf32>) -> tensor<64x8x16x256xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x8x32x512xf32>, tensor<1x1xf32>) outs (%init: tensor<64x8x16x256xf32>) -> tensor<64x8x16x256xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<64x8x16x256xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<64x8x16x256xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 8, 1], ["%arg5", 0, 16, 1], ["%arg6", 0, 256, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 3853390}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x8x256x64xf32>, tensor<1x1xf32>) outs (%init: tensor<32x8x256x64xf32>) -> tensor<32x8x256x64xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x8x256x64xf32>, tensor<1x1xf32>) outs (%init: tensor<32x8x256x64xf32>) -> tensor<32x8x256x64xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x8x256x64xf32>, %filter: tensor<1x1xf32>, %init: tensor<32x8x256x64xf32>) -> tensor<32x8x256x64xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x8x256x64xf32>, tensor<1x1xf32>) outs (%init: tensor<32x8x256x64xf32>) -> tensor<32x8x256x64xf32>\n  return %ret : tensor<32x8x256x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x8x256x64xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<32x8x256x64xf32>) -> tensor<32x8x256x64xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<32x8x256x64xf32>\n    %1 = bufferization.to_memref %arg2 : memref<32x8x256x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x8x256x64xf32>\n    memref.copy %1, %alloc : memref<32x8x256x64xf32> to memref<32x8x256x64xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 8 {\n        affine.for %arg5 = 0 to 256 {\n          affine.for %arg6 = 0 to 64 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<32x8x256x64xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x8x256x64xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x8x256x64xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<32x8x256x64xf32>\n    return %2 : tensor<32x8x256x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<32x8x256x64xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<32x8x256x64xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<32x8x256x64xf32>) -> tensor<32x8x256x64xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<32x8x256x64xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<32x8x256x64xf32>) -> tensor<32x8x256x64xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x8x256x64xf32>, tensor<1x1xf32>) outs (%init: tensor<32x8x256x64xf32>) -> tensor<32x8x256x64xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<32x8x256x64xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<32x8x256x64xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 8, 1], ["%arg5", 0, 256, 1], ["%arg6", 0, 64, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 6259082}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x1024x128x32xf32>, tensor<7x7xf32>) outs (%init: tensor<16x1024x122x26xf32>) -> tensor<16x1024x122x26xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x1024x128x32xf32>, tensor<7x7xf32>) outs (%init: tensor<16x1024x122x26xf32>) -> tensor<16x1024x122x26xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<16x1024x128x32xf32>, %filter: tensor<7x7xf32>, %init: tensor<16x1024x122x26xf32>) -> tensor<16x1024x122x26xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x1024x128x32xf32>, tensor<7x7xf32>) outs (%init: tensor<16x1024x122x26xf32>) -> tensor<16x1024x122x26xf32>\n  return %ret : tensor<16x1024x122x26xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<16x1024x128x32xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<16x1024x122x26xf32>) -> tensor<16x1024x122x26xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<16x1024x128x32xf32>\n    %1 = bufferization.to_memref %arg2 : memref<16x1024x122x26xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x1024x122x26xf32>\n    memref.copy %1, %alloc : memref<16x1024x122x26xf32> to memref<16x1024x122x26xf32>\n    affine.for %arg3 = 0 to 16 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 122 {\n          affine.for %arg6 = 0 to 26 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<16x1024x128x32xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x1024x122x26xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x1024x122x26xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<16x1024x122x26xf32>\n    return %2 : tensor<16x1024x122x26xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x1024x122x26xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<16x1024x128x32xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<16x1024x128x32xf32>) -> tensor<16x1024x128x32xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<16x1024x122x26xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<16x1024x122x26xf32>) -> tensor<16x1024x122x26xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x1024x128x32xf32>, tensor<7x7xf32>) outs (%init: tensor<16x1024x122x26xf32>) -> tensor<16x1024x122x26xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x1024x122x26xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x1024x122x26xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 16, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 122, 1], ["%arg6", 0, 26, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 10150102328}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x256x128x128xf32>, tensor<3x3xf32>) outs (%init: tensor<16x256x63x63xf32>) -> tensor<16x256x63x63xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x256x128x128xf32>, tensor<3x3xf32>) outs (%init: tensor<16x256x63x63xf32>) -> tensor<16x256x63x63xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<16x256x128x128xf32>, %filter: tensor<3x3xf32>, %init: tensor<16x256x63x63xf32>) -> tensor<16x256x63x63xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x256x128x128xf32>, tensor<3x3xf32>) outs (%init: tensor<16x256x63x63xf32>) -> tensor<16x256x63x63xf32>\n  return %ret : tensor<16x256x63x63xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<16x256x128x128xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<16x256x63x63xf32>) -> tensor<16x256x63x63xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<16x256x128x128xf32>\n    %1 = bufferization.to_memref %arg2 : memref<16x256x63x63xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x256x63x63xf32>\n    memref.copy %1, %alloc : memref<16x256x63x63xf32> to memref<16x256x63x63xf32>\n    affine.for %arg3 = 0 to 16 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 63 {\n          affine.for %arg6 = 0 to 63 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<16x256x128x128xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x256x63x63xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x256x63x63xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<16x256x63x63xf32>\n    return %2 : tensor<16x256x63x63xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x256x63x63xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<16x256x128x128xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<16x256x128x128xf32>) -> tensor<16x256x128x128xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<16x256x63x63xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<16x256x63x63xf32>) -> tensor<16x256x63x63xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x256x128x128xf32>, tensor<3x3xf32>) outs (%init: tensor<16x256x63x63xf32>) -> tensor<16x256x63x63xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x256x63x63xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x256x63x63xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 16, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 63, 1], ["%arg6", 0, 63, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 350511740}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x1024x256x128xf32>, tensor<1x1xf32>) outs (%init: tensor<16x1024x256x128xf32>) -> tensor<16x1024x256x128xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x1024x256x128xf32>, tensor<1x1xf32>) outs (%init: tensor<16x1024x256x128xf32>) -> tensor<16x1024x256x128xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<16x1024x256x128xf32>, %filter: tensor<1x1xf32>, %init: tensor<16x1024x256x128xf32>) -> tensor<16x1024x256x128xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x1024x256x128xf32>, tensor<1x1xf32>) outs (%init: tensor<16x1024x256x128xf32>) -> tensor<16x1024x256x128xf32>\n  return %ret : tensor<16x1024x256x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<16x1024x256x128xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<16x1024x256x128xf32>) -> tensor<16x1024x256x128xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<16x1024x256x128xf32>\n    %1 = bufferization.to_memref %arg2 : memref<16x1024x256x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x1024x256x128xf32>\n    memref.copy %1, %alloc : memref<16x1024x256x128xf32> to memref<16x1024x256x128xf32>\n    affine.for %arg3 = 0 to 16 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 256 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<16x1024x256x128xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x1024x256x128xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x1024x256x128xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<16x1024x256x128xf32>\n    return %2 : tensor<16x1024x256x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x1024x256x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<16x1024x256x128xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<16x1024x256x128xf32>) -> tensor<16x1024x256x128xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<16x1024x256x128xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<16x1024x256x128xf32>) -> tensor<16x1024x256x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x1024x256x128xf32>, tensor<1x1xf32>) outs (%init: tensor<16x1024x256x128xf32>) -> tensor<16x1024x256x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x1024x256x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x1024x256x128xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 16, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 256, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 802194471}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x64x256x256xf32>, tensor<1x1xf32>) outs (%init: tensor<64x64x256x256xf32>) -> tensor<64x64x256x256xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x64x256x256xf32>, tensor<1x1xf32>) outs (%init: tensor<64x64x256x256xf32>) -> tensor<64x64x256x256xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x64x256x256xf32>, %filter: tensor<1x1xf32>, %init: tensor<64x64x256x256xf32>) -> tensor<64x64x256x256xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x64x256x256xf32>, tensor<1x1xf32>) outs (%init: tensor<64x64x256x256xf32>) -> tensor<64x64x256x256xf32>\n  return %ret : tensor<64x64x256x256xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x64x256x256xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<64x64x256x256xf32>) -> tensor<64x64x256x256xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<64x64x256x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<64x64x256x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x64x256x256xf32>\n    memref.copy %1, %alloc : memref<64x64x256x256xf32> to memref<64x64x256x256xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 256 {\n          affine.for %arg6 = 0 to 256 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<64x64x256x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x64x256x256xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x64x256x256xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<64x64x256x256xf32>\n    return %2 : tensor<64x64x256x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<64x64x256x256xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<64x64x256x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<64x64x256x256xf32>) -> tensor<64x64x256x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<64x64x256x256xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<64x64x256x256xf32>) -> tensor<64x64x256x256xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x64x256x256xf32>, tensor<1x1xf32>) outs (%init: tensor<64x64x256x256xf32>) -> tensor<64x64x256x256xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<64x64x256x256xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<64x64x256x256xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 256, 1], ["%arg6", 0, 256, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 389637585}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x8x256x256xf32>, tensor<5x5xf32>) outs (%init: tensor<64x8x252x252xf32>) -> tensor<64x8x252x252xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x8x256x256xf32>, tensor<5x5xf32>) outs (%init: tensor<64x8x252x252xf32>) -> tensor<64x8x252x252xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x8x256x256xf32>, %filter: tensor<5x5xf32>, %init: tensor<64x8x252x252xf32>) -> tensor<64x8x252x252xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x8x256x256xf32>, tensor<5x5xf32>) outs (%init: tensor<64x8x252x252xf32>) -> tensor<64x8x252x252xf32>\n  return %ret : tensor<64x8x252x252xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x8x256x256xf32>, %arg1: tensor<5x5xf32>, %arg2: tensor<64x8x252x252xf32>) -> tensor<64x8x252x252xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<64x8x256x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<64x8x252x252xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x8x252x252xf32>\n    memref.copy %1, %alloc : memref<64x8x252x252xf32> to memref<64x8x252x252xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 8 {\n        affine.for %arg5 = 0 to 252 {\n          affine.for %arg6 = 0 to 252 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<64x8x256x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x8x252x252xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x8x252x252xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<64x8x252x252xf32>\n    return %2 : tensor<64x8x252x252xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<64x8x252x252xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<64x8x256x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<64x8x256x256xf32>) -> tensor<64x8x256x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<5x5xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<5x5xf32>) -> tensor<5x5xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<64x8x252x252xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<64x8x252x252xf32>) -> tensor<64x8x252x252xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x8x256x256xf32>, tensor<5x5xf32>) outs (%init: tensor<64x8x252x252xf32>) -> tensor<64x8x252x252xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<64x8x252x252xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<64x8x252x252xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 8, 1], ["%arg5", 0, 252, 1], ["%arg6", 0, 252, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 2690882412}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x512x128x32xf32>, tensor<7x7xf32>) outs (%init: tensor<4x512x61x13xf32>) -> tensor<4x512x61x13xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x512x128x32xf32>, tensor<7x7xf32>) outs (%init: tensor<4x512x61x13xf32>) -> tensor<4x512x61x13xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<4x512x128x32xf32>, %filter: tensor<7x7xf32>, %init: tensor<4x512x61x13xf32>) -> tensor<4x512x61x13xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x512x128x32xf32>, tensor<7x7xf32>) outs (%init: tensor<4x512x61x13xf32>) -> tensor<4x512x61x13xf32>\n  return %ret : tensor<4x512x61x13xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<4x512x128x32xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<4x512x61x13xf32>) -> tensor<4x512x61x13xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<4x512x128x32xf32>\n    %1 = bufferization.to_memref %arg2 : memref<4x512x61x13xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<4x512x61x13xf32>\n    memref.copy %1, %alloc : memref<4x512x61x13xf32> to memref<4x512x61x13xf32>\n    affine.for %arg3 = 0 to 4 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 61 {\n          affine.for %arg6 = 0 to 13 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<4x512x128x32xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x512x61x13xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x512x61x13xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<4x512x61x13xf32>\n    return %2 : tensor<4x512x61x13xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<4x512x61x13xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<4x512x128x32xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<4x512x128x32xf32>) -> tensor<4x512x128x32xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<4x512x61x13xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<4x512x61x13xf32>) -> tensor<4x512x61x13xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x512x128x32xf32>, tensor<7x7xf32>) outs (%init: tensor<4x512x61x13xf32>) -> tensor<4x512x61x13xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<4x512x61x13xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<4x512x61x13xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 4, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 61, 1], ["%arg6", 0, 13, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 318420451}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x64x512x128xf32>, tensor<7x7xf32>) outs (%init: tensor<8x64x506x122xf32>) -> tensor<8x64x506x122xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x64x512x128xf32>, tensor<7x7xf32>) outs (%init: tensor<8x64x506x122xf32>) -> tensor<8x64x506x122xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<8x64x512x128xf32>, %filter: tensor<7x7xf32>, %init: tensor<8x64x506x122xf32>) -> tensor<8x64x506x122xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x64x512x128xf32>, tensor<7x7xf32>) outs (%init: tensor<8x64x506x122xf32>) -> tensor<8x64x506x122xf32>\n  return %ret : tensor<8x64x506x122xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<8x64x512x128xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<8x64x506x122xf32>) -> tensor<8x64x506x122xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<8x64x512x128xf32>\n    %1 = bufferization.to_memref %arg2 : memref<8x64x506x122xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<8x64x506x122xf32>\n    memref.copy %1, %alloc : memref<8x64x506x122xf32> to memref<8x64x506x122xf32>\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 506 {\n          affine.for %arg6 = 0 to 122 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<8x64x512x128xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x64x506x122xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x64x506x122xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<8x64x506x122xf32>\n    return %2 : tensor<8x64x506x122xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<8x64x506x122xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<8x64x512x128xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<8x64x512x128xf32>) -> tensor<8x64x512x128xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<8x64x506x122xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<8x64x506x122xf32>) -> tensor<8x64x506x122xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x64x512x128xf32>, tensor<7x7xf32>) outs (%init: tensor<8x64x506x122xf32>) -> tensor<8x64x506x122xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<8x64x506x122xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<8x64x506x122xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 8, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 506, 1], ["%arg6", 0, 122, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 6164841550}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x32x32x512xf32>, tensor<5x5xf32>) outs (%init: tensor<64x32x14x254xf32>) -> tensor<64x32x14x254xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x32x32x512xf32>, tensor<5x5xf32>) outs (%init: tensor<64x32x14x254xf32>) -> tensor<64x32x14x254xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x32x32x512xf32>, %filter: tensor<5x5xf32>, %init: tensor<64x32x14x254xf32>) -> tensor<64x32x14x254xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x32x32x512xf32>, tensor<5x5xf32>) outs (%init: tensor<64x32x14x254xf32>) -> tensor<64x32x14x254xf32>\n  return %ret : tensor<64x32x14x254xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x32x32x512xf32>, %arg1: tensor<5x5xf32>, %arg2: tensor<64x32x14x254xf32>) -> tensor<64x32x14x254xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<64x32x32x512xf32>\n    %1 = bufferization.to_memref %arg2 : memref<64x32x14x254xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x32x14x254xf32>\n    memref.copy %1, %alloc : memref<64x32x14x254xf32> to memref<64x32x14x254xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 254 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<64x32x32x512xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x32x14x254xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x32x14x254xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<64x32x14x254xf32>\n    return %2 : tensor<64x32x14x254xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<64x32x14x254xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<64x32x32x512xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<64x32x32x512xf32>) -> tensor<64x32x32x512xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<5x5xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<5x5xf32>) -> tensor<5x5xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<64x32x14x254xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<64x32x14x254xf32>) -> tensor<64x32x14x254xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x32x32x512xf32>, tensor<5x5xf32>) outs (%init: tensor<64x32x14x254xf32>) -> tensor<64x32x14x254xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<64x32x14x254xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<64x32x14x254xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 254, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 607064828}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x1024x256x32xf32>, tensor<1x1xf32>) outs (%init: tensor<8x1024x256x32xf32>) -> tensor<8x1024x256x32xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x1024x256x32xf32>, tensor<1x1xf32>) outs (%init: tensor<8x1024x256x32xf32>) -> tensor<8x1024x256x32xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<8x1024x256x32xf32>, %filter: tensor<1x1xf32>, %init: tensor<8x1024x256x32xf32>) -> tensor<8x1024x256x32xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x1024x256x32xf32>, tensor<1x1xf32>) outs (%init: tensor<8x1024x256x32xf32>) -> tensor<8x1024x256x32xf32>\n  return %ret : tensor<8x1024x256x32xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<8x1024x256x32xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<8x1024x256x32xf32>) -> tensor<8x1024x256x32xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<8x1024x256x32xf32>\n    %1 = bufferization.to_memref %arg2 : memref<8x1024x256x32xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<8x1024x256x32xf32>\n    memref.copy %1, %alloc : memref<8x1024x256x32xf32> to memref<8x1024x256x32xf32>\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 256 {\n          affine.for %arg6 = 0 to 32 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<8x1024x256x32xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x1024x256x32xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x1024x256x32xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<8x1024x256x32xf32>\n    return %2 : tensor<8x1024x256x32xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<8x1024x256x32xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<8x1024x256x32xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<8x1024x256x32xf32>) -> tensor<8x1024x256x32xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<8x1024x256x32xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<8x1024x256x32xf32>) -> tensor<8x1024x256x32xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x1024x256x32xf32>, tensor<1x1xf32>) outs (%init: tensor<8x1024x256x32xf32>) -> tensor<8x1024x256x32xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<8x1024x256x32xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<8x1024x256x32xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 8, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 256, 1], ["%arg6", 0, 32, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 98090361}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x1024x32x32xf32>, tensor<1x1xf32>) outs (%init: tensor<4x1024x16x16xf32>) -> tensor<4x1024x16x16xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x1024x32x32xf32>, tensor<1x1xf32>) outs (%init: tensor<4x1024x16x16xf32>) -> tensor<4x1024x16x16xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<4x1024x32x32xf32>, %filter: tensor<1x1xf32>, %init: tensor<4x1024x16x16xf32>) -> tensor<4x1024x16x16xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x1024x32x32xf32>, tensor<1x1xf32>) outs (%init: tensor<4x1024x16x16xf32>) -> tensor<4x1024x16x16xf32>\n  return %ret : tensor<4x1024x16x16xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<4x1024x32x32xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<4x1024x16x16xf32>) -> tensor<4x1024x16x16xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<4x1024x32x32xf32>\n    %1 = bufferization.to_memref %arg2 : memref<4x1024x16x16xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<4x1024x16x16xf32>\n    memref.copy %1, %alloc : memref<4x1024x16x16xf32> to memref<4x1024x16x16xf32>\n    affine.for %arg3 = 0 to 4 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 16 {\n          affine.for %arg6 = 0 to 16 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<4x1024x32x32xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x1024x16x16xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x1024x16x16xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<4x1024x16x16xf32>\n    return %2 : tensor<4x1024x16x16xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<4x1024x16x16xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<4x1024x32x32xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<4x1024x32x32xf32>) -> tensor<4x1024x32x32xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<4x1024x16x16xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<4x1024x16x16xf32>) -> tensor<4x1024x16x16xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x1024x32x32xf32>, tensor<1x1xf32>) outs (%init: tensor<4x1024x16x16xf32>) -> tensor<4x1024x16x16xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<4x1024x16x16xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<4x1024x16x16xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 4, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 16, 1], ["%arg6", 0, 16, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 1526547}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x16x512x128xf32>, tensor<1x1xf32>) outs (%init: tensor<8x16x512x128xf32>) -> tensor<8x16x512x128xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x16x512x128xf32>, tensor<1x1xf32>) outs (%init: tensor<8x16x512x128xf32>) -> tensor<8x16x512x128xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<8x16x512x128xf32>, %filter: tensor<1x1xf32>, %init: tensor<8x16x512x128xf32>) -> tensor<8x16x512x128xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x16x512x128xf32>, tensor<1x1xf32>) outs (%init: tensor<8x16x512x128xf32>) -> tensor<8x16x512x128xf32>\n  return %ret : tensor<8x16x512x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<8x16x512x128xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<8x16x512x128xf32>) -> tensor<8x16x512x128xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<8x16x512x128xf32>\n    %1 = bufferization.to_memref %arg2 : memref<8x16x512x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<8x16x512x128xf32>\n    memref.copy %1, %alloc : memref<8x16x512x128xf32> to memref<8x16x512x128xf32>\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 16 {\n        affine.for %arg5 = 0 to 512 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<8x16x512x128xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x16x512x128xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x16x512x128xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<8x16x512x128xf32>\n    return %2 : tensor<8x16x512x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<8x16x512x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<8x16x512x128xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<8x16x512x128xf32>) -> tensor<8x16x512x128xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<8x16x512x128xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<8x16x512x128xf32>) -> tensor<8x16x512x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x16x512x128xf32>, tensor<1x1xf32>) outs (%init: tensor<8x16x512x128xf32>) -> tensor<8x16x512x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<8x16x512x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<8x16x512x128xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 8, 1], ["%arg4", 0, 16, 1], ["%arg5", 0, 512, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 12585158}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x256x64x128xf32>, tensor<7x7xf32>) outs (%init: tensor<4x256x58x122xf32>) -> tensor<4x256x58x122xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x256x64x128xf32>, tensor<7x7xf32>) outs (%init: tensor<4x256x58x122xf32>) -> tensor<4x256x58x122xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<4x256x64x128xf32>, %filter: tensor<7x7xf32>, %init: tensor<4x256x58x122xf32>) -> tensor<4x256x58x122xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x256x64x128xf32>, tensor<7x7xf32>) outs (%init: tensor<4x256x58x122xf32>) -> tensor<4x256x58x122xf32>\n  return %ret : tensor<4x256x58x122xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<4x256x64x128xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<4x256x58x122xf32>) -> tensor<4x256x58x122xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<4x256x64x128xf32>\n    %1 = bufferization.to_memref %arg2 : memref<4x256x58x122xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<4x256x58x122xf32>\n    memref.copy %1, %alloc : memref<4x256x58x122xf32> to memref<4x256x58x122xf32>\n    affine.for %arg3 = 0 to 4 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 58 {\n          affine.for %arg6 = 0 to 122 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<4x256x64x128xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x256x58x122xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x256x58x122xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<4x256x58x122xf32>\n    return %2 : tensor<4x256x58x122xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<4x256x58x122xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<4x256x64x128xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<4x256x64x128xf32>) -> tensor<4x256x64x128xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<4x256x58x122xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<4x256x58x122xf32>) -> tensor<4x256x58x122xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x256x64x128xf32>, tensor<7x7xf32>) outs (%init: tensor<4x256x58x122xf32>) -> tensor<4x256x58x122xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<4x256x58x122xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<4x256x58x122xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 4, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 58, 1], ["%arg6", 0, 122, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 1413205407}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x8x64x64xf32>, tensor<5x5xf32>) outs (%init: tensor<32x8x60x60xf32>) -> tensor<32x8x60x60xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x8x64x64xf32>, tensor<5x5xf32>) outs (%init: tensor<32x8x60x60xf32>) -> tensor<32x8x60x60xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x8x64x64xf32>, %filter: tensor<5x5xf32>, %init: tensor<32x8x60x60xf32>) -> tensor<32x8x60x60xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x8x64x64xf32>, tensor<5x5xf32>) outs (%init: tensor<32x8x60x60xf32>) -> tensor<32x8x60x60xf32>\n  return %ret : tensor<32x8x60x60xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x8x64x64xf32>, %arg1: tensor<5x5xf32>, %arg2: tensor<32x8x60x60xf32>) -> tensor<32x8x60x60xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<32x8x64x64xf32>\n    %1 = bufferization.to_memref %arg2 : memref<32x8x60x60xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x8x60x60xf32>\n    memref.copy %1, %alloc : memref<32x8x60x60xf32> to memref<32x8x60x60xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 8 {\n        affine.for %arg5 = 0 to 60 {\n          affine.for %arg6 = 0 to 60 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<32x8x64x64xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x8x60x60xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x8x60x60xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<32x8x60x60xf32>\n    return %2 : tensor<32x8x60x60xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<32x8x60x60xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<32x8x64x64xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<32x8x64x64xf32>) -> tensor<32x8x64x64xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<5x5xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<5x5xf32>) -> tensor<5x5xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<32x8x60x60xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<32x8x60x60xf32>) -> tensor<32x8x60x60xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x8x64x64xf32>, tensor<5x5xf32>) outs (%init: tensor<32x8x60x60xf32>) -> tensor<32x8x60x60xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<32x8x60x60xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<32x8x60x60xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 8, 1], ["%arg5", 0, 60, 1], ["%arg6", 0, 60, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 76141671}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x16x256x128xf32>, tensor<5x5xf32>) outs (%init: tensor<8x16x252x124xf32>) -> tensor<8x16x252x124xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x16x256x128xf32>, tensor<5x5xf32>) outs (%init: tensor<8x16x252x124xf32>) -> tensor<8x16x252x124xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<8x16x256x128xf32>, %filter: tensor<5x5xf32>, %init: tensor<8x16x252x124xf32>) -> tensor<8x16x252x124xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x16x256x128xf32>, tensor<5x5xf32>) outs (%init: tensor<8x16x252x124xf32>) -> tensor<8x16x252x124xf32>\n  return %ret : tensor<8x16x252x124xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<8x16x256x128xf32>, %arg1: tensor<5x5xf32>, %arg2: tensor<8x16x252x124xf32>) -> tensor<8x16x252x124xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<8x16x256x128xf32>\n    %1 = bufferization.to_memref %arg2 : memref<8x16x252x124xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<8x16x252x124xf32>\n    memref.copy %1, %alloc : memref<8x16x252x124xf32> to memref<8x16x252x124xf32>\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 16 {\n        affine.for %arg5 = 0 to 252 {\n          affine.for %arg6 = 0 to 124 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<8x16x256x128xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x16x252x124xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x16x252x124xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<8x16x252x124xf32>\n    return %2 : tensor<8x16x252x124xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<8x16x252x124xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<8x16x256x128xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<8x16x256x128xf32>) -> tensor<8x16x256x128xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<5x5xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<5x5xf32>) -> tensor<5x5xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<8x16x252x124xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<8x16x252x124xf32>) -> tensor<8x16x252x124xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x16x256x128xf32>, tensor<5x5xf32>) outs (%init: tensor<8x16x252x124xf32>) -> tensor<8x16x252x124xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<8x16x252x124xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<8x16x252x124xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 8, 1], ["%arg4", 0, 16, 1], ["%arg5", 0, 252, 1], ["%arg6", 0, 124, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 331149425}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x64x128x512xf32>, tensor<5x5xf32>) outs (%init: tensor<64x64x62x254xf32>) -> tensor<64x64x62x254xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x64x128x512xf32>, tensor<5x5xf32>) outs (%init: tensor<64x64x62x254xf32>) -> tensor<64x64x62x254xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x64x128x512xf32>, %filter: tensor<5x5xf32>, %init: tensor<64x64x62x254xf32>) -> tensor<64x64x62x254xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x64x128x512xf32>, tensor<5x5xf32>) outs (%init: tensor<64x64x62x254xf32>) -> tensor<64x64x62x254xf32>\n  return %ret : tensor<64x64x62x254xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x64x128x512xf32>, %arg1: tensor<5x5xf32>, %arg2: tensor<64x64x62x254xf32>) -> tensor<64x64x62x254xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<64x64x128x512xf32>\n    %1 = bufferization.to_memref %arg2 : memref<64x64x62x254xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x64x62x254xf32>\n    memref.copy %1, %alloc : memref<64x64x62x254xf32> to memref<64x64x62x254xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 62 {\n          affine.for %arg6 = 0 to 254 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<64x64x128x512xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x64x62x254xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x64x62x254xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<64x64x62x254xf32>\n    return %2 : tensor<64x64x62x254xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<64x64x62x254xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<64x64x128x512xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<64x64x128x512xf32>) -> tensor<64x64x128x512xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<5x5xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<5x5xf32>) -> tensor<5x5xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<64x64x62x254xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<64x64x62x254xf32>) -> tensor<64x64x62x254xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x64x128x512xf32>, tensor<5x5xf32>) outs (%init: tensor<64x64x62x254xf32>) -> tensor<64x64x62x254xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<64x64x62x254xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<64x64x62x254xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 62, 1], ["%arg6", 0, 254, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 5381243958}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x8x256x512xf32>, tensor<3x3xf32>) outs (%init: tensor<16x8x127x255xf32>) -> tensor<16x8x127x255xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x8x256x512xf32>, tensor<3x3xf32>) outs (%init: tensor<16x8x127x255xf32>) -> tensor<16x8x127x255xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<16x8x256x512xf32>, %filter: tensor<3x3xf32>, %init: tensor<16x8x127x255xf32>) -> tensor<16x8x127x255xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x8x256x512xf32>, tensor<3x3xf32>) outs (%init: tensor<16x8x127x255xf32>) -> tensor<16x8x127x255xf32>\n  return %ret : tensor<16x8x127x255xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<16x8x256x512xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<16x8x127x255xf32>) -> tensor<16x8x127x255xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<16x8x256x512xf32>\n    %1 = bufferization.to_memref %arg2 : memref<16x8x127x255xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x8x127x255xf32>\n    memref.copy %1, %alloc : memref<16x8x127x255xf32> to memref<16x8x127x255xf32>\n    affine.for %arg3 = 0 to 16 {\n      affine.for %arg4 = 0 to 8 {\n        affine.for %arg5 = 0 to 127 {\n          affine.for %arg6 = 0 to 255 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<16x8x256x512xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x8x127x255xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x8x127x255xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<16x8x127x255xf32>\n    return %2 : tensor<16x8x127x255xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x8x127x255xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<16x8x256x512xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<16x8x256x512xf32>) -> tensor<16x8x256x512xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<16x8x127x255xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<16x8x127x255xf32>) -> tensor<16x8x127x255xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x8x256x512xf32>, tensor<3x3xf32>) outs (%init: tensor<16x8x127x255xf32>) -> tensor<16x8x127x255xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x8x127x255xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x8x127x255xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 16, 1], ["%arg4", 0, 8, 1], ["%arg5", 0, 127, 1], ["%arg6", 0, 255, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 89376137}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x1024x256x64xf32>, tensor<3x3xf32>) outs (%init: tensor<64x1024x127x31xf32>) -> tensor<64x1024x127x31xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x1024x256x64xf32>, tensor<3x3xf32>) outs (%init: tensor<64x1024x127x31xf32>) -> tensor<64x1024x127x31xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x1024x256x64xf32>, %filter: tensor<3x3xf32>, %init: tensor<64x1024x127x31xf32>) -> tensor<64x1024x127x31xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x1024x256x64xf32>, tensor<3x3xf32>) outs (%init: tensor<64x1024x127x31xf32>) -> tensor<64x1024x127x31xf32>\n  return %ret : tensor<64x1024x127x31xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x1024x256x64xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<64x1024x127x31xf32>) -> tensor<64x1024x127x31xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<64x1024x256x64xf32>\n    %1 = bufferization.to_memref %arg2 : memref<64x1024x127x31xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x1024x127x31xf32>\n    memref.copy %1, %alloc : memref<64x1024x127x31xf32> to memref<64x1024x127x31xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 127 {\n          affine.for %arg6 = 0 to 31 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<64x1024x256x64xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x1024x127x31xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x1024x127x31xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<64x1024x127x31xf32>\n    return %2 : tensor<64x1024x127x31xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<64x1024x127x31xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<64x1024x256x64xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<64x1024x256x64xf32>) -> tensor<64x1024x256x64xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<64x1024x127x31xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<64x1024x127x31xf32>) -> tensor<64x1024x127x31xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x1024x256x64xf32>, tensor<3x3xf32>) outs (%init: tensor<64x1024x127x31xf32>) -> tensor<64x1024x127x31xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<64x1024x127x31xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<64x1024x127x31xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 127, 1], ["%arg6", 0, 31, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 5591851399}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x8x64x32xf32>, tensor<7x7xf32>) outs (%init: tensor<64x8x58x26xf32>) -> tensor<64x8x58x26xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x8x64x32xf32>, tensor<7x7xf32>) outs (%init: tensor<64x8x58x26xf32>) -> tensor<64x8x58x26xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x8x64x32xf32>, %filter: tensor<7x7xf32>, %init: tensor<64x8x58x26xf32>) -> tensor<64x8x58x26xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x8x64x32xf32>, tensor<7x7xf32>) outs (%init: tensor<64x8x58x26xf32>) -> tensor<64x8x58x26xf32>\n  return %ret : tensor<64x8x58x26xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x8x64x32xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<64x8x58x26xf32>) -> tensor<64x8x58x26xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<64x8x64x32xf32>\n    %1 = bufferization.to_memref %arg2 : memref<64x8x58x26xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x8x58x26xf32>\n    memref.copy %1, %alloc : memref<64x8x58x26xf32> to memref<64x8x58x26xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 8 {\n        affine.for %arg5 = 0 to 58 {\n          affine.for %arg6 = 0 to 26 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<64x8x64x32xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x8x58x26xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x8x58x26xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<64x8x58x26xf32>\n    return %2 : tensor<64x8x58x26xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<64x8x58x26xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<64x8x64x32xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<64x8x64x32xf32>) -> tensor<64x8x64x32xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<64x8x58x26xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<64x8x58x26xf32>) -> tensor<64x8x58x26xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x8x64x32xf32>, tensor<7x7xf32>) outs (%init: tensor<64x8x58x26xf32>) -> tensor<64x8x58x26xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<64x8x58x26xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<64x8x58x26xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 8, 1], ["%arg5", 0, 58, 1], ["%arg6", 0, 26, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 150741277}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x128x256x32xf32>, tensor<7x7xf32>) outs (%init: tensor<4x128x125x13xf32>) -> tensor<4x128x125x13xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x128x256x32xf32>, tensor<7x7xf32>) outs (%init: tensor<4x128x125x13xf32>) -> tensor<4x128x125x13xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<4x128x256x32xf32>, %filter: tensor<7x7xf32>, %init: tensor<4x128x125x13xf32>) -> tensor<4x128x125x13xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x128x256x32xf32>, tensor<7x7xf32>) outs (%init: tensor<4x128x125x13xf32>) -> tensor<4x128x125x13xf32>\n  return %ret : tensor<4x128x125x13xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<4x128x256x32xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<4x128x125x13xf32>) -> tensor<4x128x125x13xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<4x128x256x32xf32>\n    %1 = bufferization.to_memref %arg2 : memref<4x128x125x13xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<4x128x125x13xf32>\n    memref.copy %1, %alloc : memref<4x128x125x13xf32> to memref<4x128x125x13xf32>\n    affine.for %arg3 = 0 to 4 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 125 {\n          affine.for %arg6 = 0 to 13 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<4x128x256x32xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x128x125x13xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x128x125x13xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<4x128x125x13xf32>\n    return %2 : tensor<4x128x125x13xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<4x128x125x13xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<4x128x256x32xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<4x128x256x32xf32>) -> tensor<4x128x256x32xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<4x128x125x13xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<4x128x125x13xf32>) -> tensor<4x128x125x13xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x128x256x32xf32>, tensor<7x7xf32>) outs (%init: tensor<4x128x125x13xf32>) -> tensor<4x128x125x13xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<4x128x125x13xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<4x128x125x13xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 4, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 125, 1], ["%arg6", 0, 13, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 162644805}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x256x512x256xf32>, tensor<1x1xf32>) outs (%init: tensor<64x256x512x256xf32>) -> tensor<64x256x512x256xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x256x512x256xf32>, tensor<1x1xf32>) outs (%init: tensor<64x256x512x256xf32>) -> tensor<64x256x512x256xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x256x512x256xf32>, %filter: tensor<1x1xf32>, %init: tensor<64x256x512x256xf32>) -> tensor<64x256x512x256xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x256x512x256xf32>, tensor<1x1xf32>) outs (%init: tensor<64x256x512x256xf32>) -> tensor<64x256x512x256xf32>\n  return %ret : tensor<64x256x512x256xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x256x512x256xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<64x256x512x256xf32>) -> tensor<64x256x512x256xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<64x256x512x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<64x256x512x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x256x512x256xf32>\n    memref.copy %1, %alloc : memref<64x256x512x256xf32> to memref<64x256x512x256xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 512 {\n          affine.for %arg6 = 0 to 256 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<64x256x512x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x256x512x256xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x256x512x256xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<64x256x512x256xf32>\n    return %2 : tensor<64x256x512x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<64x256x512x256xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<64x256x512x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<64x256x512x256xf32>) -> tensor<64x256x512x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<64x256x512x256xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<64x256x512x256xf32>) -> tensor<64x256x512x256xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x256x512x256xf32>, tensor<1x1xf32>) outs (%init: tensor<64x256x512x256xf32>) -> tensor<64x256x512x256xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<64x256x512x256xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<64x256x512x256xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 512, 1], ["%arg6", 0, 256, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 3126470491}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x16x64x512xf32>, tensor<7x7xf32>) outs (%init: tensor<64x16x29x253xf32>) -> tensor<64x16x29x253xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x16x64x512xf32>, tensor<7x7xf32>) outs (%init: tensor<64x16x29x253xf32>) -> tensor<64x16x29x253xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x16x64x512xf32>, %filter: tensor<7x7xf32>, %init: tensor<64x16x29x253xf32>) -> tensor<64x16x29x253xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x16x64x512xf32>, tensor<7x7xf32>) outs (%init: tensor<64x16x29x253xf32>) -> tensor<64x16x29x253xf32>\n  return %ret : tensor<64x16x29x253xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x16x64x512xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<64x16x29x253xf32>) -> tensor<64x16x29x253xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<64x16x64x512xf32>\n    %1 = bufferization.to_memref %arg2 : memref<64x16x29x253xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x16x29x253xf32>\n    memref.copy %1, %alloc : memref<64x16x29x253xf32> to memref<64x16x29x253xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 16 {\n        affine.for %arg5 = 0 to 29 {\n          affine.for %arg6 = 0 to 253 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<64x16x64x512xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x16x29x253xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x16x29x253xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<64x16x29x253xf32>\n    return %2 : tensor<64x16x29x253xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<64x16x29x253xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<64x16x64x512xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<64x16x64x512xf32>) -> tensor<64x16x64x512xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<64x16x29x253xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<64x16x29x253xf32>) -> tensor<64x16x29x253xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x16x64x512xf32>, tensor<7x7xf32>) outs (%init: tensor<64x16x29x253xf32>) -> tensor<64x16x29x253xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<64x16x29x253xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<64x16x29x253xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 16, 1], ["%arg5", 0, 29, 1], ["%arg6", 0, 253, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 1466730410}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x256x64x512xf32>, tensor<1x1xf32>) outs (%init: tensor<8x256x64x512xf32>) -> tensor<8x256x64x512xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x256x64x512xf32>, tensor<1x1xf32>) outs (%init: tensor<8x256x64x512xf32>) -> tensor<8x256x64x512xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<8x256x64x512xf32>, %filter: tensor<1x1xf32>, %init: tensor<8x256x64x512xf32>) -> tensor<8x256x64x512xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x256x64x512xf32>, tensor<1x1xf32>) outs (%init: tensor<8x256x64x512xf32>) -> tensor<8x256x64x512xf32>\n  return %ret : tensor<8x256x64x512xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<8x256x64x512xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<8x256x64x512xf32>) -> tensor<8x256x64x512xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<8x256x64x512xf32>\n    %1 = bufferization.to_memref %arg2 : memref<8x256x64x512xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<8x256x64x512xf32>\n    memref.copy %1, %alloc : memref<8x256x64x512xf32> to memref<8x256x64x512xf32>\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 64 {\n          affine.for %arg6 = 0 to 512 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<8x256x64x512xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x256x64x512xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x256x64x512xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<8x256x64x512xf32>\n    return %2 : tensor<8x256x64x512xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<8x256x64x512xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<8x256x64x512xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<8x256x64x512xf32>) -> tensor<8x256x64x512xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<8x256x64x512xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<8x256x64x512xf32>) -> tensor<8x256x64x512xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x256x64x512xf32>, tensor<1x1xf32>) outs (%init: tensor<8x256x64x512xf32>) -> tensor<8x256x64x512xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<8x256x64x512xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<8x256x64x512xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 8, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 64, 1], ["%arg6", 0, 512, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 95312914}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x512x512x256xf32>, tensor<3x3xf32>) outs (%init: tensor<4x512x510x254xf32>) -> tensor<4x512x510x254xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x512x512x256xf32>, tensor<3x3xf32>) outs (%init: tensor<4x512x510x254xf32>) -> tensor<4x512x510x254xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<4x512x512x256xf32>, %filter: tensor<3x3xf32>, %init: tensor<4x512x510x254xf32>) -> tensor<4x512x510x254xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x512x512x256xf32>, tensor<3x3xf32>) outs (%init: tensor<4x512x510x254xf32>) -> tensor<4x512x510x254xf32>\n  return %ret : tensor<4x512x510x254xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<4x512x512x256xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<4x512x510x254xf32>) -> tensor<4x512x510x254xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<4x512x512x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<4x512x510x254xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<4x512x510x254xf32>\n    memref.copy %1, %alloc : memref<4x512x510x254xf32> to memref<4x512x510x254xf32>\n    affine.for %arg3 = 0 to 4 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 510 {\n          affine.for %arg6 = 0 to 254 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<4x512x512x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x512x510x254xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x512x510x254xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<4x512x510x254xf32>\n    return %2 : tensor<4x512x510x254xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<4x512x510x254xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<4x512x512x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<4x512x512x256xf32>) -> tensor<4x512x512x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<4x512x510x254xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<4x512x510x254xf32>) -> tensor<4x512x510x254xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x512x512x256xf32>, tensor<3x3xf32>) outs (%init: tensor<4x512x510x254xf32>) -> tensor<4x512x510x254xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<4x512x510x254xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<4x512x510x254xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 4, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 510, 1], ["%arg6", 0, 254, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 5694214337}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x8x256x256xf32>, tensor<3x3xf32>) outs (%init: tensor<256x8x254x254xf32>) -> tensor<256x8x254x254xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x8x256x256xf32>, tensor<3x3xf32>) outs (%init: tensor<256x8x254x254xf32>) -> tensor<256x8x254x254xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x8x256x256xf32>, %filter: tensor<3x3xf32>, %init: tensor<256x8x254x254xf32>) -> tensor<256x8x254x254xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x8x256x256xf32>, tensor<3x3xf32>) outs (%init: tensor<256x8x254x254xf32>) -> tensor<256x8x254x254xf32>\n  return %ret : tensor<256x8x254x254xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x8x256x256xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<256x8x254x254xf32>) -> tensor<256x8x254x254xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<256x8x256x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<256x8x254x254xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x8x254x254xf32>\n    memref.copy %1, %alloc : memref<256x8x254x254xf32> to memref<256x8x254x254xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 8 {\n        affine.for %arg5 = 0 to 254 {\n          affine.for %arg6 = 0 to 254 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<256x8x256x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x8x254x254xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x8x254x254xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<256x8x254x254xf32>\n    return %2 : tensor<256x8x254x254xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x8x254x254xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x8x256x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x8x256x256xf32>) -> tensor<256x8x256x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x8x254x254xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x8x254x254xf32>) -> tensor<256x8x254x254xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x8x256x256xf32>, tensor<3x3xf32>) outs (%init: tensor<256x8x254x254xf32>) -> tensor<256x8x254x254xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x8x254x254xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x8x254x254xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 8, 1], ["%arg5", 0, 254, 1], ["%arg6", 0, 254, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 2836093465}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x128x32x512xf32>, tensor<5x5xf32>) outs (%init: tensor<4x128x14x254xf32>) -> tensor<4x128x14x254xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x128x32x512xf32>, tensor<5x5xf32>) outs (%init: tensor<4x128x14x254xf32>) -> tensor<4x128x14x254xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<4x128x32x512xf32>, %filter: tensor<5x5xf32>, %init: tensor<4x128x14x254xf32>) -> tensor<4x128x14x254xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x128x32x512xf32>, tensor<5x5xf32>) outs (%init: tensor<4x128x14x254xf32>) -> tensor<4x128x14x254xf32>\n  return %ret : tensor<4x128x14x254xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<4x128x32x512xf32>, %arg1: tensor<5x5xf32>, %arg2: tensor<4x128x14x254xf32>) -> tensor<4x128x14x254xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<4x128x32x512xf32>\n    %1 = bufferization.to_memref %arg2 : memref<4x128x14x254xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<4x128x14x254xf32>\n    memref.copy %1, %alloc : memref<4x128x14x254xf32> to memref<4x128x14x254xf32>\n    affine.for %arg3 = 0 to 4 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 14 {\n          affine.for %arg6 = 0 to 254 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<4x128x32x512xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x128x14x254xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x128x14x254xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<4x128x14x254xf32>\n    return %2 : tensor<4x128x14x254xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<4x128x14x254xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<4x128x32x512xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<4x128x32x512xf32>) -> tensor<4x128x32x512xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<5x5xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<5x5xf32>) -> tensor<5x5xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<4x128x14x254xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<4x128x14x254xf32>) -> tensor<4x128x14x254xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x128x32x512xf32>, tensor<5x5xf32>) outs (%init: tensor<4x128x14x254xf32>) -> tensor<4x128x14x254xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<4x128x14x254xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<4x128x14x254xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 4, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 14, 1], ["%arg6", 0, 254, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 151223024}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x512x64x256xf32>, tensor<1x1xf32>) outs (%init: tensor<8x512x32x128xf32>) -> tensor<8x512x32x128xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x512x64x256xf32>, tensor<1x1xf32>) outs (%init: tensor<8x512x32x128xf32>) -> tensor<8x512x32x128xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<8x512x64x256xf32>, %filter: tensor<1x1xf32>, %init: tensor<8x512x32x128xf32>) -> tensor<8x512x32x128xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x512x64x256xf32>, tensor<1x1xf32>) outs (%init: tensor<8x512x32x128xf32>) -> tensor<8x512x32x128xf32>\n  return %ret : tensor<8x512x32x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<8x512x64x256xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<8x512x32x128xf32>) -> tensor<8x512x32x128xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<8x512x64x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<8x512x32x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<8x512x32x128xf32>\n    memref.copy %1, %alloc : memref<8x512x32x128xf32> to memref<8x512x32x128xf32>\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 32 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<8x512x64x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x512x32x128xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x512x32x128xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<8x512x32x128xf32>\n    return %2 : tensor<8x512x32x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<8x512x32x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<8x512x64x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<8x512x64x256xf32>) -> tensor<8x512x64x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<8x512x32x128xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<8x512x32x128xf32>) -> tensor<8x512x32x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x512x64x256xf32>, tensor<1x1xf32>) outs (%init: tensor<8x512x32x128xf32>) -> tensor<8x512x32x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<8x512x32x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<8x512x32x128xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 8, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 32, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 35939985}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x256x512x64xf32>, tensor<7x7xf32>) outs (%init: tensor<16x256x253x29xf32>) -> tensor<16x256x253x29xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x256x512x64xf32>, tensor<7x7xf32>) outs (%init: tensor<16x256x253x29xf32>) -> tensor<16x256x253x29xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<16x256x512x64xf32>, %filter: tensor<7x7xf32>, %init: tensor<16x256x253x29xf32>) -> tensor<16x256x253x29xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x256x512x64xf32>, tensor<7x7xf32>) outs (%init: tensor<16x256x253x29xf32>) -> tensor<16x256x253x29xf32>\n  return %ret : tensor<16x256x253x29xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<16x256x512x64xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<16x256x253x29xf32>) -> tensor<16x256x253x29xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<16x256x512x64xf32>\n    %1 = bufferization.to_memref %arg2 : memref<16x256x253x29xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x256x253x29xf32>\n    memref.copy %1, %alloc : memref<16x256x253x29xf32> to memref<16x256x253x29xf32>\n    affine.for %arg3 = 0 to 16 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 253 {\n          affine.for %arg6 = 0 to 29 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<16x256x512x64xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x256x253x29xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x256x253x29xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<16x256x253x29xf32>\n    return %2 : tensor<16x256x253x29xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x256x253x29xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<16x256x512x64xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<16x256x512x64xf32>) -> tensor<16x256x512x64xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<16x256x253x29xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<16x256x253x29xf32>) -> tensor<16x256x253x29xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x256x512x64xf32>, tensor<7x7xf32>) outs (%init: tensor<16x256x253x29xf32>) -> tensor<16x256x253x29xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x256x253x29xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x256x253x29xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 16, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 253, 1], ["%arg6", 0, 29, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 5988589826}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x128x512x32xf32>, tensor<1x1xf32>) outs (%init: tensor<64x128x256x16xf32>) -> tensor<64x128x256x16xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x128x512x32xf32>, tensor<1x1xf32>) outs (%init: tensor<64x128x256x16xf32>) -> tensor<64x128x256x16xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x128x512x32xf32>, %filter: tensor<1x1xf32>, %init: tensor<64x128x256x16xf32>) -> tensor<64x128x256x16xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x128x512x32xf32>, tensor<1x1xf32>) outs (%init: tensor<64x128x256x16xf32>) -> tensor<64x128x256x16xf32>\n  return %ret : tensor<64x128x256x16xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x128x512x32xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<64x128x256x16xf32>) -> tensor<64x128x256x16xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<64x128x512x32xf32>\n    %1 = bufferization.to_memref %arg2 : memref<64x128x256x16xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x128x256x16xf32>\n    memref.copy %1, %alloc : memref<64x128x256x16xf32> to memref<64x128x256x16xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 256 {\n          affine.for %arg6 = 0 to 16 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<64x128x512x32xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x128x256x16xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x128x256x16xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<64x128x256x16xf32>\n    return %2 : tensor<64x128x256x16xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<64x128x256x16xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<64x128x512x32xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<64x128x512x32xf32>) -> tensor<64x128x512x32xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<64x128x256x16xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<64x128x256x16xf32>) -> tensor<64x128x256x16xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x128x512x32xf32>, tensor<1x1xf32>) outs (%init: tensor<64x128x256x16xf32>) -> tensor<64x128x256x16xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<64x128x256x16xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<64x128x256x16xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 256, 1], ["%arg6", 0, 16, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 76986902}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x256x32x32xf32>, tensor<7x7xf32>) outs (%init: tensor<32x256x13x13xf32>) -> tensor<32x256x13x13xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x256x32x32xf32>, tensor<7x7xf32>) outs (%init: tensor<32x256x13x13xf32>) -> tensor<32x256x13x13xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x256x32x32xf32>, %filter: tensor<7x7xf32>, %init: tensor<32x256x13x13xf32>) -> tensor<32x256x13x13xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x256x32x32xf32>, tensor<7x7xf32>) outs (%init: tensor<32x256x13x13xf32>) -> tensor<32x256x13x13xf32>\n  return %ret : tensor<32x256x13x13xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x256x32x32xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<32x256x13x13xf32>) -> tensor<32x256x13x13xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<32x256x32x32xf32>\n    %1 = bufferization.to_memref %arg2 : memref<32x256x13x13xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x256x13x13xf32>\n    memref.copy %1, %alloc : memref<32x256x13x13xf32> to memref<32x256x13x13xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 13 {\n          affine.for %arg6 = 0 to 13 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<32x256x32x32xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x256x13x13xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x256x13x13xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<32x256x13x13xf32>\n    return %2 : tensor<32x256x13x13xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<32x256x13x13xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<32x256x32x32xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<32x256x32x32xf32>) -> tensor<32x256x32x32xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<32x256x13x13xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<32x256x13x13xf32>) -> tensor<32x256x13x13xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x256x32x32xf32>, tensor<7x7xf32>) outs (%init: tensor<32x256x13x13xf32>) -> tensor<32x256x13x13xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<32x256x13x13xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<32x256x13x13xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 13, 1], ["%arg6", 0, 13, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 271542669}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x512x256x32xf32>, tensor<1x1xf32>) outs (%init: tensor<64x512x256x32xf32>) -> tensor<64x512x256x32xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x512x256x32xf32>, tensor<1x1xf32>) outs (%init: tensor<64x512x256x32xf32>) -> tensor<64x512x256x32xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x512x256x32xf32>, %filter: tensor<1x1xf32>, %init: tensor<64x512x256x32xf32>) -> tensor<64x512x256x32xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x512x256x32xf32>, tensor<1x1xf32>) outs (%init: tensor<64x512x256x32xf32>) -> tensor<64x512x256x32xf32>\n  return %ret : tensor<64x512x256x32xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x512x256x32xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<64x512x256x32xf32>) -> tensor<64x512x256x32xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<64x512x256x32xf32>\n    %1 = bufferization.to_memref %arg2 : memref<64x512x256x32xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x512x256x32xf32>\n    memref.copy %1, %alloc : memref<64x512x256x32xf32> to memref<64x512x256x32xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 256 {\n          affine.for %arg6 = 0 to 32 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<64x512x256x32xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x512x256x32xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x512x256x32xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<64x512x256x32xf32>\n    return %2 : tensor<64x512x256x32xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<64x512x256x32xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<64x512x256x32xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<64x512x256x32xf32>) -> tensor<64x512x256x32xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<64x512x256x32xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<64x512x256x32xf32>) -> tensor<64x512x256x32xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x512x256x32xf32>, tensor<1x1xf32>) outs (%init: tensor<64x512x256x32xf32>) -> tensor<64x512x256x32xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<64x512x256x32xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<64x512x256x32xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 256, 1], ["%arg6", 0, 32, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 390323307}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x8x512x32xf32>, tensor<7x7xf32>) outs (%init: tensor<256x8x253x13xf32>) -> tensor<256x8x253x13xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x8x512x32xf32>, tensor<7x7xf32>) outs (%init: tensor<256x8x253x13xf32>) -> tensor<256x8x253x13xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x8x512x32xf32>, %filter: tensor<7x7xf32>, %init: tensor<256x8x253x13xf32>) -> tensor<256x8x253x13xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x8x512x32xf32>, tensor<7x7xf32>) outs (%init: tensor<256x8x253x13xf32>) -> tensor<256x8x253x13xf32>\n  return %ret : tensor<256x8x253x13xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x8x512x32xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<256x8x253x13xf32>) -> tensor<256x8x253x13xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<256x8x512x32xf32>\n    %1 = bufferization.to_memref %arg2 : memref<256x8x253x13xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x8x253x13xf32>\n    memref.copy %1, %alloc : memref<256x8x253x13xf32> to memref<256x8x253x13xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 8 {\n        affine.for %arg5 = 0 to 253 {\n          affine.for %arg6 = 0 to 13 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<256x8x512x32xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x8x253x13xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x8x253x13xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<256x8x253x13xf32>\n    return %2 : tensor<256x8x253x13xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x8x253x13xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x8x512x32xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x8x512x32xf32>) -> tensor<256x8x512x32xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x8x253x13xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x8x253x13xf32>) -> tensor<256x8x253x13xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x8x512x32xf32>, tensor<7x7xf32>) outs (%init: tensor<256x8x253x13xf32>) -> tensor<256x8x253x13xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x8x253x13xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x8x253x13xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 8, 1], ["%arg5", 0, 253, 1], ["%arg6", 0, 13, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 1318425582}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x256x128x256xf32>, tensor<5x5xf32>) outs (%init: tensor<16x256x62x126xf32>) -> tensor<16x256x62x126xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x256x128x256xf32>, tensor<5x5xf32>) outs (%init: tensor<16x256x62x126xf32>) -> tensor<16x256x62x126xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<16x256x128x256xf32>, %filter: tensor<5x5xf32>, %init: tensor<16x256x62x126xf32>) -> tensor<16x256x62x126xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x256x128x256xf32>, tensor<5x5xf32>) outs (%init: tensor<16x256x62x126xf32>) -> tensor<16x256x62x126xf32>\n  return %ret : tensor<16x256x62x126xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<16x256x128x256xf32>, %arg1: tensor<5x5xf32>, %arg2: tensor<16x256x62x126xf32>) -> tensor<16x256x62x126xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<16x256x128x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<16x256x62x126xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x256x62x126xf32>\n    memref.copy %1, %alloc : memref<16x256x62x126xf32> to memref<16x256x62x126xf32>\n    affine.for %arg3 = 0 to 16 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 62 {\n          affine.for %arg6 = 0 to 126 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<16x256x128x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x256x62x126xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x256x62x126xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<16x256x62x126xf32>\n    return %2 : tensor<16x256x62x126xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x256x62x126xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<16x256x128x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<16x256x128x256xf32>) -> tensor<16x256x128x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<5x5xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<5x5xf32>) -> tensor<5x5xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<16x256x62x126xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<16x256x62x126xf32>) -> tensor<16x256x62x126xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x256x128x256xf32>, tensor<5x5xf32>) outs (%init: tensor<16x256x62x126xf32>) -> tensor<16x256x62x126xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x256x62x126xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x256x62x126xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 16, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 62, 1], ["%arg6", 0, 126, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 2668321188}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x128x128x128xf32>, tensor<3x3xf32>) outs (%init: tensor<32x128x63x63xf32>) -> tensor<32x128x63x63xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x128x128x128xf32>, tensor<3x3xf32>) outs (%init: tensor<32x128x63x63xf32>) -> tensor<32x128x63x63xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x128x128x128xf32>, %filter: tensor<3x3xf32>, %init: tensor<32x128x63x63xf32>) -> tensor<32x128x63x63xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x128x128x128xf32>, tensor<3x3xf32>) outs (%init: tensor<32x128x63x63xf32>) -> tensor<32x128x63x63xf32>\n  return %ret : tensor<32x128x63x63xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x128x128x128xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<32x128x63x63xf32>) -> tensor<32x128x63x63xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<32x128x128x128xf32>\n    %1 = bufferization.to_memref %arg2 : memref<32x128x63x63xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x128x63x63xf32>\n    memref.copy %1, %alloc : memref<32x128x63x63xf32> to memref<32x128x63x63xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 63 {\n          affine.for %arg6 = 0 to 63 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<32x128x128x128xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x128x63x63xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x128x63x63xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<32x128x63x63xf32>\n    return %2 : tensor<32x128x63x63xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<32x128x63x63xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<32x128x128x128xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<32x128x128x128xf32>) -> tensor<32x128x128x128xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<32x128x63x63xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<32x128x63x63xf32>) -> tensor<32x128x63x63xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x128x128x128xf32>, tensor<3x3xf32>) outs (%init: tensor<32x128x63x63xf32>) -> tensor<32x128x63x63xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<32x128x63x63xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<32x128x63x63xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 63, 1], ["%arg6", 0, 63, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 350423317}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x1024x256x128xf32>, tensor<3x3xf32>) outs (%init: tensor<8x1024x127x63xf32>) -> tensor<8x1024x127x63xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x1024x256x128xf32>, tensor<3x3xf32>) outs (%init: tensor<8x1024x127x63xf32>) -> tensor<8x1024x127x63xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<8x1024x256x128xf32>, %filter: tensor<3x3xf32>, %init: tensor<8x1024x127x63xf32>) -> tensor<8x1024x127x63xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x1024x256x128xf32>, tensor<3x3xf32>) outs (%init: tensor<8x1024x127x63xf32>) -> tensor<8x1024x127x63xf32>\n  return %ret : tensor<8x1024x127x63xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<8x1024x256x128xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<8x1024x127x63xf32>) -> tensor<8x1024x127x63xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<8x1024x256x128xf32>\n    %1 = bufferization.to_memref %arg2 : memref<8x1024x127x63xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<8x1024x127x63xf32>\n    memref.copy %1, %alloc : memref<8x1024x127x63xf32> to memref<8x1024x127x63xf32>\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 127 {\n          affine.for %arg6 = 0 to 63 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<8x1024x256x128xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x1024x127x63xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x1024x127x63xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<8x1024x127x63xf32>\n    return %2 : tensor<8x1024x127x63xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<8x1024x127x63xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<8x1024x256x128xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<8x1024x256x128xf32>) -> tensor<8x1024x256x128xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<8x1024x127x63xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<8x1024x127x63xf32>) -> tensor<8x1024x127x63xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x1024x256x128xf32>, tensor<3x3xf32>) outs (%init: tensor<8x1024x127x63xf32>) -> tensor<8x1024x127x63xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<8x1024x127x63xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<8x1024x127x63xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 8, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 127, 1], ["%arg6", 0, 63, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 1411982869}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x32x256x64xf32>, tensor<7x7xf32>) outs (%init: tensor<8x32x250x58xf32>) -> tensor<8x32x250x58xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x32x256x64xf32>, tensor<7x7xf32>) outs (%init: tensor<8x32x250x58xf32>) -> tensor<8x32x250x58xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<8x32x256x64xf32>, %filter: tensor<7x7xf32>, %init: tensor<8x32x250x58xf32>) -> tensor<8x32x250x58xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x32x256x64xf32>, tensor<7x7xf32>) outs (%init: tensor<8x32x250x58xf32>) -> tensor<8x32x250x58xf32>\n  return %ret : tensor<8x32x250x58xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<8x32x256x64xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<8x32x250x58xf32>) -> tensor<8x32x250x58xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<8x32x256x64xf32>\n    %1 = bufferization.to_memref %arg2 : memref<8x32x250x58xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<8x32x250x58xf32>\n    memref.copy %1, %alloc : memref<8x32x250x58xf32> to memref<8x32x250x58xf32>\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 250 {\n          affine.for %arg6 = 0 to 58 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<8x32x256x64xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x32x250x58xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x32x250x58xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<8x32x250x58xf32>\n    return %2 : tensor<8x32x250x58xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<8x32x250x58xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<8x32x256x64xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<8x32x256x64xf32>) -> tensor<8x32x256x64xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<8x32x250x58xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<8x32x250x58xf32>) -> tensor<8x32x250x58xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x32x256x64xf32>, tensor<7x7xf32>) outs (%init: tensor<8x32x250x58xf32>) -> tensor<8x32x250x58xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<8x32x250x58xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<8x32x250x58xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 8, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 250, 1], ["%arg6", 0, 58, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 723960068}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x1024x128x256xf32>, tensor<3x3xf32>) outs (%init: tensor<8x1024x126x254xf32>) -> tensor<8x1024x126x254xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x1024x128x256xf32>, tensor<3x3xf32>) outs (%init: tensor<8x1024x126x254xf32>) -> tensor<8x1024x126x254xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<8x1024x128x256xf32>, %filter: tensor<3x3xf32>, %init: tensor<8x1024x126x254xf32>) -> tensor<8x1024x126x254xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x1024x128x256xf32>, tensor<3x3xf32>) outs (%init: tensor<8x1024x126x254xf32>) -> tensor<8x1024x126x254xf32>\n  return %ret : tensor<8x1024x126x254xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<8x1024x128x256xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<8x1024x126x254xf32>) -> tensor<8x1024x126x254xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<8x1024x128x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<8x1024x126x254xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<8x1024x126x254xf32>\n    memref.copy %1, %alloc : memref<8x1024x126x254xf32> to memref<8x1024x126x254xf32>\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 126 {\n          affine.for %arg6 = 0 to 254 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<8x1024x128x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x1024x126x254xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x1024x126x254xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<8x1024x126x254xf32>\n    return %2 : tensor<8x1024x126x254xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<8x1024x126x254xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<8x1024x128x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<8x1024x128x256xf32>) -> tensor<8x1024x128x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<8x1024x126x254xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<8x1024x126x254xf32>) -> tensor<8x1024x126x254xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x1024x128x256xf32>, tensor<3x3xf32>) outs (%init: tensor<8x1024x126x254xf32>) -> tensor<8x1024x126x254xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<8x1024x126x254xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<8x1024x126x254xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 8, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 126, 1], ["%arg6", 0, 254, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 5630792534}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x256x128x512xf32>, tensor<3x3xf32>) outs (%init: tensor<8x256x126x510xf32>) -> tensor<8x256x126x510xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x256x128x512xf32>, tensor<3x3xf32>) outs (%init: tensor<8x256x126x510xf32>) -> tensor<8x256x126x510xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<8x256x128x512xf32>, %filter: tensor<3x3xf32>, %init: tensor<8x256x126x510xf32>) -> tensor<8x256x126x510xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x256x128x512xf32>, tensor<3x3xf32>) outs (%init: tensor<8x256x126x510xf32>) -> tensor<8x256x126x510xf32>\n  return %ret : tensor<8x256x126x510xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<8x256x128x512xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<8x256x126x510xf32>) -> tensor<8x256x126x510xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<8x256x128x512xf32>\n    %1 = bufferization.to_memref %arg2 : memref<8x256x126x510xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<8x256x126x510xf32>\n    memref.copy %1, %alloc : memref<8x256x126x510xf32> to memref<8x256x126x510xf32>\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 126 {\n          affine.for %arg6 = 0 to 510 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<8x256x128x512xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x256x126x510xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x256x126x510xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<8x256x126x510xf32>\n    return %2 : tensor<8x256x126x510xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<8x256x126x510xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<8x256x128x512xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<8x256x128x512xf32>) -> tensor<8x256x128x512xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<8x256x126x510xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<8x256x126x510xf32>) -> tensor<8x256x126x510xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x256x128x512xf32>, tensor<3x3xf32>) outs (%init: tensor<8x256x126x510xf32>) -> tensor<8x256x126x510xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<8x256x126x510xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<8x256x126x510xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 8, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 126, 1], ["%arg6", 0, 510, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 2824033250}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x32x128x32xf32>, tensor<7x7xf32>) outs (%init: tensor<16x32x61x13xf32>) -> tensor<16x32x61x13xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x32x128x32xf32>, tensor<7x7xf32>) outs (%init: tensor<16x32x61x13xf32>) -> tensor<16x32x61x13xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<16x32x128x32xf32>, %filter: tensor<7x7xf32>, %init: tensor<16x32x61x13xf32>) -> tensor<16x32x61x13xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x32x128x32xf32>, tensor<7x7xf32>) outs (%init: tensor<16x32x61x13xf32>) -> tensor<16x32x61x13xf32>\n  return %ret : tensor<16x32x61x13xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<16x32x128x32xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<16x32x61x13xf32>) -> tensor<16x32x61x13xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<16x32x128x32xf32>\n    %1 = bufferization.to_memref %arg2 : memref<16x32x61x13xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x32x61x13xf32>\n    memref.copy %1, %alloc : memref<16x32x61x13xf32> to memref<16x32x61x13xf32>\n    affine.for %arg3 = 0 to 16 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 61 {\n          affine.for %arg6 = 0 to 13 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<16x32x128x32xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x32x61x13xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x32x61x13xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<16x32x61x13xf32>\n    return %2 : tensor<16x32x61x13xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x32x61x13xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<16x32x128x32xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<16x32x128x32xf32>) -> tensor<16x32x128x32xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<16x32x61x13xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<16x32x61x13xf32>) -> tensor<16x32x61x13xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x32x128x32xf32>, tensor<7x7xf32>) outs (%init: tensor<16x32x61x13xf32>) -> tensor<16x32x61x13xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x32x61x13xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x32x61x13xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 16, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 61, 1], ["%arg6", 0, 13, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 79230926}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x256x256x32xf32>, tensor<7x7xf32>) outs (%init: tensor<8x256x250x26xf32>) -> tensor<8x256x250x26xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x256x256x32xf32>, tensor<7x7xf32>) outs (%init: tensor<8x256x250x26xf32>) -> tensor<8x256x250x26xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<8x256x256x32xf32>, %filter: tensor<7x7xf32>, %init: tensor<8x256x250x26xf32>) -> tensor<8x256x250x26xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x256x256x32xf32>, tensor<7x7xf32>) outs (%init: tensor<8x256x250x26xf32>) -> tensor<8x256x250x26xf32>\n  return %ret : tensor<8x256x250x26xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<8x256x256x32xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<8x256x250x26xf32>) -> tensor<8x256x250x26xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<8x256x256x32xf32>\n    %1 = bufferization.to_memref %arg2 : memref<8x256x250x26xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<8x256x250x26xf32>\n    memref.copy %1, %alloc : memref<8x256x250x26xf32> to memref<8x256x250x26xf32>\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 250 {\n          affine.for %arg6 = 0 to 26 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<8x256x256x32xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x256x250x26xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x256x250x26xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<8x256x250x26xf32>\n    return %2 : tensor<8x256x250x26xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<8x256x250x26xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<8x256x256x32xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<8x256x256x32xf32>) -> tensor<8x256x256x32xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<8x256x250x26xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<8x256x250x26xf32>) -> tensor<8x256x250x26xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x256x256x32xf32>, tensor<7x7xf32>) outs (%init: tensor<8x256x250x26xf32>) -> tensor<8x256x250x26xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<8x256x250x26xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<8x256x250x26xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 8, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 250, 1], ["%arg6", 0, 26, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 2598621230}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x16x32x128xf32>, tensor<7x7xf32>) outs (%init: tensor<256x16x13x61xf32>) -> tensor<256x16x13x61xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x16x32x128xf32>, tensor<7x7xf32>) outs (%init: tensor<256x16x13x61xf32>) -> tensor<256x16x13x61xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x16x32x128xf32>, %filter: tensor<7x7xf32>, %init: tensor<256x16x13x61xf32>) -> tensor<256x16x13x61xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x16x32x128xf32>, tensor<7x7xf32>) outs (%init: tensor<256x16x13x61xf32>) -> tensor<256x16x13x61xf32>\n  return %ret : tensor<256x16x13x61xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x16x32x128xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<256x16x13x61xf32>) -> tensor<256x16x13x61xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<256x16x32x128xf32>\n    %1 = bufferization.to_memref %arg2 : memref<256x16x13x61xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x16x13x61xf32>\n    memref.copy %1, %alloc : memref<256x16x13x61xf32> to memref<256x16x13x61xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 16 {\n        affine.for %arg5 = 0 to 13 {\n          affine.for %arg6 = 0 to 61 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<256x16x32x128xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x16x13x61xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x16x13x61xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<256x16x13x61xf32>\n    return %2 : tensor<256x16x13x61xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x16x13x61xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x16x32x128xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x16x32x128xf32>) -> tensor<256x16x32x128xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x16x13x61xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x16x13x61xf32>) -> tensor<256x16x13x61xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x16x32x128xf32>, tensor<7x7xf32>) outs (%init: tensor<256x16x13x61xf32>) -> tensor<256x16x13x61xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x16x13x61xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x16x13x61xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 16, 1], ["%arg5", 0, 13, 1], ["%arg6", 0, 61, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 634439079}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x8x512x32xf32>, tensor<5x5xf32>) outs (%init: tensor<256x8x508x28xf32>) -> tensor<256x8x508x28xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x8x512x32xf32>, tensor<5x5xf32>) outs (%init: tensor<256x8x508x28xf32>) -> tensor<256x8x508x28xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x8x512x32xf32>, %filter: tensor<5x5xf32>, %init: tensor<256x8x508x28xf32>) -> tensor<256x8x508x28xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x8x512x32xf32>, tensor<5x5xf32>) outs (%init: tensor<256x8x508x28xf32>) -> tensor<256x8x508x28xf32>\n  return %ret : tensor<256x8x508x28xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x8x512x32xf32>, %arg1: tensor<5x5xf32>, %arg2: tensor<256x8x508x28xf32>) -> tensor<256x8x508x28xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<256x8x512x32xf32>\n    %1 = bufferization.to_memref %arg2 : memref<256x8x508x28xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x8x508x28xf32>\n    memref.copy %1, %alloc : memref<256x8x508x28xf32> to memref<256x8x508x28xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 8 {\n        affine.for %arg5 = 0 to 508 {\n          affine.for %arg6 = 0 to 28 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<256x8x512x32xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x8x508x28xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x8x508x28xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<256x8x508x28xf32>\n    return %2 : tensor<256x8x508x28xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x8x508x28xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x8x512x32xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x8x512x32xf32>) -> tensor<256x8x512x32xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<5x5xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<5x5xf32>) -> tensor<5x5xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x8x508x28xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x8x508x28xf32>) -> tensor<256x8x508x28xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x8x512x32xf32>, tensor<5x5xf32>) outs (%init: tensor<256x8x508x28xf32>) -> tensor<256x8x508x28xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x8x508x28xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x8x508x28xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 8, 1], ["%arg5", 0, 508, 1], ["%arg6", 0, 28, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 2418551959}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x256x64x256xf32>, tensor<1x1xf32>) outs (%init: tensor<8x256x32x128xf32>) -> tensor<8x256x32x128xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x256x64x256xf32>, tensor<1x1xf32>) outs (%init: tensor<8x256x32x128xf32>) -> tensor<8x256x32x128xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<8x256x64x256xf32>, %filter: tensor<1x1xf32>, %init: tensor<8x256x32x128xf32>) -> tensor<8x256x32x128xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x256x64x256xf32>, tensor<1x1xf32>) outs (%init: tensor<8x256x32x128xf32>) -> tensor<8x256x32x128xf32>\n  return %ret : tensor<8x256x32x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<8x256x64x256xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<8x256x32x128xf32>) -> tensor<8x256x32x128xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<8x256x64x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<8x256x32x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<8x256x32x128xf32>\n    memref.copy %1, %alloc : memref<8x256x32x128xf32> to memref<8x256x32x128xf32>\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 32 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<8x256x64x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x256x32x128xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x256x32x128xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<8x256x32x128xf32>\n    return %2 : tensor<8x256x32x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<8x256x32x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<8x256x64x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<8x256x64x256xf32>) -> tensor<8x256x64x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<8x256x32x128xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<8x256x32x128xf32>) -> tensor<8x256x32x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x256x64x256xf32>, tensor<1x1xf32>) outs (%init: tensor<8x256x32x128xf32>) -> tensor<8x256x32x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<8x256x32x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<8x256x32x128xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 8, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 32, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 18391470}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x64x512x256xf32>, tensor<3x3xf32>) outs (%init: tensor<4x64x510x254xf32>) -> tensor<4x64x510x254xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x64x512x256xf32>, tensor<3x3xf32>) outs (%init: tensor<4x64x510x254xf32>) -> tensor<4x64x510x254xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<4x64x512x256xf32>, %filter: tensor<3x3xf32>, %init: tensor<4x64x510x254xf32>) -> tensor<4x64x510x254xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x64x512x256xf32>, tensor<3x3xf32>) outs (%init: tensor<4x64x510x254xf32>) -> tensor<4x64x510x254xf32>\n  return %ret : tensor<4x64x510x254xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<4x64x512x256xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<4x64x510x254xf32>) -> tensor<4x64x510x254xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<4x64x512x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<4x64x510x254xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<4x64x510x254xf32>\n    memref.copy %1, %alloc : memref<4x64x510x254xf32> to memref<4x64x510x254xf32>\n    affine.for %arg3 = 0 to 4 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 510 {\n          affine.for %arg6 = 0 to 254 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<4x64x512x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x64x510x254xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x64x510x254xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<4x64x510x254xf32>\n    return %2 : tensor<4x64x510x254xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<4x64x510x254xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<4x64x512x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<4x64x512x256xf32>) -> tensor<4x64x512x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<4x64x510x254xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<4x64x510x254xf32>) -> tensor<4x64x510x254xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x64x512x256xf32>, tensor<3x3xf32>) outs (%init: tensor<4x64x510x254xf32>) -> tensor<4x64x510x254xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<4x64x510x254xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<4x64x510x254xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 4, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 510, 1], ["%arg6", 0, 254, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 711716182}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x512x256x32xf32>, tensor<3x3xf32>) outs (%init: tensor<32x512x127x15xf32>) -> tensor<32x512x127x15xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x512x256x32xf32>, tensor<3x3xf32>) outs (%init: tensor<32x512x127x15xf32>) -> tensor<32x512x127x15xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x512x256x32xf32>, %filter: tensor<3x3xf32>, %init: tensor<32x512x127x15xf32>) -> tensor<32x512x127x15xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x512x256x32xf32>, tensor<3x3xf32>) outs (%init: tensor<32x512x127x15xf32>) -> tensor<32x512x127x15xf32>\n  return %ret : tensor<32x512x127x15xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x512x256x32xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<32x512x127x15xf32>) -> tensor<32x512x127x15xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<32x512x256x32xf32>\n    %1 = bufferization.to_memref %arg2 : memref<32x512x127x15xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x512x127x15xf32>\n    memref.copy %1, %alloc : memref<32x512x127x15xf32> to memref<32x512x127x15xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 127 {\n          affine.for %arg6 = 0 to 15 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<32x512x256x32xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x512x127x15xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x512x127x15xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<32x512x127x15xf32>\n    return %2 : tensor<32x512x127x15xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<32x512x127x15xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<32x512x256x32xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<32x512x256x32xf32>) -> tensor<32x512x256x32xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<32x512x127x15xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<32x512x127x15xf32>) -> tensor<32x512x127x15xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x512x256x32xf32>, tensor<3x3xf32>) outs (%init: tensor<32x512x127x15xf32>) -> tensor<32x512x127x15xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<32x512x127x15xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<32x512x127x15xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 127, 1], ["%arg6", 0, 15, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 686269585}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x32x128x64xf32>, tensor<1x1xf32>) outs (%init: tensor<32x32x128x64xf32>) -> tensor<32x32x128x64xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x32x128x64xf32>, tensor<1x1xf32>) outs (%init: tensor<32x32x128x64xf32>) -> tensor<32x32x128x64xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x32x128x64xf32>, %filter: tensor<1x1xf32>, %init: tensor<32x32x128x64xf32>) -> tensor<32x32x128x64xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x32x128x64xf32>, tensor<1x1xf32>) outs (%init: tensor<32x32x128x64xf32>) -> tensor<32x32x128x64xf32>\n  return %ret : tensor<32x32x128x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x32x128x64xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<32x32x128x64xf32>) -> tensor<32x32x128x64xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<32x32x128x64xf32>\n    %1 = bufferization.to_memref %arg2 : memref<32x32x128x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x32x128x64xf32>\n    memref.copy %1, %alloc : memref<32x32x128x64xf32> to memref<32x32x128x64xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 128 {\n          affine.for %arg6 = 0 to 64 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<32x32x128x64xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x32x128x64xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x32x128x64xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<32x32x128x64xf32>\n    return %2 : tensor<32x32x128x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<32x32x128x64xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<32x32x128x64xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<32x32x128x64xf32>) -> tensor<32x32x128x64xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<32x32x128x64xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<32x32x128x64xf32>) -> tensor<32x32x128x64xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x32x128x64xf32>, tensor<1x1xf32>) outs (%init: tensor<32x32x128x64xf32>) -> tensor<32x32x128x64xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<32x32x128x64xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<32x32x128x64xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 128, 1], ["%arg6", 0, 64, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 13185622}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x8x256x64xf32>, tensor<5x5xf32>) outs (%init: tensor<8x8x252x60xf32>) -> tensor<8x8x252x60xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x8x256x64xf32>, tensor<5x5xf32>) outs (%init: tensor<8x8x252x60xf32>) -> tensor<8x8x252x60xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<8x8x256x64xf32>, %filter: tensor<5x5xf32>, %init: tensor<8x8x252x60xf32>) -> tensor<8x8x252x60xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x8x256x64xf32>, tensor<5x5xf32>) outs (%init: tensor<8x8x252x60xf32>) -> tensor<8x8x252x60xf32>\n  return %ret : tensor<8x8x252x60xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<8x8x256x64xf32>, %arg1: tensor<5x5xf32>, %arg2: tensor<8x8x252x60xf32>) -> tensor<8x8x252x60xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<8x8x256x64xf32>\n    %1 = bufferization.to_memref %arg2 : memref<8x8x252x60xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<8x8x252x60xf32>\n    memref.copy %1, %alloc : memref<8x8x252x60xf32> to memref<8x8x252x60xf32>\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 8 {\n        affine.for %arg5 = 0 to 252 {\n          affine.for %arg6 = 0 to 60 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<8x8x256x64xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x8x252x60xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x8x252x60xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<8x8x252x60xf32>\n    return %2 : tensor<8x8x252x60xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<8x8x252x60xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<8x8x256x64xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<8x8x256x64xf32>) -> tensor<8x8x256x64xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<5x5xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<5x5xf32>) -> tensor<5x5xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<8x8x252x60xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<8x8x252x60xf32>) -> tensor<8x8x252x60xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x8x256x64xf32>, tensor<5x5xf32>) outs (%init: tensor<8x8x252x60xf32>) -> tensor<8x8x252x60xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<8x8x252x60xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<8x8x252x60xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 8, 1], ["%arg4", 0, 8, 1], ["%arg5", 0, 252, 1], ["%arg6", 0, 60, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 80167714}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x512x256x64xf32>, tensor<3x3xf32>) outs (%init: tensor<16x512x254x62xf32>) -> tensor<16x512x254x62xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x512x256x64xf32>, tensor<3x3xf32>) outs (%init: tensor<16x512x254x62xf32>) -> tensor<16x512x254x62xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<16x512x256x64xf32>, %filter: tensor<3x3xf32>, %init: tensor<16x512x254x62xf32>) -> tensor<16x512x254x62xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x512x256x64xf32>, tensor<3x3xf32>) outs (%init: tensor<16x512x254x62xf32>) -> tensor<16x512x254x62xf32>\n  return %ret : tensor<16x512x254x62xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<16x512x256x64xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<16x512x254x62xf32>) -> tensor<16x512x254x62xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<16x512x256x64xf32>\n    %1 = bufferization.to_memref %arg2 : memref<16x512x254x62xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x512x254x62xf32>\n    memref.copy %1, %alloc : memref<16x512x254x62xf32> to memref<16x512x254x62xf32>\n    affine.for %arg3 = 0 to 16 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 254 {\n          affine.for %arg6 = 0 to 62 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<16x512x256x64xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x512x254x62xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x512x254x62xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<16x512x254x62xf32>\n    return %2 : tensor<16x512x254x62xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x512x254x62xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<16x512x256x64xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<16x512x256x64xf32>) -> tensor<16x512x256x64xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<16x512x254x62xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<16x512x254x62xf32>) -> tensor<16x512x254x62xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x512x256x64xf32>, tensor<3x3xf32>) outs (%init: tensor<16x512x254x62xf32>) -> tensor<16x512x254x62xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x512x254x62xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x512x254x62xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 16, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 254, 1], ["%arg6", 0, 62, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 2789614772}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x256x256x256xf32>, tensor<3x3xf32>) outs (%init: tensor<4x256x254x254xf32>) -> tensor<4x256x254x254xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x256x256x256xf32>, tensor<3x3xf32>) outs (%init: tensor<4x256x254x254xf32>) -> tensor<4x256x254x254xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<4x256x256x256xf32>, %filter: tensor<3x3xf32>, %init: tensor<4x256x254x254xf32>) -> tensor<4x256x254x254xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x256x256x256xf32>, tensor<3x3xf32>) outs (%init: tensor<4x256x254x254xf32>) -> tensor<4x256x254x254xf32>\n  return %ret : tensor<4x256x254x254xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<4x256x256x256xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<4x256x254x254xf32>) -> tensor<4x256x254x254xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<4x256x256x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<4x256x254x254xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<4x256x254x254xf32>\n    memref.copy %1, %alloc : memref<4x256x254x254xf32> to memref<4x256x254x254xf32>\n    affine.for %arg3 = 0 to 4 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 254 {\n          affine.for %arg6 = 0 to 254 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<4x256x256x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x256x254x254xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x256x254x254xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<4x256x254x254xf32>\n    return %2 : tensor<4x256x254x254xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<4x256x254x254xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<4x256x256x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<4x256x256x256xf32>) -> tensor<4x256x256x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<4x256x254x254xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<4x256x254x254xf32>) -> tensor<4x256x254x254xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x256x256x256xf32>, tensor<3x3xf32>) outs (%init: tensor<4x256x254x254xf32>) -> tensor<4x256x254x254xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<4x256x254x254xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<4x256x254x254xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 4, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 254, 1], ["%arg6", 0, 254, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 1417502615}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x128x512x64xf32>, tensor<5x5xf32>) outs (%init: tensor<8x128x508x60xf32>) -> tensor<8x128x508x60xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x128x512x64xf32>, tensor<5x5xf32>) outs (%init: tensor<8x128x508x60xf32>) -> tensor<8x128x508x60xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<8x128x512x64xf32>, %filter: tensor<5x5xf32>, %init: tensor<8x128x508x60xf32>) -> tensor<8x128x508x60xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x128x512x64xf32>, tensor<5x5xf32>) outs (%init: tensor<8x128x508x60xf32>) -> tensor<8x128x508x60xf32>\n  return %ret : tensor<8x128x508x60xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<8x128x512x64xf32>, %arg1: tensor<5x5xf32>, %arg2: tensor<8x128x508x60xf32>) -> tensor<8x128x508x60xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<8x128x512x64xf32>\n    %1 = bufferization.to_memref %arg2 : memref<8x128x508x60xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<8x128x508x60xf32>\n    memref.copy %1, %alloc : memref<8x128x508x60xf32> to memref<8x128x508x60xf32>\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 508 {\n          affine.for %arg6 = 0 to 60 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<8x128x512x64xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x128x508x60xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x128x508x60xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<8x128x508x60xf32>\n    return %2 : tensor<8x128x508x60xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<8x128x508x60xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<8x128x512x64xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<8x128x512x64xf32>) -> tensor<8x128x512x64xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<5x5xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<5x5xf32>) -> tensor<5x5xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<8x128x508x60xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<8x128x508x60xf32>) -> tensor<8x128x508x60xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x128x512x64xf32>, tensor<5x5xf32>) outs (%init: tensor<8x128x508x60xf32>) -> tensor<8x128x508x60xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<8x128x508x60xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<8x128x508x60xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 8, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 508, 1], ["%arg6", 0, 60, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 2586339137}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x64x128x64xf32>, tensor<1x1xf32>) outs (%init: tensor<4x64x128x64xf32>) -> tensor<4x64x128x64xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x64x128x64xf32>, tensor<1x1xf32>) outs (%init: tensor<4x64x128x64xf32>) -> tensor<4x64x128x64xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<4x64x128x64xf32>, %filter: tensor<1x1xf32>, %init: tensor<4x64x128x64xf32>) -> tensor<4x64x128x64xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x64x128x64xf32>, tensor<1x1xf32>) outs (%init: tensor<4x64x128x64xf32>) -> tensor<4x64x128x64xf32>\n  return %ret : tensor<4x64x128x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<4x64x128x64xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<4x64x128x64xf32>) -> tensor<4x64x128x64xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<4x64x128x64xf32>\n    %1 = bufferization.to_memref %arg2 : memref<4x64x128x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<4x64x128x64xf32>\n    memref.copy %1, %alloc : memref<4x64x128x64xf32> to memref<4x64x128x64xf32>\n    affine.for %arg3 = 0 to 4 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 128 {\n          affine.for %arg6 = 0 to 64 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<4x64x128x64xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x64x128x64xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x64x128x64xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<4x64x128x64xf32>\n    return %2 : tensor<4x64x128x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<4x64x128x64xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<4x64x128x64xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<4x64x128x64xf32>) -> tensor<4x64x128x64xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<4x64x128x64xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<4x64x128x64xf32>) -> tensor<4x64x128x64xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x64x128x64xf32>, tensor<1x1xf32>) outs (%init: tensor<4x64x128x64xf32>) -> tensor<4x64x128x64xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<4x64x128x64xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<4x64x128x64xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 4, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 128, 1], ["%arg6", 0, 64, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 3178483}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x16x128x128xf32>, tensor<7x7xf32>) outs (%init: tensor<32x16x122x122xf32>) -> tensor<32x16x122x122xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x16x128x128xf32>, tensor<7x7xf32>) outs (%init: tensor<32x16x122x122xf32>) -> tensor<32x16x122x122xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x16x128x128xf32>, %filter: tensor<7x7xf32>, %init: tensor<32x16x122x122xf32>) -> tensor<32x16x122x122xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x16x128x128xf32>, tensor<7x7xf32>) outs (%init: tensor<32x16x122x122xf32>) -> tensor<32x16x122x122xf32>\n  return %ret : tensor<32x16x122x122xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x16x128x128xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<32x16x122x122xf32>) -> tensor<32x16x122x122xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<32x16x128x128xf32>\n    %1 = bufferization.to_memref %arg2 : memref<32x16x122x122xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x16x122x122xf32>\n    memref.copy %1, %alloc : memref<32x16x122x122xf32> to memref<32x16x122x122xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 16 {\n        affine.for %arg5 = 0 to 122 {\n          affine.for %arg6 = 0 to 122 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<32x16x128x128xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x16x122x122xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x16x122x122xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<32x16x122x122xf32>\n    return %2 : tensor<32x16x122x122xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<32x16x122x122xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<32x16x128x128xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<32x16x128x128xf32>) -> tensor<32x16x128x128xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<32x16x122x122xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<32x16x122x122xf32>) -> tensor<32x16x122x122xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x16x128x128xf32>, tensor<7x7xf32>) outs (%init: tensor<32x16x122x122xf32>) -> tensor<32x16x122x122xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<32x16x122x122xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<32x16x122x122xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 16, 1], ["%arg5", 0, 122, 1], ["%arg6", 0, 122, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 1486815991}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x16x512x256xf32>, tensor<1x1xf32>) outs (%init: tensor<8x16x512x256xf32>) -> tensor<8x16x512x256xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x16x512x256xf32>, tensor<1x1xf32>) outs (%init: tensor<8x16x512x256xf32>) -> tensor<8x16x512x256xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<8x16x512x256xf32>, %filter: tensor<1x1xf32>, %init: tensor<8x16x512x256xf32>) -> tensor<8x16x512x256xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x16x512x256xf32>, tensor<1x1xf32>) outs (%init: tensor<8x16x512x256xf32>) -> tensor<8x16x512x256xf32>\n  return %ret : tensor<8x16x512x256xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<8x16x512x256xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<8x16x512x256xf32>) -> tensor<8x16x512x256xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<8x16x512x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<8x16x512x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<8x16x512x256xf32>\n    memref.copy %1, %alloc : memref<8x16x512x256xf32> to memref<8x16x512x256xf32>\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 16 {\n        affine.for %arg5 = 0 to 512 {\n          affine.for %arg6 = 0 to 256 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<8x16x512x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x16x512x256xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x16x512x256xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<8x16x512x256xf32>\n    return %2 : tensor<8x16x512x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<8x16x512x256xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<8x16x512x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<8x16x512x256xf32>) -> tensor<8x16x512x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<8x16x512x256xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<8x16x512x256xf32>) -> tensor<8x16x512x256xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x16x512x256xf32>, tensor<1x1xf32>) outs (%init: tensor<8x16x512x256xf32>) -> tensor<8x16x512x256xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<8x16x512x256xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<8x16x512x256xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 8, 1], ["%arg4", 0, 16, 1], ["%arg5", 0, 512, 1], ["%arg6", 0, 256, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 24342726}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x32x128x64xf32>, tensor<7x7xf32>) outs (%init: tensor<16x32x122x58xf32>) -> tensor<16x32x122x58xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x32x128x64xf32>, tensor<7x7xf32>) outs (%init: tensor<16x32x122x58xf32>) -> tensor<16x32x122x58xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<16x32x128x64xf32>, %filter: tensor<7x7xf32>, %init: tensor<16x32x122x58xf32>) -> tensor<16x32x122x58xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x32x128x64xf32>, tensor<7x7xf32>) outs (%init: tensor<16x32x122x58xf32>) -> tensor<16x32x122x58xf32>\n  return %ret : tensor<16x32x122x58xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<16x32x128x64xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<16x32x122x58xf32>) -> tensor<16x32x122x58xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<16x32x128x64xf32>\n    %1 = bufferization.to_memref %arg2 : memref<16x32x122x58xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x32x122x58xf32>\n    memref.copy %1, %alloc : memref<16x32x122x58xf32> to memref<16x32x122x58xf32>\n    affine.for %arg3 = 0 to 16 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 122 {\n          affine.for %arg6 = 0 to 58 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<16x32x128x64xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x32x122x58xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x32x122x58xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<16x32x122x58xf32>\n    return %2 : tensor<16x32x122x58xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x32x122x58xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<16x32x128x64xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<16x32x128x64xf32>) -> tensor<16x32x128x64xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<16x32x122x58xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<16x32x122x58xf32>) -> tensor<16x32x122x58xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x32x128x64xf32>, tensor<7x7xf32>) outs (%init: tensor<16x32x122x58xf32>) -> tensor<16x32x122x58xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x32x122x58xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x32x122x58xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 16, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 122, 1], ["%arg6", 0, 58, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 706255467}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x64x32x64xf32>, tensor<3x3xf32>) outs (%init: tensor<4x64x30x62xf32>) -> tensor<4x64x30x62xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x64x32x64xf32>, tensor<3x3xf32>) outs (%init: tensor<4x64x30x62xf32>) -> tensor<4x64x30x62xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<4x64x32x64xf32>, %filter: tensor<3x3xf32>, %init: tensor<4x64x30x62xf32>) -> tensor<4x64x30x62xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x64x32x64xf32>, tensor<3x3xf32>) outs (%init: tensor<4x64x30x62xf32>) -> tensor<4x64x30x62xf32>\n  return %ret : tensor<4x64x30x62xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<4x64x32x64xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<4x64x30x62xf32>) -> tensor<4x64x30x62xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<4x64x32x64xf32>\n    %1 = bufferization.to_memref %arg2 : memref<4x64x30x62xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<4x64x30x62xf32>\n    memref.copy %1, %alloc : memref<4x64x30x62xf32> to memref<4x64x30x62xf32>\n    affine.for %arg3 = 0 to 4 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 30 {\n          affine.for %arg6 = 0 to 62 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<4x64x32x64xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x64x30x62xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x64x30x62xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<4x64x30x62xf32>\n    return %2 : tensor<4x64x30x62xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<4x64x30x62xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<4x64x32x64xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<4x64x32x64xf32>) -> tensor<4x64x32x64xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<4x64x30x62xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<4x64x30x62xf32>) -> tensor<4x64x30x62xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x64x32x64xf32>, tensor<3x3xf32>) outs (%init: tensor<4x64x30x62xf32>) -> tensor<4x64x30x62xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<4x64x30x62xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<4x64x30x62xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 4, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 30, 1], ["%arg6", 0, 62, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 10152009}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x32x256x128xf32>, tensor<7x7xf32>) outs (%init: tensor<8x32x125x61xf32>) -> tensor<8x32x125x61xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x32x256x128xf32>, tensor<7x7xf32>) outs (%init: tensor<8x32x125x61xf32>) -> tensor<8x32x125x61xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<8x32x256x128xf32>, %filter: tensor<7x7xf32>, %init: tensor<8x32x125x61xf32>) -> tensor<8x32x125x61xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x32x256x128xf32>, tensor<7x7xf32>) outs (%init: tensor<8x32x125x61xf32>) -> tensor<8x32x125x61xf32>\n  return %ret : tensor<8x32x125x61xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<8x32x256x128xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<8x32x125x61xf32>) -> tensor<8x32x125x61xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<8x32x256x128xf32>\n    %1 = bufferization.to_memref %arg2 : memref<8x32x125x61xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<8x32x125x61xf32>\n    memref.copy %1, %alloc : memref<8x32x125x61xf32> to memref<8x32x125x61xf32>\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 125 {\n          affine.for %arg6 = 0 to 61 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<8x32x256x128xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x32x125x61xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x32x125x61xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<8x32x125x61xf32>\n    return %2 : tensor<8x32x125x61xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<8x32x125x61xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<8x32x256x128xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<8x32x256x128xf32>) -> tensor<8x32x256x128xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<8x32x125x61xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<8x32x125x61xf32>) -> tensor<8x32x125x61xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x32x256x128xf32>, tensor<7x7xf32>) outs (%init: tensor<8x32x125x61xf32>) -> tensor<8x32x125x61xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<8x32x125x61xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<8x32x125x61xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 8, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 125, 1], ["%arg6", 0, 61, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 380922900}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x128x256x256xf32>, tensor<7x7xf32>) outs (%init: tensor<4x128x250x250xf32>) -> tensor<4x128x250x250xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x128x256x256xf32>, tensor<7x7xf32>) outs (%init: tensor<4x128x250x250xf32>) -> tensor<4x128x250x250xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<4x128x256x256xf32>, %filter: tensor<7x7xf32>, %init: tensor<4x128x250x250xf32>) -> tensor<4x128x250x250xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x128x256x256xf32>, tensor<7x7xf32>) outs (%init: tensor<4x128x250x250xf32>) -> tensor<4x128x250x250xf32>\n  return %ret : tensor<4x128x250x250xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<4x128x256x256xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<4x128x250x250xf32>) -> tensor<4x128x250x250xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<4x128x256x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<4x128x250x250xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<4x128x250x250xf32>\n    memref.copy %1, %alloc : memref<4x128x250x250xf32> to memref<4x128x250x250xf32>\n    affine.for %arg3 = 0 to 4 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 250 {\n          affine.for %arg6 = 0 to 250 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<4x128x256x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x128x250x250xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x128x250x250xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<4x128x250x250xf32>\n    return %2 : tensor<4x128x250x250xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<4x128x250x250xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<4x128x256x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<4x128x256x256xf32>) -> tensor<4x128x256x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<4x128x250x250xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<4x128x250x250xf32>) -> tensor<4x128x250x250xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x128x256x256xf32>, tensor<7x7xf32>) outs (%init: tensor<4x128x250x250xf32>) -> tensor<4x128x250x250xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<4x128x250x250xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<4x128x250x250xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 4, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 250, 1], ["%arg6", 0, 250, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 6237884839}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x128x32x512xf32>, tensor<5x5xf32>) outs (%init: tensor<4x128x28x508xf32>) -> tensor<4x128x28x508xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x128x32x512xf32>, tensor<5x5xf32>) outs (%init: tensor<4x128x28x508xf32>) -> tensor<4x128x28x508xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<4x128x32x512xf32>, %filter: tensor<5x5xf32>, %init: tensor<4x128x28x508xf32>) -> tensor<4x128x28x508xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x128x32x512xf32>, tensor<5x5xf32>) outs (%init: tensor<4x128x28x508xf32>) -> tensor<4x128x28x508xf32>\n  return %ret : tensor<4x128x28x508xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<4x128x32x512xf32>, %arg1: tensor<5x5xf32>, %arg2: tensor<4x128x28x508xf32>) -> tensor<4x128x28x508xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<4x128x32x512xf32>\n    %1 = bufferization.to_memref %arg2 : memref<4x128x28x508xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<4x128x28x508xf32>\n    memref.copy %1, %alloc : memref<4x128x28x508xf32> to memref<4x128x28x508xf32>\n    affine.for %arg3 = 0 to 4 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 508 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<4x128x32x512xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x128x28x508xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x128x28x508xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<4x128x28x508xf32>\n    return %2 : tensor<4x128x28x508xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<4x128x28x508xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<4x128x32x512xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<4x128x32x512xf32>) -> tensor<4x128x32x512xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<5x5xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<5x5xf32>) -> tensor<5x5xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<4x128x28x508xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<4x128x28x508xf32>) -> tensor<4x128x28x508xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x128x32x512xf32>, tensor<5x5xf32>) outs (%init: tensor<4x128x28x508xf32>) -> tensor<4x128x28x508xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<4x128x28x508xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<4x128x28x508xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 4, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 508, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 602577924}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x64x256x256xf32>, tensor<1x1xf32>) outs (%init: tensor<4x64x128x128xf32>) -> tensor<4x64x128x128xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x64x256x256xf32>, tensor<1x1xf32>) outs (%init: tensor<4x64x128x128xf32>) -> tensor<4x64x128x128xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<4x64x256x256xf32>, %filter: tensor<1x1xf32>, %init: tensor<4x64x128x128xf32>) -> tensor<4x64x128x128xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x64x256x256xf32>, tensor<1x1xf32>) outs (%init: tensor<4x64x128x128xf32>) -> tensor<4x64x128x128xf32>\n  return %ret : tensor<4x64x128x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<4x64x256x256xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<4x64x128x128xf32>) -> tensor<4x64x128x128xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<4x64x256x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<4x64x128x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<4x64x128x128xf32>\n    memref.copy %1, %alloc : memref<4x64x128x128xf32> to memref<4x64x128x128xf32>\n    affine.for %arg3 = 0 to 4 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 128 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<4x64x256x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x64x128x128xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x64x128x128xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<4x64x128x128xf32>\n    return %2 : tensor<4x64x128x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<4x64x128x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<4x64x256x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<4x64x256x256xf32>) -> tensor<4x64x256x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<4x64x128x128xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<4x64x128x128xf32>) -> tensor<4x64x128x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x64x256x256xf32>, tensor<1x1xf32>) outs (%init: tensor<4x64x128x128xf32>) -> tensor<4x64x128x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<4x64x128x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<4x64x128x128xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 4, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 128, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 9119645}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x128x64x64xf32>, tensor<1x1xf32>) outs (%init: tensor<256x128x32x32xf32>) -> tensor<256x128x32x32xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x128x64x64xf32>, tensor<1x1xf32>) outs (%init: tensor<256x128x32x32xf32>) -> tensor<256x128x32x32xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x128x64x64xf32>, %filter: tensor<1x1xf32>, %init: tensor<256x128x32x32xf32>) -> tensor<256x128x32x32xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x128x64x64xf32>, tensor<1x1xf32>) outs (%init: tensor<256x128x32x32xf32>) -> tensor<256x128x32x32xf32>\n  return %ret : tensor<256x128x32x32xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x128x64x64xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<256x128x32x32xf32>) -> tensor<256x128x32x32xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<256x128x64x64xf32>\n    %1 = bufferization.to_memref %arg2 : memref<256x128x32x32xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x128x32x32xf32>\n    memref.copy %1, %alloc : memref<256x128x32x32xf32> to memref<256x128x32x32xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 32 {\n          affine.for %arg6 = 0 to 32 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<256x128x64x64xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x128x32x32xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x128x32x32xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<256x128x32x32xf32>\n    return %2 : tensor<256x128x32x32xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x128x32x32xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x128x64x64xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x128x64x64xf32>) -> tensor<256x128x64x64xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x128x32x32xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x128x32x32xf32>) -> tensor<256x128x32x32xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x128x64x64xf32>, tensor<1x1xf32>) outs (%init: tensor<256x128x32x32xf32>) -> tensor<256x128x32x32xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x128x32x32xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x128x32x32xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 32, 1], ["%arg6", 0, 32, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 69167642}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x256x512x64xf32>, tensor<3x3xf32>) outs (%init: tensor<64x256x510x62xf32>) -> tensor<64x256x510x62xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x256x512x64xf32>, tensor<3x3xf32>) outs (%init: tensor<64x256x510x62xf32>) -> tensor<64x256x510x62xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x256x512x64xf32>, %filter: tensor<3x3xf32>, %init: tensor<64x256x510x62xf32>) -> tensor<64x256x510x62xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x256x512x64xf32>, tensor<3x3xf32>) outs (%init: tensor<64x256x510x62xf32>) -> tensor<64x256x510x62xf32>\n  return %ret : tensor<64x256x510x62xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x256x512x64xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<64x256x510x62xf32>) -> tensor<64x256x510x62xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<64x256x512x64xf32>\n    %1 = bufferization.to_memref %arg2 : memref<64x256x510x62xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x256x510x62xf32>\n    memref.copy %1, %alloc : memref<64x256x510x62xf32> to memref<64x256x510x62xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 510 {\n          affine.for %arg6 = 0 to 62 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<64x256x512x64xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x256x510x62xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x256x510x62xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<64x256x510x62xf32>\n    return %2 : tensor<64x256x510x62xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<64x256x510x62xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<64x256x512x64xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<64x256x512x64xf32>) -> tensor<64x256x512x64xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<64x256x510x62xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<64x256x510x62xf32>) -> tensor<64x256x510x62xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x256x512x64xf32>, tensor<3x3xf32>) outs (%init: tensor<64x256x510x62xf32>) -> tensor<64x256x510x62xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<64x256x510x62xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<64x256x510x62xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 510, 1], ["%arg6", 0, 62, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 11206487470}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x1024x32x64xf32>, tensor<3x3xf32>) outs (%init: tensor<4x1024x15x31xf32>) -> tensor<4x1024x15x31xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x1024x32x64xf32>, tensor<3x3xf32>) outs (%init: tensor<4x1024x15x31xf32>) -> tensor<4x1024x15x31xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<4x1024x32x64xf32>, %filter: tensor<3x3xf32>, %init: tensor<4x1024x15x31xf32>) -> tensor<4x1024x15x31xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x1024x32x64xf32>, tensor<3x3xf32>) outs (%init: tensor<4x1024x15x31xf32>) -> tensor<4x1024x15x31xf32>\n  return %ret : tensor<4x1024x15x31xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<4x1024x32x64xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<4x1024x15x31xf32>) -> tensor<4x1024x15x31xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<4x1024x32x64xf32>\n    %1 = bufferization.to_memref %arg2 : memref<4x1024x15x31xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<4x1024x15x31xf32>\n    memref.copy %1, %alloc : memref<4x1024x15x31xf32> to memref<4x1024x15x31xf32>\n    affine.for %arg3 = 0 to 4 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 15 {\n          affine.for %arg6 = 0 to 31 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<4x1024x32x64xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x1024x15x31xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x1024x15x31xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<4x1024x15x31xf32>\n    return %2 : tensor<4x1024x15x31xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<4x1024x15x31xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<4x1024x32x64xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<4x1024x32x64xf32>) -> tensor<4x1024x32x64xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<4x1024x15x31xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<4x1024x15x31xf32>) -> tensor<4x1024x15x31xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x1024x32x64xf32>, tensor<3x3xf32>) outs (%init: tensor<4x1024x15x31xf32>) -> tensor<4x1024x15x31xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<4x1024x15x31xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<4x1024x15x31xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 4, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 15, 1], ["%arg6", 0, 31, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 41665513}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x16x512x256xf32>, tensor<5x5xf32>) outs (%init: tensor<16x16x254x126xf32>) -> tensor<16x16x254x126xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x16x512x256xf32>, tensor<5x5xf32>) outs (%init: tensor<16x16x254x126xf32>) -> tensor<16x16x254x126xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<16x16x512x256xf32>, %filter: tensor<5x5xf32>, %init: tensor<16x16x254x126xf32>) -> tensor<16x16x254x126xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x16x512x256xf32>, tensor<5x5xf32>) outs (%init: tensor<16x16x254x126xf32>) -> tensor<16x16x254x126xf32>\n  return %ret : tensor<16x16x254x126xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<16x16x512x256xf32>, %arg1: tensor<5x5xf32>, %arg2: tensor<16x16x254x126xf32>) -> tensor<16x16x254x126xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<16x16x512x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<16x16x254x126xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x16x254x126xf32>\n    memref.copy %1, %alloc : memref<16x16x254x126xf32> to memref<16x16x254x126xf32>\n    affine.for %arg3 = 0 to 16 {\n      affine.for %arg4 = 0 to 16 {\n        affine.for %arg5 = 0 to 254 {\n          affine.for %arg6 = 0 to 126 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<16x16x512x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x16x254x126xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x16x254x126xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<16x16x254x126xf32>\n    return %2 : tensor<16x16x254x126xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x16x254x126xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<16x16x512x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<16x16x512x256xf32>) -> tensor<16x16x512x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<5x5xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<5x5xf32>) -> tensor<5x5xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<16x16x254x126xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<16x16x254x126xf32>) -> tensor<16x16x254x126xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x16x512x256xf32>, tensor<5x5xf32>) outs (%init: tensor<16x16x254x126xf32>) -> tensor<16x16x254x126xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x16x254x126xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x16x254x126xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 16, 1], ["%arg4", 0, 16, 1], ["%arg5", 0, 254, 1], ["%arg6", 0, 126, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 683705425}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x1024x64x64xf32>, tensor<7x7xf32>) outs (%init: tensor<16x1024x58x58xf32>) -> tensor<16x1024x58x58xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x1024x64x64xf32>, tensor<7x7xf32>) outs (%init: tensor<16x1024x58x58xf32>) -> tensor<16x1024x58x58xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<16x1024x64x64xf32>, %filter: tensor<7x7xf32>, %init: tensor<16x1024x58x58xf32>) -> tensor<16x1024x58x58xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x1024x64x64xf32>, tensor<7x7xf32>) outs (%init: tensor<16x1024x58x58xf32>) -> tensor<16x1024x58x58xf32>\n  return %ret : tensor<16x1024x58x58xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<16x1024x64x64xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<16x1024x58x58xf32>) -> tensor<16x1024x58x58xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<16x1024x64x64xf32>\n    %1 = bufferization.to_memref %arg2 : memref<16x1024x58x58xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x1024x58x58xf32>\n    memref.copy %1, %alloc : memref<16x1024x58x58xf32> to memref<16x1024x58x58xf32>\n    affine.for %arg3 = 0 to 16 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 58 {\n          affine.for %arg6 = 0 to 58 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<16x1024x64x64xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x1024x58x58xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x1024x58x58xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<16x1024x58x58xf32>\n    return %2 : tensor<16x1024x58x58xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x1024x58x58xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<16x1024x64x64xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<16x1024x64x64xf32>) -> tensor<16x1024x64x64xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<16x1024x58x58xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<16x1024x58x58xf32>) -> tensor<16x1024x58x58xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x1024x64x64xf32>, tensor<7x7xf32>) outs (%init: tensor<16x1024x58x58xf32>) -> tensor<16x1024x58x58xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x1024x58x58xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x1024x58x58xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 16, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 58, 1], ["%arg6", 0, 58, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 10753348758}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x512x32x256xf32>, tensor<7x7xf32>) outs (%init: tensor<8x512x26x250xf32>) -> tensor<8x512x26x250xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x512x32x256xf32>, tensor<7x7xf32>) outs (%init: tensor<8x512x26x250xf32>) -> tensor<8x512x26x250xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<8x512x32x256xf32>, %filter: tensor<7x7xf32>, %init: tensor<8x512x26x250xf32>) -> tensor<8x512x26x250xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x512x32x256xf32>, tensor<7x7xf32>) outs (%init: tensor<8x512x26x250xf32>) -> tensor<8x512x26x250xf32>\n  return %ret : tensor<8x512x26x250xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<8x512x32x256xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<8x512x26x250xf32>) -> tensor<8x512x26x250xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<8x512x32x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<8x512x26x250xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<8x512x26x250xf32>\n    memref.copy %1, %alloc : memref<8x512x26x250xf32> to memref<8x512x26x250xf32>\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 26 {\n          affine.for %arg6 = 0 to 250 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<8x512x32x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x512x26x250xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x512x26x250xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<8x512x26x250xf32>\n    return %2 : tensor<8x512x26x250xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<8x512x26x250xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<8x512x32x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<8x512x32x256xf32>) -> tensor<8x512x32x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<8x512x26x250xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<8x512x26x250xf32>) -> tensor<8x512x26x250xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x512x32x256xf32>, tensor<7x7xf32>) outs (%init: tensor<8x512x26x250xf32>) -> tensor<8x512x26x250xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<8x512x26x250xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<8x512x26x250xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 8, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 26, 1], ["%arg6", 0, 250, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 5189944866}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x32x256x32xf32>, tensor<7x7xf32>) outs (%init: tensor<64x32x250x26xf32>) -> tensor<64x32x250x26xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x32x256x32xf32>, tensor<7x7xf32>) outs (%init: tensor<64x32x250x26xf32>) -> tensor<64x32x250x26xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x32x256x32xf32>, %filter: tensor<7x7xf32>, %init: tensor<64x32x250x26xf32>) -> tensor<64x32x250x26xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x32x256x32xf32>, tensor<7x7xf32>) outs (%init: tensor<64x32x250x26xf32>) -> tensor<64x32x250x26xf32>\n  return %ret : tensor<64x32x250x26xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x32x256x32xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<64x32x250x26xf32>) -> tensor<64x32x250x26xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<64x32x256x32xf32>\n    %1 = bufferization.to_memref %arg2 : memref<64x32x250x26xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x32x250x26xf32>\n    memref.copy %1, %alloc : memref<64x32x250x26xf32> to memref<64x32x250x26xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 250 {\n          affine.for %arg6 = 0 to 26 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<64x32x256x32xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x32x250x26xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x32x250x26xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<64x32x250x26xf32>\n    return %2 : tensor<64x32x250x26xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<64x32x250x26xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<64x32x256x32xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<64x32x256x32xf32>) -> tensor<64x32x256x32xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<64x32x250x26xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<64x32x250x26xf32>) -> tensor<64x32x250x26xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x32x256x32xf32>, tensor<7x7xf32>) outs (%init: tensor<64x32x250x26xf32>) -> tensor<64x32x250x26xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<64x32x250x26xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<64x32x250x26xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 250, 1], ["%arg6", 0, 26, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 2598390765}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x64x128x64xf32>, tensor<7x7xf32>) outs (%init: tensor<32x64x61x29xf32>) -> tensor<32x64x61x29xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x64x128x64xf32>, tensor<7x7xf32>) outs (%init: tensor<32x64x61x29xf32>) -> tensor<32x64x61x29xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x64x128x64xf32>, %filter: tensor<7x7xf32>, %init: tensor<32x64x61x29xf32>) -> tensor<32x64x61x29xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x64x128x64xf32>, tensor<7x7xf32>) outs (%init: tensor<32x64x61x29xf32>) -> tensor<32x64x61x29xf32>\n  return %ret : tensor<32x64x61x29xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x64x128x64xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<32x64x61x29xf32>) -> tensor<32x64x61x29xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<32x64x128x64xf32>\n    %1 = bufferization.to_memref %arg2 : memref<32x64x61x29xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x64x61x29xf32>\n    memref.copy %1, %alloc : memref<32x64x61x29xf32> to memref<32x64x61x29xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 61 {\n          affine.for %arg6 = 0 to 29 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<32x64x128x64xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x64x61x29xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x64x61x29xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<32x64x61x29xf32>\n    return %2 : tensor<32x64x61x29xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<32x64x61x29xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<32x64x128x64xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<32x64x128x64xf32>) -> tensor<32x64x128x64xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<32x64x61x29xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<32x64x61x29xf32>) -> tensor<32x64x61x29xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x64x128x64xf32>, tensor<7x7xf32>) outs (%init: tensor<32x64x61x29xf32>) -> tensor<32x64x61x29xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<32x64x61x29xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<32x64x61x29xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 61, 1], ["%arg6", 0, 29, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 721200059}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x8x32x256xf32>, tensor<3x3xf32>) outs (%init: tensor<64x8x15x127xf32>) -> tensor<64x8x15x127xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x8x32x256xf32>, tensor<3x3xf32>) outs (%init: tensor<64x8x15x127xf32>) -> tensor<64x8x15x127xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x8x32x256xf32>, %filter: tensor<3x3xf32>, %init: tensor<64x8x15x127xf32>) -> tensor<64x8x15x127xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x8x32x256xf32>, tensor<3x3xf32>) outs (%init: tensor<64x8x15x127xf32>) -> tensor<64x8x15x127xf32>\n  return %ret : tensor<64x8x15x127xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x8x32x256xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<64x8x15x127xf32>) -> tensor<64x8x15x127xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<64x8x32x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<64x8x15x127xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x8x15x127xf32>\n    memref.copy %1, %alloc : memref<64x8x15x127xf32> to memref<64x8x15x127xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 8 {\n        affine.for %arg5 = 0 to 15 {\n          affine.for %arg6 = 0 to 127 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<64x8x32x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x8x15x127xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x8x15x127xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<64x8x15x127xf32>\n    return %2 : tensor<64x8x15x127xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<64x8x15x127xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<64x8x32x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<64x8x32x256xf32>) -> tensor<64x8x32x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<64x8x15x127xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<64x8x15x127xf32>) -> tensor<64x8x15x127xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x8x32x256xf32>, tensor<3x3xf32>) outs (%init: tensor<64x8x15x127xf32>) -> tensor<64x8x15x127xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<64x8x15x127xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<64x8x15x127xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 8, 1], ["%arg5", 0, 15, 1], ["%arg6", 0, 127, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 20669038}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x512x512x256xf32>, tensor<1x1xf32>) outs (%init: tensor<4x512x512x256xf32>) -> tensor<4x512x512x256xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x512x512x256xf32>, tensor<1x1xf32>) outs (%init: tensor<4x512x512x256xf32>) -> tensor<4x512x512x256xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<4x512x512x256xf32>, %filter: tensor<1x1xf32>, %init: tensor<4x512x512x256xf32>) -> tensor<4x512x512x256xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x512x512x256xf32>, tensor<1x1xf32>) outs (%init: tensor<4x512x512x256xf32>) -> tensor<4x512x512x256xf32>\n  return %ret : tensor<4x512x512x256xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<4x512x512x256xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<4x512x512x256xf32>) -> tensor<4x512x512x256xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<4x512x512x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<4x512x512x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<4x512x512x256xf32>\n    memref.copy %1, %alloc : memref<4x512x512x256xf32> to memref<4x512x512x256xf32>\n    affine.for %arg3 = 0 to 4 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 512 {\n          affine.for %arg6 = 0 to 256 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<4x512x512x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x512x512x256xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x512x512x256xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<4x512x512x256xf32>\n    return %2 : tensor<4x512x512x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<4x512x512x256xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<4x512x512x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<4x512x512x256xf32>) -> tensor<4x512x512x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<4x512x512x256xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<4x512x512x256xf32>) -> tensor<4x512x512x256xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x512x512x256xf32>, tensor<1x1xf32>) outs (%init: tensor<4x512x512x256xf32>) -> tensor<4x512x512x256xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<4x512x512x256xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<4x512x512x256xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 4, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 512, 1], ["%arg6", 0, 256, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 389610902}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x64x128x256xf32>, tensor<3x3xf32>) outs (%init: tensor<32x64x63x127xf32>) -> tensor<32x64x63x127xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x64x128x256xf32>, tensor<3x3xf32>) outs (%init: tensor<32x64x63x127xf32>) -> tensor<32x64x63x127xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x64x128x256xf32>, %filter: tensor<3x3xf32>, %init: tensor<32x64x63x127xf32>) -> tensor<32x64x63x127xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x64x128x256xf32>, tensor<3x3xf32>) outs (%init: tensor<32x64x63x127xf32>) -> tensor<32x64x63x127xf32>\n  return %ret : tensor<32x64x63x127xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x64x128x256xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<32x64x63x127xf32>) -> tensor<32x64x63x127xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<32x64x128x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<32x64x63x127xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x64x63x127xf32>\n    memref.copy %1, %alloc : memref<32x64x63x127xf32> to memref<32x64x63x127xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 63 {\n          affine.for %arg6 = 0 to 127 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<32x64x128x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x64x63x127xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x64x63x127xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<32x64x63x127xf32>\n    return %2 : tensor<32x64x63x127xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<32x64x63x127xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<32x64x128x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<32x64x128x256xf32>) -> tensor<32x64x128x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<32x64x63x127xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<32x64x63x127xf32>) -> tensor<32x64x63x127xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x64x128x256xf32>, tensor<3x3xf32>) outs (%init: tensor<32x64x63x127xf32>) -> tensor<32x64x63x127xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<32x64x63x127xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<32x64x63x127xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 63, 1], ["%arg6", 0, 127, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 355098110}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x64x64x32xf32>, tensor<7x7xf32>) outs (%init: tensor<32x64x29x13xf32>) -> tensor<32x64x29x13xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x64x64x32xf32>, tensor<7x7xf32>) outs (%init: tensor<32x64x29x13xf32>) -> tensor<32x64x29x13xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x64x64x32xf32>, %filter: tensor<7x7xf32>, %init: tensor<32x64x29x13xf32>) -> tensor<32x64x29x13xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x64x64x32xf32>, tensor<7x7xf32>) outs (%init: tensor<32x64x29x13xf32>) -> tensor<32x64x29x13xf32>\n  return %ret : tensor<32x64x29x13xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x64x64x32xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<32x64x29x13xf32>) -> tensor<32x64x29x13xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<32x64x64x32xf32>\n    %1 = bufferization.to_memref %arg2 : memref<32x64x29x13xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x64x29x13xf32>\n    memref.copy %1, %alloc : memref<32x64x29x13xf32> to memref<32x64x29x13xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 29 {\n          affine.for %arg6 = 0 to 13 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<32x64x64x32xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x64x29x13xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x64x29x13xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<32x64x29x13xf32>\n    return %2 : tensor<32x64x29x13xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<32x64x29x13xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<32x64x64x32xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<32x64x64x32xf32>) -> tensor<32x64x64x32xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<32x64x29x13xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<32x64x29x13xf32>) -> tensor<32x64x29x13xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x64x64x32xf32>, tensor<7x7xf32>) outs (%init: tensor<32x64x29x13xf32>) -> tensor<32x64x29x13xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<32x64x29x13xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<32x64x29x13xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 29, 1], ["%arg6", 0, 13, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 151167728}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x16x32x256xf32>, tensor<7x7xf32>) outs (%init: tensor<256x16x26x250xf32>) -> tensor<256x16x26x250xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x16x32x256xf32>, tensor<7x7xf32>) outs (%init: tensor<256x16x26x250xf32>) -> tensor<256x16x26x250xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x16x32x256xf32>, %filter: tensor<7x7xf32>, %init: tensor<256x16x26x250xf32>) -> tensor<256x16x26x250xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x16x32x256xf32>, tensor<7x7xf32>) outs (%init: tensor<256x16x26x250xf32>) -> tensor<256x16x26x250xf32>\n  return %ret : tensor<256x16x26x250xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x16x32x256xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<256x16x26x250xf32>) -> tensor<256x16x26x250xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<256x16x32x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<256x16x26x250xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x16x26x250xf32>\n    memref.copy %1, %alloc : memref<256x16x26x250xf32> to memref<256x16x26x250xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 16 {\n        affine.for %arg5 = 0 to 26 {\n          affine.for %arg6 = 0 to 250 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<256x16x32x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x16x26x250xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x16x26x250xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<256x16x26x250xf32>\n    return %2 : tensor<256x16x26x250xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x16x26x250xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x16x32x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x16x32x256xf32>) -> tensor<256x16x32x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x16x26x250xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x16x26x250xf32>) -> tensor<256x16x26x250xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x16x32x256xf32>, tensor<7x7xf32>) outs (%init: tensor<256x16x26x250xf32>) -> tensor<256x16x26x250xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x16x26x250xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x16x26x250xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 16, 1], ["%arg5", 0, 26, 1], ["%arg6", 0, 250, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 5190734673}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x16x64x64xf32>, tensor<1x1xf32>) outs (%init: tensor<16x16x64x64xf32>) -> tensor<16x16x64x64xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x16x64x64xf32>, tensor<1x1xf32>) outs (%init: tensor<16x16x64x64xf32>) -> tensor<16x16x64x64xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<16x16x64x64xf32>, %filter: tensor<1x1xf32>, %init: tensor<16x16x64x64xf32>) -> tensor<16x16x64x64xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x16x64x64xf32>, tensor<1x1xf32>) outs (%init: tensor<16x16x64x64xf32>) -> tensor<16x16x64x64xf32>\n  return %ret : tensor<16x16x64x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<16x16x64x64xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<16x16x64x64xf32>) -> tensor<16x16x64x64xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<16x16x64x64xf32>\n    %1 = bufferization.to_memref %arg2 : memref<16x16x64x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x16x64x64xf32>\n    memref.copy %1, %alloc : memref<16x16x64x64xf32> to memref<16x16x64x64xf32>\n    affine.for %arg3 = 0 to 16 {\n      affine.for %arg4 = 0 to 16 {\n        affine.for %arg5 = 0 to 64 {\n          affine.for %arg6 = 0 to 64 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<16x16x64x64xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x16x64x64xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x16x64x64xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<16x16x64x64xf32>\n    return %2 : tensor<16x16x64x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x16x64x64xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<16x16x64x64xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<16x16x64x64xf32>) -> tensor<16x16x64x64xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<16x16x64x64xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<16x16x64x64xf32>) -> tensor<16x16x64x64xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x16x64x64xf32>, tensor<1x1xf32>) outs (%init: tensor<16x16x64x64xf32>) -> tensor<16x16x64x64xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x16x64x64xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x16x64x64xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 16, 1], ["%arg4", 0, 16, 1], ["%arg5", 0, 64, 1], ["%arg6", 0, 64, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 1514657}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x64x64x256xf32>, tensor<5x5xf32>) outs (%init: tensor<16x64x30x126xf32>) -> tensor<16x64x30x126xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x64x64x256xf32>, tensor<5x5xf32>) outs (%init: tensor<16x64x30x126xf32>) -> tensor<16x64x30x126xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<16x64x64x256xf32>, %filter: tensor<5x5xf32>, %init: tensor<16x64x30x126xf32>) -> tensor<16x64x30x126xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x64x64x256xf32>, tensor<5x5xf32>) outs (%init: tensor<16x64x30x126xf32>) -> tensor<16x64x30x126xf32>\n  return %ret : tensor<16x64x30x126xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<16x64x64x256xf32>, %arg1: tensor<5x5xf32>, %arg2: tensor<16x64x30x126xf32>) -> tensor<16x64x30x126xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<16x64x64x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<16x64x30x126xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x64x30x126xf32>\n    memref.copy %1, %alloc : memref<16x64x30x126xf32> to memref<16x64x30x126xf32>\n    affine.for %arg3 = 0 to 16 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 30 {\n          affine.for %arg6 = 0 to 126 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<16x64x64x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x64x30x126xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x64x30x126xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<16x64x30x126xf32>\n    return %2 : tensor<16x64x30x126xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x64x30x126xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<16x64x64x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<16x64x64x256xf32>) -> tensor<16x64x64x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<5x5xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<5x5xf32>) -> tensor<5x5xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<16x64x30x126xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<16x64x30x126xf32>) -> tensor<16x64x30x126xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x64x64x256xf32>, tensor<5x5xf32>) outs (%init: tensor<16x64x30x126xf32>) -> tensor<16x64x30x126xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x64x30x126xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x64x30x126xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 16, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 30, 1], ["%arg6", 0, 126, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 322500028}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x16x512x256xf32>, tensor<5x5xf32>) outs (%init: tensor<64x16x508x252xf32>) -> tensor<64x16x508x252xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x16x512x256xf32>, tensor<5x5xf32>) outs (%init: tensor<64x16x508x252xf32>) -> tensor<64x16x508x252xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x16x512x256xf32>, %filter: tensor<5x5xf32>, %init: tensor<64x16x508x252xf32>) -> tensor<64x16x508x252xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x16x512x256xf32>, tensor<5x5xf32>) outs (%init: tensor<64x16x508x252xf32>) -> tensor<64x16x508x252xf32>\n  return %ret : tensor<64x16x508x252xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x16x512x256xf32>, %arg1: tensor<5x5xf32>, %arg2: tensor<64x16x508x252xf32>) -> tensor<64x16x508x252xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<64x16x512x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<64x16x508x252xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x16x508x252xf32>\n    memref.copy %1, %alloc : memref<64x16x508x252xf32> to memref<64x16x508x252xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 16 {\n        affine.for %arg5 = 0 to 508 {\n          affine.for %arg6 = 0 to 252 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<64x16x512x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x16x508x252xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x16x508x252xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<64x16x508x252xf32>\n    return %2 : tensor<64x16x508x252xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<64x16x508x252xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<64x16x512x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<64x16x512x256xf32>) -> tensor<64x16x512x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<5x5xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<5x5xf32>) -> tensor<5x5xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<64x16x508x252xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<64x16x508x252xf32>) -> tensor<64x16x508x252xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x16x512x256xf32>, tensor<5x5xf32>) outs (%init: tensor<64x16x508x252xf32>) -> tensor<64x16x508x252xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<64x16x508x252xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<64x16x508x252xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 16, 1], ["%arg5", 0, 508, 1], ["%arg6", 0, 252, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 10843832911}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x512x64x512xf32>, tensor<3x3xf32>) outs (%init: tensor<32x512x31x255xf32>) -> tensor<32x512x31x255xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x512x64x512xf32>, tensor<3x3xf32>) outs (%init: tensor<32x512x31x255xf32>) -> tensor<32x512x31x255xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x512x64x512xf32>, %filter: tensor<3x3xf32>, %init: tensor<32x512x31x255xf32>) -> tensor<32x512x31x255xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x512x64x512xf32>, tensor<3x3xf32>) outs (%init: tensor<32x512x31x255xf32>) -> tensor<32x512x31x255xf32>\n  return %ret : tensor<32x512x31x255xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x512x64x512xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<32x512x31x255xf32>) -> tensor<32x512x31x255xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<32x512x64x512xf32>\n    %1 = bufferization.to_memref %arg2 : memref<32x512x31x255xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x512x31x255xf32>\n    memref.copy %1, %alloc : memref<32x512x31x255xf32> to memref<32x512x31x255xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 31 {\n          affine.for %arg6 = 0 to 255 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<32x512x64x512xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x512x31x255xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x512x31x255xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<32x512x31x255xf32>\n    return %2 : tensor<32x512x31x255xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<32x512x31x255xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<32x512x64x512xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<32x512x64x512xf32>) -> tensor<32x512x64x512xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<32x512x31x255xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<32x512x31x255xf32>) -> tensor<32x512x31x255xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x512x64x512xf32>, tensor<3x3xf32>) outs (%init: tensor<32x512x31x255xf32>) -> tensor<32x512x31x255xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<32x512x31x255xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<32x512x31x255xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 31, 1], ["%arg6", 0, 255, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 2788720007}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x1024x512x32xf32>, tensor<1x1xf32>) outs (%init: tensor<256x1024x256x16xf32>) -> tensor<256x1024x256x16xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x1024x512x32xf32>, tensor<1x1xf32>) outs (%init: tensor<256x1024x256x16xf32>) -> tensor<256x1024x256x16xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x1024x512x32xf32>, %filter: tensor<1x1xf32>, %init: tensor<256x1024x256x16xf32>) -> tensor<256x1024x256x16xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x1024x512x32xf32>, tensor<1x1xf32>) outs (%init: tensor<256x1024x256x16xf32>) -> tensor<256x1024x256x16xf32>\n  return %ret : tensor<256x1024x256x16xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1024x512x32xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<256x1024x256x16xf32>) -> tensor<256x1024x256x16xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<256x1024x512x32xf32>\n    %1 = bufferization.to_memref %arg2 : memref<256x1024x256x16xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1024x256x16xf32>\n    memref.copy %1, %alloc : memref<256x1024x256x16xf32> to memref<256x1024x256x16xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 256 {\n          affine.for %arg6 = 0 to 16 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<256x1024x512x32xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1024x256x16xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1024x256x16xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<256x1024x256x16xf32>\n    return %2 : tensor<256x1024x256x16xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x1024x256x16xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x1024x512x32xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x1024x512x32xf32>) -> tensor<256x1024x512x32xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x1024x256x16xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x1024x256x16xf32>) -> tensor<256x1024x256x16xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x1024x512x32xf32>, tensor<1x1xf32>) outs (%init: tensor<256x1024x256x16xf32>) -> tensor<256x1024x256x16xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x1024x256x16xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x1024x256x16xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 256, 1], ["%arg6", 0, 16, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 2443317255}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x1024x512x64xf32>, tensor<1x1xf32>) outs (%init: tensor<64x1024x256x32xf32>) -> tensor<64x1024x256x32xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x1024x512x64xf32>, tensor<1x1xf32>) outs (%init: tensor<64x1024x256x32xf32>) -> tensor<64x1024x256x32xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x1024x512x64xf32>, %filter: tensor<1x1xf32>, %init: tensor<64x1024x256x32xf32>) -> tensor<64x1024x256x32xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x1024x512x64xf32>, tensor<1x1xf32>) outs (%init: tensor<64x1024x256x32xf32>) -> tensor<64x1024x256x32xf32>\n  return %ret : tensor<64x1024x256x32xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x1024x512x64xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<64x1024x256x32xf32>) -> tensor<64x1024x256x32xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<64x1024x512x64xf32>\n    %1 = bufferization.to_memref %arg2 : memref<64x1024x256x32xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x1024x256x32xf32>\n    memref.copy %1, %alloc : memref<64x1024x256x32xf32> to memref<64x1024x256x32xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 256 {\n          affine.for %arg6 = 0 to 32 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<64x1024x512x64xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x1024x256x32xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x1024x256x32xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<64x1024x256x32xf32>\n    return %2 : tensor<64x1024x256x32xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<64x1024x256x32xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<64x1024x512x64xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<64x1024x512x64xf32>) -> tensor<64x1024x512x64xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<64x1024x256x32xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<64x1024x256x32xf32>) -> tensor<64x1024x256x32xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x1024x512x64xf32>, tensor<1x1xf32>) outs (%init: tensor<64x1024x256x32xf32>) -> tensor<64x1024x256x32xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<64x1024x256x32xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<64x1024x256x32xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 256, 1], ["%arg6", 0, 32, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 1094948062}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x128x32x64xf32>, tensor<1x1xf32>) outs (%init: tensor<4x128x32x64xf32>) -> tensor<4x128x32x64xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x128x32x64xf32>, tensor<1x1xf32>) outs (%init: tensor<4x128x32x64xf32>) -> tensor<4x128x32x64xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<4x128x32x64xf32>, %filter: tensor<1x1xf32>, %init: tensor<4x128x32x64xf32>) -> tensor<4x128x32x64xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x128x32x64xf32>, tensor<1x1xf32>) outs (%init: tensor<4x128x32x64xf32>) -> tensor<4x128x32x64xf32>\n  return %ret : tensor<4x128x32x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<4x128x32x64xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<4x128x32x64xf32>) -> tensor<4x128x32x64xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<4x128x32x64xf32>\n    %1 = bufferization.to_memref %arg2 : memref<4x128x32x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<4x128x32x64xf32>\n    memref.copy %1, %alloc : memref<4x128x32x64xf32> to memref<4x128x32x64xf32>\n    affine.for %arg3 = 0 to 4 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 32 {\n          affine.for %arg6 = 0 to 64 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<4x128x32x64xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x128x32x64xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x128x32x64xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<4x128x32x64xf32>\n    return %2 : tensor<4x128x32x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<4x128x32x64xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<4x128x32x64xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<4x128x32x64xf32>) -> tensor<4x128x32x64xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<4x128x32x64xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<4x128x32x64xf32>) -> tensor<4x128x32x64xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x128x32x64xf32>, tensor<1x1xf32>) outs (%init: tensor<4x128x32x64xf32>) -> tensor<4x128x32x64xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<4x128x32x64xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<4x128x32x64xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 4, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 32, 1], ["%arg6", 0, 64, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 1517096}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x128x32x128xf32>, tensor<7x7xf32>) outs (%init: tensor<256x128x13x61xf32>) -> tensor<256x128x13x61xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x128x32x128xf32>, tensor<7x7xf32>) outs (%init: tensor<256x128x13x61xf32>) -> tensor<256x128x13x61xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x128x32x128xf32>, %filter: tensor<7x7xf32>, %init: tensor<256x128x13x61xf32>) -> tensor<256x128x13x61xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x128x32x128xf32>, tensor<7x7xf32>) outs (%init: tensor<256x128x13x61xf32>) -> tensor<256x128x13x61xf32>\n  return %ret : tensor<256x128x13x61xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x128x32x128xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<256x128x13x61xf32>) -> tensor<256x128x13x61xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<256x128x32x128xf32>\n    %1 = bufferization.to_memref %arg2 : memref<256x128x13x61xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x128x13x61xf32>\n    memref.copy %1, %alloc : memref<256x128x13x61xf32> to memref<256x128x13x61xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 13 {\n          affine.for %arg6 = 0 to 61 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<256x128x32x128xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x128x13x61xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x128x13x61xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<256x128x13x61xf32>\n    return %2 : tensor<256x128x13x61xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x128x13x61xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x128x32x128xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x128x32x128xf32>) -> tensor<256x128x32x128xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x128x13x61xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x128x13x61xf32>) -> tensor<256x128x13x61xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x128x32x128xf32>, tensor<7x7xf32>) outs (%init: tensor<256x128x13x61xf32>) -> tensor<256x128x13x61xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x128x13x61xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x128x13x61xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 13, 1], ["%arg6", 0, 61, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 5073437762}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x16x128x32xf32>, tensor<7x7xf32>) outs (%init: tensor<32x16x122x26xf32>) -> tensor<32x16x122x26xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x16x128x32xf32>, tensor<7x7xf32>) outs (%init: tensor<32x16x122x26xf32>) -> tensor<32x16x122x26xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x16x128x32xf32>, %filter: tensor<7x7xf32>, %init: tensor<32x16x122x26xf32>) -> tensor<32x16x122x26xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x16x128x32xf32>, tensor<7x7xf32>) outs (%init: tensor<32x16x122x26xf32>) -> tensor<32x16x122x26xf32>\n  return %ret : tensor<32x16x122x26xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x16x128x32xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<32x16x122x26xf32>) -> tensor<32x16x122x26xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<32x16x128x32xf32>\n    %1 = bufferization.to_memref %arg2 : memref<32x16x122x26xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x16x122x26xf32>\n    memref.copy %1, %alloc : memref<32x16x122x26xf32> to memref<32x16x122x26xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 16 {\n        affine.for %arg5 = 0 to 122 {\n          affine.for %arg6 = 0 to 26 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<32x16x128x32xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x16x122x26xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x16x122x26xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<32x16x122x26xf32>\n    return %2 : tensor<32x16x122x26xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<32x16x122x26xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<32x16x128x32xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<32x16x128x32xf32>) -> tensor<32x16x128x32xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<32x16x122x26xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<32x16x122x26xf32>) -> tensor<32x16x122x26xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x16x128x32xf32>, tensor<7x7xf32>) outs (%init: tensor<32x16x122x26xf32>) -> tensor<32x16x122x26xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<32x16x122x26xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<32x16x122x26xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 16, 1], ["%arg5", 0, 122, 1], ["%arg6", 0, 26, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 316865388}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x16x128x32xf32>, tensor<5x5xf32>) outs (%init: tensor<16x16x124x28xf32>) -> tensor<16x16x124x28xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x16x128x32xf32>, tensor<5x5xf32>) outs (%init: tensor<16x16x124x28xf32>) -> tensor<16x16x124x28xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<16x16x128x32xf32>, %filter: tensor<5x5xf32>, %init: tensor<16x16x124x28xf32>) -> tensor<16x16x124x28xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x16x128x32xf32>, tensor<5x5xf32>) outs (%init: tensor<16x16x124x28xf32>) -> tensor<16x16x124x28xf32>\n  return %ret : tensor<16x16x124x28xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<16x16x128x32xf32>, %arg1: tensor<5x5xf32>, %arg2: tensor<16x16x124x28xf32>) -> tensor<16x16x124x28xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<16x16x128x32xf32>\n    %1 = bufferization.to_memref %arg2 : memref<16x16x124x28xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x16x124x28xf32>\n    memref.copy %1, %alloc : memref<16x16x124x28xf32> to memref<16x16x124x28xf32>\n    affine.for %arg3 = 0 to 16 {\n      affine.for %arg4 = 0 to 16 {\n        affine.for %arg5 = 0 to 124 {\n          affine.for %arg6 = 0 to 28 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<16x16x128x32xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x16x124x28xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x16x124x28xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<16x16x124x28xf32>\n    return %2 : tensor<16x16x124x28xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x16x124x28xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<16x16x128x32xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<16x16x128x32xf32>) -> tensor<16x16x128x32xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<5x5xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<5x5xf32>) -> tensor<5x5xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<16x16x124x28xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<16x16x124x28xf32>) -> tensor<16x16x124x28xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x16x128x32xf32>, tensor<5x5xf32>) outs (%init: tensor<16x16x124x28xf32>) -> tensor<16x16x124x28xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x16x124x28xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x16x124x28xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 16, 1], ["%arg4", 0, 16, 1], ["%arg5", 0, 124, 1], ["%arg6", 0, 28, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 73613380}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x256x64x128xf32>, tensor<5x5xf32>) outs (%init: tensor<8x256x60x124xf32>) -> tensor<8x256x60x124xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x256x64x128xf32>, tensor<5x5xf32>) outs (%init: tensor<8x256x60x124xf32>) -> tensor<8x256x60x124xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<8x256x64x128xf32>, %filter: tensor<5x5xf32>, %init: tensor<8x256x60x124xf32>) -> tensor<8x256x60x124xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x256x64x128xf32>, tensor<5x5xf32>) outs (%init: tensor<8x256x60x124xf32>) -> tensor<8x256x60x124xf32>\n  return %ret : tensor<8x256x60x124xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<8x256x64x128xf32>, %arg1: tensor<5x5xf32>, %arg2: tensor<8x256x60x124xf32>) -> tensor<8x256x60x124xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<8x256x64x128xf32>\n    %1 = bufferization.to_memref %arg2 : memref<8x256x60x124xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<8x256x60x124xf32>\n    memref.copy %1, %alloc : memref<8x256x60x124xf32> to memref<8x256x60x124xf32>\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 60 {\n          affine.for %arg6 = 0 to 124 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<8x256x64x128xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x256x60x124xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x256x60x124xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<8x256x60x124xf32>\n    return %2 : tensor<8x256x60x124xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<8x256x60x124xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<8x256x64x128xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<8x256x64x128xf32>) -> tensor<8x256x64x128xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<5x5xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<5x5xf32>) -> tensor<5x5xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<8x256x60x124xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<8x256x60x124xf32>) -> tensor<8x256x60x124xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x256x64x128xf32>, tensor<5x5xf32>) outs (%init: tensor<8x256x60x124xf32>) -> tensor<8x256x60x124xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<8x256x60x124xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<8x256x60x124xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 8, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 60, 1], ["%arg6", 0, 124, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 1260890877}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x8x64x32xf32>, tensor<7x7xf32>) outs (%init: tensor<64x8x29x13xf32>) -> tensor<64x8x29x13xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x8x64x32xf32>, tensor<7x7xf32>) outs (%init: tensor<64x8x29x13xf32>) -> tensor<64x8x29x13xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x8x64x32xf32>, %filter: tensor<7x7xf32>, %init: tensor<64x8x29x13xf32>) -> tensor<64x8x29x13xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x8x64x32xf32>, tensor<7x7xf32>) outs (%init: tensor<64x8x29x13xf32>) -> tensor<64x8x29x13xf32>\n  return %ret : tensor<64x8x29x13xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x8x64x32xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<64x8x29x13xf32>) -> tensor<64x8x29x13xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<64x8x64x32xf32>\n    %1 = bufferization.to_memref %arg2 : memref<64x8x29x13xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x8x29x13xf32>\n    memref.copy %1, %alloc : memref<64x8x29x13xf32> to memref<64x8x29x13xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 8 {\n        affine.for %arg5 = 0 to 29 {\n          affine.for %arg6 = 0 to 13 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<64x8x64x32xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x8x29x13xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x8x29x13xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<64x8x29x13xf32>\n    return %2 : tensor<64x8x29x13xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<64x8x29x13xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<64x8x64x32xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<64x8x64x32xf32>) -> tensor<64x8x64x32xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<64x8x29x13xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<64x8x29x13xf32>) -> tensor<64x8x29x13xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x8x64x32xf32>, tensor<7x7xf32>) outs (%init: tensor<64x8x29x13xf32>) -> tensor<64x8x29x13xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<64x8x29x13xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<64x8x29x13xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 8, 1], ["%arg5", 0, 29, 1], ["%arg6", 0, 13, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 37662040}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x64x256x32xf32>, tensor<7x7xf32>) outs (%init: tensor<4x64x250x26xf32>) -> tensor<4x64x250x26xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x64x256x32xf32>, tensor<7x7xf32>) outs (%init: tensor<4x64x250x26xf32>) -> tensor<4x64x250x26xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<4x64x256x32xf32>, %filter: tensor<7x7xf32>, %init: tensor<4x64x250x26xf32>) -> tensor<4x64x250x26xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x64x256x32xf32>, tensor<7x7xf32>) outs (%init: tensor<4x64x250x26xf32>) -> tensor<4x64x250x26xf32>\n  return %ret : tensor<4x64x250x26xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<4x64x256x32xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<4x64x250x26xf32>) -> tensor<4x64x250x26xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<4x64x256x32xf32>\n    %1 = bufferization.to_memref %arg2 : memref<4x64x250x26xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<4x64x250x26xf32>\n    memref.copy %1, %alloc : memref<4x64x250x26xf32> to memref<4x64x250x26xf32>\n    affine.for %arg3 = 0 to 4 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 250 {\n          affine.for %arg6 = 0 to 26 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<4x64x256x32xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x64x250x26xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x64x250x26xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<4x64x250x26xf32>\n    return %2 : tensor<4x64x250x26xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<4x64x250x26xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<4x64x256x32xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<4x64x256x32xf32>) -> tensor<4x64x256x32xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<4x64x250x26xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<4x64x250x26xf32>) -> tensor<4x64x250x26xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x64x256x32xf32>, tensor<7x7xf32>) outs (%init: tensor<4x64x250x26xf32>) -> tensor<4x64x250x26xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<4x64x250x26xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<4x64x250x26xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 4, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 250, 1], ["%arg6", 0, 26, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 324595318}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x64x512x32xf32>, tensor<7x7xf32>) outs (%init: tensor<8x64x506x26xf32>) -> tensor<8x64x506x26xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x64x512x32xf32>, tensor<7x7xf32>) outs (%init: tensor<8x64x506x26xf32>) -> tensor<8x64x506x26xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<8x64x512x32xf32>, %filter: tensor<7x7xf32>, %init: tensor<8x64x506x26xf32>) -> tensor<8x64x506x26xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x64x512x32xf32>, tensor<7x7xf32>) outs (%init: tensor<8x64x506x26xf32>) -> tensor<8x64x506x26xf32>\n  return %ret : tensor<8x64x506x26xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<8x64x512x32xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<8x64x506x26xf32>) -> tensor<8x64x506x26xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<8x64x512x32xf32>\n    %1 = bufferization.to_memref %arg2 : memref<8x64x506x26xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<8x64x506x26xf32>\n    memref.copy %1, %alloc : memref<8x64x506x26xf32> to memref<8x64x506x26xf32>\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 506 {\n          affine.for %arg6 = 0 to 26 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<8x64x512x32xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x64x506x26xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x64x506x26xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<8x64x506x26xf32>\n    return %2 : tensor<8x64x506x26xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<8x64x506x26xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<8x64x512x32xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<8x64x512x32xf32>) -> tensor<8x64x512x32xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<8x64x506x26xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<8x64x506x26xf32>) -> tensor<8x64x506x26xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x64x512x32xf32>, tensor<7x7xf32>) outs (%init: tensor<8x64x506x26xf32>) -> tensor<8x64x506x26xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<8x64x506x26xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<8x64x506x26xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 8, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 506, 1], ["%arg6", 0, 26, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 1314631346}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x512x32x128xf32>, tensor<1x1xf32>) outs (%init: tensor<256x512x16x64xf32>) -> tensor<256x512x16x64xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x512x32x128xf32>, tensor<1x1xf32>) outs (%init: tensor<256x512x16x64xf32>) -> tensor<256x512x16x64xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x512x32x128xf32>, %filter: tensor<1x1xf32>, %init: tensor<256x512x16x64xf32>) -> tensor<256x512x16x64xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x512x32x128xf32>, tensor<1x1xf32>) outs (%init: tensor<256x512x16x64xf32>) -> tensor<256x512x16x64xf32>\n  return %ret : tensor<256x512x16x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x512x32x128xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<256x512x16x64xf32>) -> tensor<256x512x16x64xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<256x512x32x128xf32>\n    %1 = bufferization.to_memref %arg2 : memref<256x512x16x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x512x16x64xf32>\n    memref.copy %1, %alloc : memref<256x512x16x64xf32> to memref<256x512x16x64xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 16 {\n          affine.for %arg6 = 0 to 64 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<256x512x32x128xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x512x16x64xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x512x16x64xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<256x512x16x64xf32>\n    return %2 : tensor<256x512x16x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x512x16x64xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x512x32x128xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x512x32x128xf32>) -> tensor<256x512x32x128xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x512x16x64xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x512x16x64xf32>) -> tensor<256x512x16x64xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x512x32x128xf32>, tensor<1x1xf32>) outs (%init: tensor<256x512x16x64xf32>) -> tensor<256x512x16x64xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x512x16x64xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x512x16x64xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 16, 1], ["%arg6", 0, 64, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 348851868}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x256x256x512xf32>, tensor<1x1xf32>) outs (%init: tensor<64x256x128x256xf32>) -> tensor<64x256x128x256xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x256x256x512xf32>, tensor<1x1xf32>) outs (%init: tensor<64x256x128x256xf32>) -> tensor<64x256x128x256xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x256x256x512xf32>, %filter: tensor<1x1xf32>, %init: tensor<64x256x128x256xf32>) -> tensor<64x256x128x256xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x256x256x512xf32>, tensor<1x1xf32>) outs (%init: tensor<64x256x128x256xf32>) -> tensor<64x256x128x256xf32>\n  return %ret : tensor<64x256x128x256xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x256x256x512xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<64x256x128x256xf32>) -> tensor<64x256x128x256xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<64x256x256x512xf32>\n    %1 = bufferization.to_memref %arg2 : memref<64x256x128x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x256x128x256xf32>\n    memref.copy %1, %alloc : memref<64x256x128x256xf32> to memref<64x256x128x256xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 128 {\n          affine.for %arg6 = 0 to 256 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<64x256x256x512xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x256x128x256xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x256x128x256xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<64x256x128x256xf32>\n    return %2 : tensor<64x256x128x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<64x256x128x256xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<64x256x256x512xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<64x256x256x512xf32>) -> tensor<64x256x256x512xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<64x256x128x256xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<64x256x128x256xf32>) -> tensor<64x256x128x256xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x256x256x512xf32>, tensor<1x1xf32>) outs (%init: tensor<64x256x128x256xf32>) -> tensor<64x256x128x256xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<64x256x128x256xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<64x256x128x256xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 128, 1], ["%arg6", 0, 256, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 1360771230}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x128x32x128xf32>, tensor<7x7xf32>) outs (%init: tensor<16x128x13x61xf32>) -> tensor<16x128x13x61xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x128x32x128xf32>, tensor<7x7xf32>) outs (%init: tensor<16x128x13x61xf32>) -> tensor<16x128x13x61xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<16x128x32x128xf32>, %filter: tensor<7x7xf32>, %init: tensor<16x128x13x61xf32>) -> tensor<16x128x13x61xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x128x32x128xf32>, tensor<7x7xf32>) outs (%init: tensor<16x128x13x61xf32>) -> tensor<16x128x13x61xf32>\n  return %ret : tensor<16x128x13x61xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<16x128x32x128xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<16x128x13x61xf32>) -> tensor<16x128x13x61xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<16x128x32x128xf32>\n    %1 = bufferization.to_memref %arg2 : memref<16x128x13x61xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x128x13x61xf32>\n    memref.copy %1, %alloc : memref<16x128x13x61xf32> to memref<16x128x13x61xf32>\n    affine.for %arg3 = 0 to 16 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 13 {\n          affine.for %arg6 = 0 to 61 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<16x128x32x128xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x128x13x61xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x128x13x61xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<16x128x13x61xf32>\n    return %2 : tensor<16x128x13x61xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x128x13x61xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<16x128x32x128xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<16x128x32x128xf32>) -> tensor<16x128x32x128xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<16x128x13x61xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<16x128x13x61xf32>) -> tensor<16x128x13x61xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x128x32x128xf32>, tensor<7x7xf32>) outs (%init: tensor<16x128x13x61xf32>) -> tensor<16x128x13x61xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x128x13x61xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x128x13x61xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 16, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 13, 1], ["%arg6", 0, 61, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 317353359}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x128x128x512xf32>, tensor<7x7xf32>) outs (%init: tensor<8x128x122x506xf32>) -> tensor<8x128x122x506xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x128x128x512xf32>, tensor<7x7xf32>) outs (%init: tensor<8x128x122x506xf32>) -> tensor<8x128x122x506xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<8x128x128x512xf32>, %filter: tensor<7x7xf32>, %init: tensor<8x128x122x506xf32>) -> tensor<8x128x122x506xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x128x128x512xf32>, tensor<7x7xf32>) outs (%init: tensor<8x128x122x506xf32>) -> tensor<8x128x122x506xf32>\n  return %ret : tensor<8x128x122x506xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<8x128x128x512xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<8x128x122x506xf32>) -> tensor<8x128x122x506xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<8x128x128x512xf32>\n    %1 = bufferization.to_memref %arg2 : memref<8x128x122x506xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<8x128x122x506xf32>\n    memref.copy %1, %alloc : memref<8x128x122x506xf32> to memref<8x128x122x506xf32>\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 122 {\n          affine.for %arg6 = 0 to 506 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<8x128x128x512xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x128x122x506xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x128x122x506xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<8x128x122x506xf32>\n    return %2 : tensor<8x128x122x506xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<8x128x122x506xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<8x128x128x512xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<8x128x128x512xf32>) -> tensor<8x128x128x512xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<8x128x122x506xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<8x128x122x506xf32>) -> tensor<8x128x122x506xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x128x128x512xf32>, tensor<7x7xf32>) outs (%init: tensor<8x128x122x506xf32>) -> tensor<8x128x122x506xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<8x128x122x506xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<8x128x122x506xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 8, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 122, 1], ["%arg6", 0, 506, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 12315415051}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x16x128x32xf32>, tensor<7x7xf32>) outs (%init: tensor<32x16x61x13xf32>) -> tensor<32x16x61x13xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x16x128x32xf32>, tensor<7x7xf32>) outs (%init: tensor<32x16x61x13xf32>) -> tensor<32x16x61x13xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x16x128x32xf32>, %filter: tensor<7x7xf32>, %init: tensor<32x16x61x13xf32>) -> tensor<32x16x61x13xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x16x128x32xf32>, tensor<7x7xf32>) outs (%init: tensor<32x16x61x13xf32>) -> tensor<32x16x61x13xf32>\n  return %ret : tensor<32x16x61x13xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x16x128x32xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<32x16x61x13xf32>) -> tensor<32x16x61x13xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<32x16x128x32xf32>\n    %1 = bufferization.to_memref %arg2 : memref<32x16x61x13xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x16x61x13xf32>\n    memref.copy %1, %alloc : memref<32x16x61x13xf32> to memref<32x16x61x13xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 16 {\n        affine.for %arg5 = 0 to 61 {\n          affine.for %arg6 = 0 to 13 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<32x16x128x32xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x16x61x13xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x16x61x13xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<32x16x61x13xf32>\n    return %2 : tensor<32x16x61x13xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<32x16x61x13xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<32x16x128x32xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<32x16x128x32xf32>) -> tensor<32x16x128x32xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<32x16x61x13xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<32x16x61x13xf32>) -> tensor<32x16x61x13xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x16x128x32xf32>, tensor<7x7xf32>) outs (%init: tensor<32x16x61x13xf32>) -> tensor<32x16x61x13xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<32x16x61x13xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<32x16x61x13xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 16, 1], ["%arg5", 0, 61, 1], ["%arg6", 0, 13, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 79416313}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x128x64x64xf32>, tensor<1x1xf32>) outs (%init: tensor<4x128x32x32xf32>) -> tensor<4x128x32x32xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x128x64x64xf32>, tensor<1x1xf32>) outs (%init: tensor<4x128x32x32xf32>) -> tensor<4x128x32x32xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<4x128x64x64xf32>, %filter: tensor<1x1xf32>, %init: tensor<4x128x32x32xf32>) -> tensor<4x128x32x32xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x128x64x64xf32>, tensor<1x1xf32>) outs (%init: tensor<4x128x32x32xf32>) -> tensor<4x128x32x32xf32>\n  return %ret : tensor<4x128x32x32xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<4x128x64x64xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<4x128x32x32xf32>) -> tensor<4x128x32x32xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<4x128x64x64xf32>\n    %1 = bufferization.to_memref %arg2 : memref<4x128x32x32xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<4x128x32x32xf32>\n    memref.copy %1, %alloc : memref<4x128x32x32xf32> to memref<4x128x32x32xf32>\n    affine.for %arg3 = 0 to 4 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 32 {\n          affine.for %arg6 = 0 to 32 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<4x128x64x64xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x128x32x32xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x128x32x32xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<4x128x32x32xf32>\n    return %2 : tensor<4x128x32x32xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<4x128x32x32xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<4x128x64x64xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<4x128x64x64xf32>) -> tensor<4x128x64x64xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<4x128x32x32xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<4x128x32x32xf32>) -> tensor<4x128x32x32xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x128x64x64xf32>, tensor<1x1xf32>) outs (%init: tensor<4x128x32x32xf32>) -> tensor<4x128x32x32xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<4x128x32x32xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<4x128x32x32xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 4, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 32, 1], ["%arg6", 0, 32, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 756756}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x1024x32x64xf32>, tensor<7x7xf32>) outs (%init: tensor<8x1024x26x58xf32>) -> tensor<8x1024x26x58xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x1024x32x64xf32>, tensor<7x7xf32>) outs (%init: tensor<8x1024x26x58xf32>) -> tensor<8x1024x26x58xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<8x1024x32x64xf32>, %filter: tensor<7x7xf32>, %init: tensor<8x1024x26x58xf32>) -> tensor<8x1024x26x58xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x1024x32x64xf32>, tensor<7x7xf32>) outs (%init: tensor<8x1024x26x58xf32>) -> tensor<8x1024x26x58xf32>\n  return %ret : tensor<8x1024x26x58xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<8x1024x32x64xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<8x1024x26x58xf32>) -> tensor<8x1024x26x58xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<8x1024x32x64xf32>\n    %1 = bufferization.to_memref %arg2 : memref<8x1024x26x58xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<8x1024x26x58xf32>\n    memref.copy %1, %alloc : memref<8x1024x26x58xf32> to memref<8x1024x26x58xf32>\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 26 {\n          affine.for %arg6 = 0 to 58 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<8x1024x32x64xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x1024x26x58xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x1024x26x58xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<8x1024x26x58xf32>\n    return %2 : tensor<8x1024x26x58xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<8x1024x26x58xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<8x1024x32x64xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<8x1024x32x64xf32>) -> tensor<8x1024x32x64xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<8x1024x26x58xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<8x1024x26x58xf32>) -> tensor<8x1024x26x58xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x1024x32x64xf32>, tensor<7x7xf32>) outs (%init: tensor<8x1024x26x58xf32>) -> tensor<8x1024x26x58xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<8x1024x26x58xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<8x1024x26x58xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 8, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 26, 1], ["%arg6", 0, 58, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 2410239902}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x8x128x256xf32>, tensor<7x7xf32>) outs (%init: tensor<32x8x61x125xf32>) -> tensor<32x8x61x125xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x8x128x256xf32>, tensor<7x7xf32>) outs (%init: tensor<32x8x61x125xf32>) -> tensor<32x8x61x125xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x8x128x256xf32>, %filter: tensor<7x7xf32>, %init: tensor<32x8x61x125xf32>) -> tensor<32x8x61x125xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x8x128x256xf32>, tensor<7x7xf32>) outs (%init: tensor<32x8x61x125xf32>) -> tensor<32x8x61x125xf32>\n  return %ret : tensor<32x8x61x125xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x8x128x256xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<32x8x61x125xf32>) -> tensor<32x8x61x125xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<32x8x128x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<32x8x61x125xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x8x61x125xf32>\n    memref.copy %1, %alloc : memref<32x8x61x125xf32> to memref<32x8x61x125xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 8 {\n        affine.for %arg5 = 0 to 61 {\n          affine.for %arg6 = 0 to 125 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<32x8x128x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x8x61x125xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x8x61x125xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<32x8x61x125xf32>\n    return %2 : tensor<32x8x61x125xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<32x8x61x125xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<32x8x128x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<32x8x128x256xf32>) -> tensor<32x8x128x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<32x8x61x125xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<32x8x61x125xf32>) -> tensor<32x8x61x125xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x8x128x256xf32>, tensor<7x7xf32>) outs (%init: tensor<32x8x61x125xf32>) -> tensor<32x8x61x125xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<32x8x61x125xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<32x8x61x125xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 8, 1], ["%arg5", 0, 61, 1], ["%arg6", 0, 125, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 381203447}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x8x256x64xf32>, tensor<1x1xf32>) outs (%init: tensor<4x8x256x64xf32>) -> tensor<4x8x256x64xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x8x256x64xf32>, tensor<1x1xf32>) outs (%init: tensor<4x8x256x64xf32>) -> tensor<4x8x256x64xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<4x8x256x64xf32>, %filter: tensor<1x1xf32>, %init: tensor<4x8x256x64xf32>) -> tensor<4x8x256x64xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x8x256x64xf32>, tensor<1x1xf32>) outs (%init: tensor<4x8x256x64xf32>) -> tensor<4x8x256x64xf32>\n  return %ret : tensor<4x8x256x64xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<4x8x256x64xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<4x8x256x64xf32>) -> tensor<4x8x256x64xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<4x8x256x64xf32>\n    %1 = bufferization.to_memref %arg2 : memref<4x8x256x64xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<4x8x256x64xf32>\n    memref.copy %1, %alloc : memref<4x8x256x64xf32> to memref<4x8x256x64xf32>\n    affine.for %arg3 = 0 to 4 {\n      affine.for %arg4 = 0 to 8 {\n        affine.for %arg5 = 0 to 256 {\n          affine.for %arg6 = 0 to 64 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<4x8x256x64xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x8x256x64xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x8x256x64xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<4x8x256x64xf32>\n    return %2 : tensor<4x8x256x64xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<4x8x256x64xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<4x8x256x64xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<4x8x256x64xf32>) -> tensor<4x8x256x64xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<4x8x256x64xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<4x8x256x64xf32>) -> tensor<4x8x256x64xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x8x256x64xf32>, tensor<1x1xf32>) outs (%init: tensor<4x8x256x64xf32>) -> tensor<4x8x256x64xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<4x8x256x64xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<4x8x256x64xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 4, 1], ["%arg4", 0, 8, 1], ["%arg5", 0, 256, 1], ["%arg6", 0, 64, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 749022}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x128x128x512xf32>, tensor<1x1xf32>) outs (%init: tensor<64x128x64x256xf32>) -> tensor<64x128x64x256xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x128x128x512xf32>, tensor<1x1xf32>) outs (%init: tensor<64x128x64x256xf32>) -> tensor<64x128x64x256xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x128x128x512xf32>, %filter: tensor<1x1xf32>, %init: tensor<64x128x64x256xf32>) -> tensor<64x128x64x256xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x128x128x512xf32>, tensor<1x1xf32>) outs (%init: tensor<64x128x64x256xf32>) -> tensor<64x128x64x256xf32>\n  return %ret : tensor<64x128x64x256xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x128x128x512xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<64x128x64x256xf32>) -> tensor<64x128x64x256xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<64x128x128x512xf32>\n    %1 = bufferization.to_memref %arg2 : memref<64x128x64x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x128x64x256xf32>\n    memref.copy %1, %alloc : memref<64x128x64x256xf32> to memref<64x128x64x256xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 64 {\n          affine.for %arg6 = 0 to 256 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<64x128x128x512xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x128x64x256xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x128x64x256xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<64x128x64x256xf32>\n    return %2 : tensor<64x128x64x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<64x128x64x256xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<64x128x128x512xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<64x128x128x512xf32>) -> tensor<64x128x128x512xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<64x128x64x256xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<64x128x64x256xf32>) -> tensor<64x128x64x256xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x128x128x512xf32>, tensor<1x1xf32>) outs (%init: tensor<64x128x64x256xf32>) -> tensor<64x128x64x256xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<64x128x64x256xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<64x128x64x256xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 64, 1], ["%arg6", 0, 256, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 256859042}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x8x64x512xf32>, tensor<1x1xf32>) outs (%init: tensor<8x8x32x256xf32>) -> tensor<8x8x32x256xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x8x64x512xf32>, tensor<1x1xf32>) outs (%init: tensor<8x8x32x256xf32>) -> tensor<8x8x32x256xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<8x8x64x512xf32>, %filter: tensor<1x1xf32>, %init: tensor<8x8x32x256xf32>) -> tensor<8x8x32x256xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x8x64x512xf32>, tensor<1x1xf32>) outs (%init: tensor<8x8x32x256xf32>) -> tensor<8x8x32x256xf32>\n  return %ret : tensor<8x8x32x256xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<8x8x64x512xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<8x8x32x256xf32>) -> tensor<8x8x32x256xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<8x8x64x512xf32>\n    %1 = bufferization.to_memref %arg2 : memref<8x8x32x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<8x8x32x256xf32>\n    memref.copy %1, %alloc : memref<8x8x32x256xf32> to memref<8x8x32x256xf32>\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 8 {\n        affine.for %arg5 = 0 to 32 {\n          affine.for %arg6 = 0 to 256 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<8x8x64x512xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x8x32x256xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x8x32x256xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<8x8x32x256xf32>\n    return %2 : tensor<8x8x32x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<8x8x32x256xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<8x8x64x512xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<8x8x64x512xf32>) -> tensor<8x8x64x512xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<8x8x32x256xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<8x8x32x256xf32>) -> tensor<8x8x32x256xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x8x64x512xf32>, tensor<1x1xf32>) outs (%init: tensor<8x8x32x256xf32>) -> tensor<8x8x32x256xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<8x8x32x256xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<8x8x32x256xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 8, 1], ["%arg4", 0, 8, 1], ["%arg5", 0, 32, 1], ["%arg6", 0, 256, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 729112}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x32x32x256xf32>, tensor<1x1xf32>) outs (%init: tensor<16x32x16x128xf32>) -> tensor<16x32x16x128xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x32x32x256xf32>, tensor<1x1xf32>) outs (%init: tensor<16x32x16x128xf32>) -> tensor<16x32x16x128xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<16x32x32x256xf32>, %filter: tensor<1x1xf32>, %init: tensor<16x32x16x128xf32>) -> tensor<16x32x16x128xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x32x32x256xf32>, tensor<1x1xf32>) outs (%init: tensor<16x32x16x128xf32>) -> tensor<16x32x16x128xf32>\n  return %ret : tensor<16x32x16x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<16x32x32x256xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<16x32x16x128xf32>) -> tensor<16x32x16x128xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<16x32x32x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<16x32x16x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x32x16x128xf32>\n    memref.copy %1, %alloc : memref<16x32x16x128xf32> to memref<16x32x16x128xf32>\n    affine.for %arg3 = 0 to 16 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 16 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<16x32x32x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x32x16x128xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x32x16x128xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<16x32x16x128xf32>\n    return %2 : tensor<16x32x16x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x32x16x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<16x32x32x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<16x32x32x256xf32>) -> tensor<16x32x32x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<16x32x16x128xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<16x32x16x128xf32>) -> tensor<16x32x16x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x32x32x256xf32>, tensor<1x1xf32>) outs (%init: tensor<16x32x16x128xf32>) -> tensor<16x32x16x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x32x16x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x32x16x128xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 16, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 16, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 1470249}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x8x128x128xf32>, tensor<3x3xf32>) outs (%init: tensor<8x8x126x126xf32>) -> tensor<8x8x126x126xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x8x128x128xf32>, tensor<3x3xf32>) outs (%init: tensor<8x8x126x126xf32>) -> tensor<8x8x126x126xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<8x8x128x128xf32>, %filter: tensor<3x3xf32>, %init: tensor<8x8x126x126xf32>) -> tensor<8x8x126x126xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x8x128x128xf32>, tensor<3x3xf32>) outs (%init: tensor<8x8x126x126xf32>) -> tensor<8x8x126x126xf32>\n  return %ret : tensor<8x8x126x126xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<8x8x128x128xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<8x8x126x126xf32>) -> tensor<8x8x126x126xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<8x8x128x128xf32>\n    %1 = bufferization.to_memref %arg2 : memref<8x8x126x126xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<8x8x126x126xf32>\n    memref.copy %1, %alloc : memref<8x8x126x126xf32> to memref<8x8x126x126xf32>\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 8 {\n        affine.for %arg5 = 0 to 126 {\n          affine.for %arg6 = 0 to 126 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<8x8x128x128xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x8x126x126xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x8x126x126xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<8x8x126x126xf32>\n    return %2 : tensor<8x8x126x126xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<8x8x126x126xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<8x8x128x128xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<8x8x128x128xf32>) -> tensor<8x8x128x128xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<8x8x126x126xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<8x8x126x126xf32>) -> tensor<8x8x126x126xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x8x128x128xf32>, tensor<3x3xf32>) outs (%init: tensor<8x8x126x126xf32>) -> tensor<8x8x126x126xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<8x8x126x126xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<8x8x126x126xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 8, 1], ["%arg4", 0, 8, 1], ["%arg5", 0, 126, 1], ["%arg6", 0, 126, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 21796545}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x256x512x32xf32>, tensor<3x3xf32>) outs (%init: tensor<4x256x510x30xf32>) -> tensor<4x256x510x30xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x256x512x32xf32>, tensor<3x3xf32>) outs (%init: tensor<4x256x510x30xf32>) -> tensor<4x256x510x30xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<4x256x512x32xf32>, %filter: tensor<3x3xf32>, %init: tensor<4x256x510x30xf32>) -> tensor<4x256x510x30xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x256x512x32xf32>, tensor<3x3xf32>) outs (%init: tensor<4x256x510x30xf32>) -> tensor<4x256x510x30xf32>\n  return %ret : tensor<4x256x510x30xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<4x256x512x32xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<4x256x510x30xf32>) -> tensor<4x256x510x30xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<4x256x512x32xf32>\n    %1 = bufferization.to_memref %arg2 : memref<4x256x510x30xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<4x256x510x30xf32>\n    memref.copy %1, %alloc : memref<4x256x510x30xf32> to memref<4x256x510x30xf32>\n    affine.for %arg3 = 0 to 4 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 510 {\n          affine.for %arg6 = 0 to 30 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<4x256x512x32xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x256x510x30xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x256x510x30xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<4x256x510x30xf32>\n    return %2 : tensor<4x256x510x30xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<4x256x510x30xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<4x256x512x32xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<4x256x512x32xf32>) -> tensor<4x256x512x32xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<4x256x510x30xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<4x256x510x30xf32>) -> tensor<4x256x510x30xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x256x512x32xf32>, tensor<3x3xf32>) outs (%init: tensor<4x256x510x30xf32>) -> tensor<4x256x510x30xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<4x256x510x30xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<4x256x510x30xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 4, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 510, 1], ["%arg6", 0, 30, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 341555838}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x16x64x256xf32>, tensor<1x1xf32>) outs (%init: tensor<4x16x32x128xf32>) -> tensor<4x16x32x128xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x16x64x256xf32>, tensor<1x1xf32>) outs (%init: tensor<4x16x32x128xf32>) -> tensor<4x16x32x128xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<4x16x64x256xf32>, %filter: tensor<1x1xf32>, %init: tensor<4x16x32x128xf32>) -> tensor<4x16x32x128xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x16x64x256xf32>, tensor<1x1xf32>) outs (%init: tensor<4x16x32x128xf32>) -> tensor<4x16x32x128xf32>\n  return %ret : tensor<4x16x32x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<4x16x64x256xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<4x16x32x128xf32>) -> tensor<4x16x32x128xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<4x16x64x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<4x16x32x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<4x16x32x128xf32>\n    memref.copy %1, %alloc : memref<4x16x32x128xf32> to memref<4x16x32x128xf32>\n    affine.for %arg3 = 0 to 4 {\n      affine.for %arg4 = 0 to 16 {\n        affine.for %arg5 = 0 to 32 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<4x16x64x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x16x32x128xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x16x32x128xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<4x16x32x128xf32>\n    return %2 : tensor<4x16x32x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<4x16x32x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<4x16x64x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<4x16x64x256xf32>) -> tensor<4x16x64x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<4x16x32x128xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<4x16x32x128xf32>) -> tensor<4x16x32x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x16x64x256xf32>, tensor<1x1xf32>) outs (%init: tensor<4x16x32x128xf32>) -> tensor<4x16x32x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<4x16x32x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<4x16x32x128xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 4, 1], ["%arg4", 0, 16, 1], ["%arg5", 0, 32, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 368332}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x1024x32x128xf32>, tensor<1x1xf32>) outs (%init: tensor<64x1024x32x128xf32>) -> tensor<64x1024x32x128xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x1024x32x128xf32>, tensor<1x1xf32>) outs (%init: tensor<64x1024x32x128xf32>) -> tensor<64x1024x32x128xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x1024x32x128xf32>, %filter: tensor<1x1xf32>, %init: tensor<64x1024x32x128xf32>) -> tensor<64x1024x32x128xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x1024x32x128xf32>, tensor<1x1xf32>) outs (%init: tensor<64x1024x32x128xf32>) -> tensor<64x1024x32x128xf32>\n  return %ret : tensor<64x1024x32x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x1024x32x128xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<64x1024x32x128xf32>) -> tensor<64x1024x32x128xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<64x1024x32x128xf32>\n    %1 = bufferization.to_memref %arg2 : memref<64x1024x32x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x1024x32x128xf32>\n    memref.copy %1, %alloc : memref<64x1024x32x128xf32> to memref<64x1024x32x128xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 32 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<64x1024x32x128xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x1024x32x128xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x1024x32x128xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<64x1024x32x128xf32>\n    return %2 : tensor<64x1024x32x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<64x1024x32x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<64x1024x32x128xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<64x1024x32x128xf32>) -> tensor<64x1024x32x128xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<64x1024x32x128xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<64x1024x32x128xf32>) -> tensor<64x1024x32x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x1024x32x128xf32>, tensor<1x1xf32>) outs (%init: tensor<64x1024x32x128xf32>) -> tensor<64x1024x32x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<64x1024x32x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<64x1024x32x128xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 32, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 401118345}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x1024x32x512xf32>, tensor<1x1xf32>) outs (%init: tensor<8x1024x32x512xf32>) -> tensor<8x1024x32x512xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x1024x32x512xf32>, tensor<1x1xf32>) outs (%init: tensor<8x1024x32x512xf32>) -> tensor<8x1024x32x512xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<8x1024x32x512xf32>, %filter: tensor<1x1xf32>, %init: tensor<8x1024x32x512xf32>) -> tensor<8x1024x32x512xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x1024x32x512xf32>, tensor<1x1xf32>) outs (%init: tensor<8x1024x32x512xf32>) -> tensor<8x1024x32x512xf32>\n  return %ret : tensor<8x1024x32x512xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<8x1024x32x512xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<8x1024x32x512xf32>) -> tensor<8x1024x32x512xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<8x1024x32x512xf32>\n    %1 = bufferization.to_memref %arg2 : memref<8x1024x32x512xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<8x1024x32x512xf32>\n    memref.copy %1, %alloc : memref<8x1024x32x512xf32> to memref<8x1024x32x512xf32>\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 32 {\n          affine.for %arg6 = 0 to 512 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<8x1024x32x512xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x1024x32x512xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x1024x32x512xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<8x1024x32x512xf32>\n    return %2 : tensor<8x1024x32x512xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<8x1024x32x512xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<8x1024x32x512xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<8x1024x32x512xf32>) -> tensor<8x1024x32x512xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<8x1024x32x512xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<8x1024x32x512xf32>) -> tensor<8x1024x32x512xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x1024x32x512xf32>, tensor<1x1xf32>) outs (%init: tensor<8x1024x32x512xf32>) -> tensor<8x1024x32x512xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<8x1024x32x512xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<8x1024x32x512xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 8, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 32, 1], ["%arg6", 0, 512, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 190301850}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x32x32x128xf32>, tensor<3x3xf32>) outs (%init: tensor<16x32x15x63xf32>) -> tensor<16x32x15x63xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x32x32x128xf32>, tensor<3x3xf32>) outs (%init: tensor<16x32x15x63xf32>) -> tensor<16x32x15x63xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<16x32x32x128xf32>, %filter: tensor<3x3xf32>, %init: tensor<16x32x15x63xf32>) -> tensor<16x32x15x63xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x32x32x128xf32>, tensor<3x3xf32>) outs (%init: tensor<16x32x15x63xf32>) -> tensor<16x32x15x63xf32>\n  return %ret : tensor<16x32x15x63xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<16x32x32x128xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<16x32x15x63xf32>) -> tensor<16x32x15x63xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<16x32x32x128xf32>\n    %1 = bufferization.to_memref %arg2 : memref<16x32x15x63xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x32x15x63xf32>\n    memref.copy %1, %alloc : memref<16x32x15x63xf32> to memref<16x32x15x63xf32>\n    affine.for %arg3 = 0 to 16 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 15 {\n          affine.for %arg6 = 0 to 63 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<16x32x32x128xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x32x15x63xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x32x15x63xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<16x32x15x63xf32>\n    return %2 : tensor<16x32x15x63xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x32x15x63xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<16x32x32x128xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<16x32x32x128xf32>) -> tensor<16x32x32x128xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<16x32x15x63xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<16x32x15x63xf32>) -> tensor<16x32x15x63xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x32x32x128xf32>, tensor<3x3xf32>) outs (%init: tensor<16x32x15x63xf32>) -> tensor<16x32x15x63xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x32x15x63xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x32x15x63xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 16, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 15, 1], ["%arg6", 0, 63, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 10310733}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x32x128x256xf32>, tensor<3x3xf32>) outs (%init: tensor<32x32x63x127xf32>) -> tensor<32x32x63x127xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x32x128x256xf32>, tensor<3x3xf32>) outs (%init: tensor<32x32x63x127xf32>) -> tensor<32x32x63x127xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x32x128x256xf32>, %filter: tensor<3x3xf32>, %init: tensor<32x32x63x127xf32>) -> tensor<32x32x63x127xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x32x128x256xf32>, tensor<3x3xf32>) outs (%init: tensor<32x32x63x127xf32>) -> tensor<32x32x63x127xf32>\n  return %ret : tensor<32x32x63x127xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x32x128x256xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<32x32x63x127xf32>) -> tensor<32x32x63x127xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<32x32x128x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<32x32x63x127xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x32x63x127xf32>\n    memref.copy %1, %alloc : memref<32x32x63x127xf32> to memref<32x32x63x127xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 63 {\n          affine.for %arg6 = 0 to 127 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<32x32x128x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x32x63x127xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x32x63x127xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<32x32x63x127xf32>\n    return %2 : tensor<32x32x63x127xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<32x32x63x127xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<32x32x128x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<32x32x128x256xf32>) -> tensor<32x32x128x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<32x32x63x127xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<32x32x63x127xf32>) -> tensor<32x32x63x127xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<32x32x128x256xf32>, tensor<3x3xf32>) outs (%init: tensor<32x32x63x127xf32>) -> tensor<32x32x63x127xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<32x32x63x127xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<32x32x63x127xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 63, 1], ["%arg6", 0, 127, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 177483281}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x256x512x64xf32>, tensor<5x5xf32>) outs (%init: tensor<4x256x254x30xf32>) -> tensor<4x256x254x30xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x256x512x64xf32>, tensor<5x5xf32>) outs (%init: tensor<4x256x254x30xf32>) -> tensor<4x256x254x30xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<4x256x512x64xf32>, %filter: tensor<5x5xf32>, %init: tensor<4x256x254x30xf32>) -> tensor<4x256x254x30xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x256x512x64xf32>, tensor<5x5xf32>) outs (%init: tensor<4x256x254x30xf32>) -> tensor<4x256x254x30xf32>\n  return %ret : tensor<4x256x254x30xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<4x256x512x64xf32>, %arg1: tensor<5x5xf32>, %arg2: tensor<4x256x254x30xf32>) -> tensor<4x256x254x30xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<4x256x512x64xf32>\n    %1 = bufferization.to_memref %arg2 : memref<4x256x254x30xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<4x256x254x30xf32>\n    memref.copy %1, %alloc : memref<4x256x254x30xf32> to memref<4x256x254x30xf32>\n    affine.for %arg3 = 0 to 4 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 254 {\n          affine.for %arg6 = 0 to 30 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<4x256x512x64xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x256x254x30xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x256x254x30xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<4x256x254x30xf32>\n    return %2 : tensor<4x256x254x30xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<4x256x254x30xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<4x256x512x64xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<4x256x512x64xf32>) -> tensor<4x256x512x64xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<5x5xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<5x5xf32>) -> tensor<5x5xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<4x256x254x30xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<4x256x254x30xf32>) -> tensor<4x256x254x30xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x256x512x64xf32>, tensor<5x5xf32>) outs (%init: tensor<4x256x254x30xf32>) -> tensor<4x256x254x30xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<4x256x254x30xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<4x256x254x30xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 4, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 254, 1], ["%arg6", 0, 30, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 652276539}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x128x64x32xf32>, tensor<3x3xf32>) outs (%init: tensor<4x128x62x30xf32>) -> tensor<4x128x62x30xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x128x64x32xf32>, tensor<3x3xf32>) outs (%init: tensor<4x128x62x30xf32>) -> tensor<4x128x62x30xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<4x128x64x32xf32>, %filter: tensor<3x3xf32>, %init: tensor<4x128x62x30xf32>) -> tensor<4x128x62x30xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x128x64x32xf32>, tensor<3x3xf32>) outs (%init: tensor<4x128x62x30xf32>) -> tensor<4x128x62x30xf32>\n  return %ret : tensor<4x128x62x30xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<4x128x64x32xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<4x128x62x30xf32>) -> tensor<4x128x62x30xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<4x128x64x32xf32>\n    %1 = bufferization.to_memref %arg2 : memref<4x128x62x30xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<4x128x62x30xf32>\n    memref.copy %1, %alloc : memref<4x128x62x30xf32> to memref<4x128x62x30xf32>\n    affine.for %arg3 = 0 to 4 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 62 {\n          affine.for %arg6 = 0 to 30 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<4x128x64x32xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x128x62x30xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x128x62x30xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<4x128x62x30xf32>\n    return %2 : tensor<4x128x62x30xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<4x128x62x30xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<4x128x64x32xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<4x128x64x32xf32>) -> tensor<4x128x64x32xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<4x128x62x30xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<4x128x62x30xf32>) -> tensor<4x128x62x30xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x128x64x32xf32>, tensor<3x3xf32>) outs (%init: tensor<4x128x62x30xf32>) -> tensor<4x128x62x30xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<4x128x62x30xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<4x128x62x30xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 4, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 62, 1], ["%arg6", 0, 30, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 20483891}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x32x32x32xf32>, tensor<5x5xf32>) outs (%init: tensor<4x32x28x28xf32>) -> tensor<4x32x28x28xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x32x32x32xf32>, tensor<5x5xf32>) outs (%init: tensor<4x32x28x28xf32>) -> tensor<4x32x28x28xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<4x32x32x32xf32>, %filter: tensor<5x5xf32>, %init: tensor<4x32x28x28xf32>) -> tensor<4x32x28x28xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x32x32x32xf32>, tensor<5x5xf32>) outs (%init: tensor<4x32x28x28xf32>) -> tensor<4x32x28x28xf32>\n  return %ret : tensor<4x32x28x28xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<4x32x32x32xf32>, %arg1: tensor<5x5xf32>, %arg2: tensor<4x32x28x28xf32>) -> tensor<4x32x28x28xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<4x32x32x32xf32>\n    %1 = bufferization.to_memref %arg2 : memref<4x32x28x28xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<4x32x28x28xf32>\n    memref.copy %1, %alloc : memref<4x32x28x28xf32> to memref<4x32x28x28xf32>\n    affine.for %arg3 = 0 to 4 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 28 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<4x32x32x32xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x32x28x28xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x32x28x28xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<4x32x28x28xf32>\n    return %2 : tensor<4x32x28x28xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<4x32x28x28xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<4x32x32x32xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<4x32x32x32xf32>) -> tensor<4x32x32x32xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<5x5xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<5x5xf32>) -> tensor<5x5xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<4x32x28x28xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<4x32x28x28xf32>) -> tensor<4x32x28x28xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x32x32x32xf32>, tensor<5x5xf32>) outs (%init: tensor<4x32x28x28xf32>) -> tensor<4x32x28x28xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<4x32x28x28xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<4x32x28x28xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 4, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 28, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 8315939}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x32x64x64xf32>, tensor<3x3xf32>) outs (%init: tensor<16x32x62x62xf32>) -> tensor<16x32x62x62xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x32x64x64xf32>, tensor<3x3xf32>) outs (%init: tensor<16x32x62x62xf32>) -> tensor<16x32x62x62xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<16x32x64x64xf32>, %filter: tensor<3x3xf32>, %init: tensor<16x32x62x62xf32>) -> tensor<16x32x62x62xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x32x64x64xf32>, tensor<3x3xf32>) outs (%init: tensor<16x32x62x62xf32>) -> tensor<16x32x62x62xf32>\n  return %ret : tensor<16x32x62x62xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<16x32x64x64xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<16x32x62x62xf32>) -> tensor<16x32x62x62xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<16x32x64x64xf32>\n    %1 = bufferization.to_memref %arg2 : memref<16x32x62x62xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x32x62x62xf32>\n    memref.copy %1, %alloc : memref<16x32x62x62xf32> to memref<16x32x62x62xf32>\n    affine.for %arg3 = 0 to 16 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 62 {\n          affine.for %arg6 = 0 to 62 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<16x32x64x64xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x32x62x62xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x32x62x62xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<16x32x62x62xf32>\n    return %2 : tensor<16x32x62x62xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x32x62x62xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<16x32x64x64xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<16x32x64x64xf32>) -> tensor<16x32x64x64xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<16x32x62x62xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<16x32x62x62xf32>) -> tensor<16x32x62x62xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<16x32x64x64xf32>, tensor<3x3xf32>) outs (%init: tensor<16x32x62x62xf32>) -> tensor<16x32x62x62xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x32x62x62xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x32x62x62xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 16, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 62, 1], ["%arg6", 0, 62, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 41990662}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x512x64x32xf32>, tensor<1x1xf32>) outs (%init: tensor<8x512x64x32xf32>) -> tensor<8x512x64x32xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x512x64x32xf32>, tensor<1x1xf32>) outs (%init: tensor<8x512x64x32xf32>) -> tensor<8x512x64x32xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<8x512x64x32xf32>, %filter: tensor<1x1xf32>, %init: tensor<8x512x64x32xf32>) -> tensor<8x512x64x32xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x512x64x32xf32>, tensor<1x1xf32>) outs (%init: tensor<8x512x64x32xf32>) -> tensor<8x512x64x32xf32>\n  return %ret : tensor<8x512x64x32xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<8x512x64x32xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<8x512x64x32xf32>) -> tensor<8x512x64x32xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<8x512x64x32xf32>\n    %1 = bufferization.to_memref %arg2 : memref<8x512x64x32xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<8x512x64x32xf32>\n    memref.copy %1, %alloc : memref<8x512x64x32xf32> to memref<8x512x64x32xf32>\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 64 {\n          affine.for %arg6 = 0 to 32 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<8x512x64x32xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x512x64x32xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x512x64x32xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<8x512x64x32xf32>\n    return %2 : tensor<8x512x64x32xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<8x512x64x32xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<8x512x64x32xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<8x512x64x32xf32>) -> tensor<8x512x64x32xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<8x512x64x32xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<8x512x64x32xf32>) -> tensor<8x512x64x32xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x512x64x32xf32>, tensor<1x1xf32>) outs (%init: tensor<8x512x64x32xf32>) -> tensor<8x512x64x32xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<8x512x64x32xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<8x512x64x32xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 8, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 64, 1], ["%arg6", 0, 32, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 12672217}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x32x128x32xf32>, tensor<7x7xf32>) outs (%init: tensor<32x32x122x26xf32>) -> tensor<32x32x122x26xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x32x128x32xf32>, tensor<7x7xf32>) outs (%init: tensor<32x32x122x26xf32>) -> tensor<32x32x122x26xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x32x128x32xf32>, %filter: tensor<7x7xf32>, %init: tensor<32x32x122x26xf32>) -> tensor<32x32x122x26xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x32x128x32xf32>, tensor<7x7xf32>) outs (%init: tensor<32x32x122x26xf32>) -> tensor<32x32x122x26xf32>\n  return %ret : tensor<32x32x122x26xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x32x128x32xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<32x32x122x26xf32>) -> tensor<32x32x122x26xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<32x32x128x32xf32>\n    %1 = bufferization.to_memref %arg2 : memref<32x32x122x26xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x32x122x26xf32>\n    memref.copy %1, %alloc : memref<32x32x122x26xf32> to memref<32x32x122x26xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 122 {\n          affine.for %arg6 = 0 to 26 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<32x32x128x32xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x32x122x26xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x32x122x26xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<32x32x122x26xf32>\n    return %2 : tensor<32x32x122x26xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<32x32x122x26xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<32x32x128x32xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<32x32x128x32xf32>) -> tensor<32x32x128x32xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<32x32x122x26xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<32x32x122x26xf32>) -> tensor<32x32x122x26xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x32x128x32xf32>, tensor<7x7xf32>) outs (%init: tensor<32x32x122x26xf32>) -> tensor<32x32x122x26xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<32x32x122x26xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<32x32x122x26xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 122, 1], ["%arg6", 0, 26, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 633958146}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x128x64x128xf32>, tensor<5x5xf32>) outs (%init: tensor<8x128x30x62xf32>) -> tensor<8x128x30x62xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x128x64x128xf32>, tensor<5x5xf32>) outs (%init: tensor<8x128x30x62xf32>) -> tensor<8x128x30x62xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<8x128x64x128xf32>, %filter: tensor<5x5xf32>, %init: tensor<8x128x30x62xf32>) -> tensor<8x128x30x62xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x128x64x128xf32>, tensor<5x5xf32>) outs (%init: tensor<8x128x30x62xf32>) -> tensor<8x128x30x62xf32>\n  return %ret : tensor<8x128x30x62xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<8x128x64x128xf32>, %arg1: tensor<5x5xf32>, %arg2: tensor<8x128x30x62xf32>) -> tensor<8x128x30x62xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<8x128x64x128xf32>\n    %1 = bufferization.to_memref %arg2 : memref<8x128x30x62xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<8x128x30x62xf32>\n    memref.copy %1, %alloc : memref<8x128x30x62xf32> to memref<8x128x30x62xf32>\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 30 {\n          affine.for %arg6 = 0 to 62 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<8x128x64x128xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x128x30x62xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x128x30x62xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<8x128x30x62xf32>\n    return %2 : tensor<8x128x30x62xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<8x128x30x62xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<8x128x64x128xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<8x128x64x128xf32>) -> tensor<8x128x64x128xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<5x5xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<5x5xf32>) -> tensor<5x5xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<8x128x30x62xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<8x128x30x62xf32>) -> tensor<8x128x30x62xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x128x64x128xf32>, tensor<5x5xf32>) outs (%init: tensor<8x128x30x62xf32>) -> tensor<8x128x30x62xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<8x128x30x62xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<8x128x30x62xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 8, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 30, 1], ["%arg6", 0, 62, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 158687971}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x16x256x256xf32>, tensor<3x3xf32>) outs (%init: tensor<16x16x127x127xf32>) -> tensor<16x16x127x127xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x16x256x256xf32>, tensor<3x3xf32>) outs (%init: tensor<16x16x127x127xf32>) -> tensor<16x16x127x127xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<16x16x256x256xf32>, %filter: tensor<3x3xf32>, %init: tensor<16x16x127x127xf32>) -> tensor<16x16x127x127xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x16x256x256xf32>, tensor<3x3xf32>) outs (%init: tensor<16x16x127x127xf32>) -> tensor<16x16x127x127xf32>\n  return %ret : tensor<16x16x127x127xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<16x16x256x256xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<16x16x127x127xf32>) -> tensor<16x16x127x127xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<16x16x256x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<16x16x127x127xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x16x127x127xf32>\n    memref.copy %1, %alloc : memref<16x16x127x127xf32> to memref<16x16x127x127xf32>\n    affine.for %arg3 = 0 to 16 {\n      affine.for %arg4 = 0 to 16 {\n        affine.for %arg5 = 0 to 127 {\n          affine.for %arg6 = 0 to 127 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<16x16x256x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x16x127x127xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x16x127x127xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<16x16x127x127xf32>\n    return %2 : tensor<16x16x127x127xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x16x127x127xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<16x16x256x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<16x16x256x256xf32>) -> tensor<16x16x256x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<16x16x127x127xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<16x16x127x127xf32>) -> tensor<16x16x127x127xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x16x256x256xf32>, tensor<3x3xf32>) outs (%init: tensor<16x16x127x127xf32>) -> tensor<16x16x127x127xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x16x127x127xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x16x127x127xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 16, 1], ["%arg4", 0, 16, 1], ["%arg5", 0, 127, 1], ["%arg6", 0, 127, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 89417845}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x32x256x256xf32>, tensor<7x7xf32>) outs (%init: tensor<8x32x125x125xf32>) -> tensor<8x32x125x125xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x32x256x256xf32>, tensor<7x7xf32>) outs (%init: tensor<8x32x125x125xf32>) -> tensor<8x32x125x125xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<8x32x256x256xf32>, %filter: tensor<7x7xf32>, %init: tensor<8x32x125x125xf32>) -> tensor<8x32x125x125xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x32x256x256xf32>, tensor<7x7xf32>) outs (%init: tensor<8x32x125x125xf32>) -> tensor<8x32x125x125xf32>\n  return %ret : tensor<8x32x125x125xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<8x32x256x256xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<8x32x125x125xf32>) -> tensor<8x32x125x125xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<8x32x256x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<8x32x125x125xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<8x32x125x125xf32>\n    memref.copy %1, %alloc : memref<8x32x125x125xf32> to memref<8x32x125x125xf32>\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 125 {\n          affine.for %arg6 = 0 to 125 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<8x32x256x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x32x125x125xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x32x125x125xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<8x32x125x125xf32>\n    return %2 : tensor<8x32x125x125xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<8x32x125x125xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<8x32x256x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<8x32x256x256xf32>) -> tensor<8x32x256x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<8x32x125x125xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<8x32x125x125xf32>) -> tensor<8x32x125x125xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x32x256x256xf32>, tensor<7x7xf32>) outs (%init: tensor<8x32x125x125xf32>) -> tensor<8x32x125x125xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<8x32x125x125xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<8x32x125x125xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 8, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 125, 1], ["%arg6", 0, 125, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 780403505}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x16x512x512xf32>, tensor<5x5xf32>) outs (%init: tensor<8x16x508x508xf32>) -> tensor<8x16x508x508xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x16x512x512xf32>, tensor<5x5xf32>) outs (%init: tensor<8x16x508x508xf32>) -> tensor<8x16x508x508xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<8x16x512x512xf32>, %filter: tensor<5x5xf32>, %init: tensor<8x16x508x508xf32>) -> tensor<8x16x508x508xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x16x512x512xf32>, tensor<5x5xf32>) outs (%init: tensor<8x16x508x508xf32>) -> tensor<8x16x508x508xf32>\n  return %ret : tensor<8x16x508x508xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<8x16x512x512xf32>, %arg1: tensor<5x5xf32>, %arg2: tensor<8x16x508x508xf32>) -> tensor<8x16x508x508xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<8x16x512x512xf32>\n    %1 = bufferization.to_memref %arg2 : memref<8x16x508x508xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<8x16x508x508xf32>\n    memref.copy %1, %alloc : memref<8x16x508x508xf32> to memref<8x16x508x508xf32>\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 16 {\n        affine.for %arg5 = 0 to 508 {\n          affine.for %arg6 = 0 to 508 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<8x16x512x512xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x16x508x508xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x16x508x508xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<8x16x508x508xf32>\n    return %2 : tensor<8x16x508x508xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<8x16x508x508xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<8x16x512x512xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<8x16x512x512xf32>) -> tensor<8x16x512x512xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<5x5xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<5x5xf32>) -> tensor<5x5xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<8x16x508x508xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<8x16x508x508xf32>) -> tensor<8x16x508x508xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x16x512x512xf32>, tensor<5x5xf32>) outs (%init: tensor<8x16x508x508xf32>) -> tensor<8x16x508x508xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<8x16x508x508xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<8x16x508x508xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 8, 1], ["%arg4", 0, 16, 1], ["%arg5", 0, 508, 1], ["%arg6", 0, 508, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 2733490272}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x8x256x256xf32>, tensor<1x1xf32>) outs (%init: tensor<256x8x256x256xf32>) -> tensor<256x8x256x256xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x8x256x256xf32>, tensor<1x1xf32>) outs (%init: tensor<256x8x256x256xf32>) -> tensor<256x8x256x256xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x8x256x256xf32>, %filter: tensor<1x1xf32>, %init: tensor<256x8x256x256xf32>) -> tensor<256x8x256x256xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x8x256x256xf32>, tensor<1x1xf32>) outs (%init: tensor<256x8x256x256xf32>) -> tensor<256x8x256x256xf32>\n  return %ret : tensor<256x8x256x256xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x8x256x256xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<256x8x256x256xf32>) -> tensor<256x8x256x256xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<256x8x256x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<256x8x256x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x8x256x256xf32>\n    memref.copy %1, %alloc : memref<256x8x256x256xf32> to memref<256x8x256x256xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 8 {\n        affine.for %arg5 = 0 to 256 {\n          affine.for %arg6 = 0 to 256 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<256x8x256x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x8x256x256xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x8x256x256xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<256x8x256x256xf32>\n    return %2 : tensor<256x8x256x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x8x256x256xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x8x256x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x8x256x256xf32>) -> tensor<256x8x256x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x8x256x256xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x8x256x256xf32>) -> tensor<256x8x256x256xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<256x8x256x256xf32>, tensor<1x1xf32>) outs (%init: tensor<256x8x256x256xf32>) -> tensor<256x8x256x256xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x8x256x256xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x8x256x256xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 8, 1], ["%arg5", 0, 256, 1], ["%arg6", 0, 256, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 194647607}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x64x32x512xf32>, tensor<3x3xf32>) outs (%init: tensor<4x64x30x510xf32>) -> tensor<4x64x30x510xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x64x32x512xf32>, tensor<3x3xf32>) outs (%init: tensor<4x64x30x510xf32>) -> tensor<4x64x30x510xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<4x64x32x512xf32>, %filter: tensor<3x3xf32>, %init: tensor<4x64x30x510xf32>) -> tensor<4x64x30x510xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x64x32x512xf32>, tensor<3x3xf32>) outs (%init: tensor<4x64x30x510xf32>) -> tensor<4x64x30x510xf32>\n  return %ret : tensor<4x64x30x510xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<4x64x32x512xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<4x64x30x510xf32>) -> tensor<4x64x30x510xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<4x64x32x512xf32>\n    %1 = bufferization.to_memref %arg2 : memref<4x64x30x510xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<4x64x30x510xf32>\n    memref.copy %1, %alloc : memref<4x64x30x510xf32> to memref<4x64x30x510xf32>\n    affine.for %arg3 = 0 to 4 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 30 {\n          affine.for %arg6 = 0 to 510 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<4x64x32x512xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x64x30x510xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x64x30x510xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<4x64x30x510xf32>\n    return %2 : tensor<4x64x30x510xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<4x64x30x510xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<4x64x32x512xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<4x64x32x512xf32>) -> tensor<4x64x32x512xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<4x64x30x510xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<4x64x30x510xf32>) -> tensor<4x64x30x510xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<4x64x32x512xf32>, tensor<3x3xf32>) outs (%init: tensor<4x64x30x510xf32>) -> tensor<4x64x30x510xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<4x64x30x510xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<4x64x30x510xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 4, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 30, 1], ["%arg6", 0, 510, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 83090550}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x8x256x128xf32>, tensor<3x3xf32>) outs (%init: tensor<8x8x127x63xf32>) -> tensor<8x8x127x63xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x8x256x128xf32>, tensor<3x3xf32>) outs (%init: tensor<8x8x127x63xf32>) -> tensor<8x8x127x63xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<8x8x256x128xf32>, %filter: tensor<3x3xf32>, %init: tensor<8x8x127x63xf32>) -> tensor<8x8x127x63xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x8x256x128xf32>, tensor<3x3xf32>) outs (%init: tensor<8x8x127x63xf32>) -> tensor<8x8x127x63xf32>\n  return %ret : tensor<8x8x127x63xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<8x8x256x128xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<8x8x127x63xf32>) -> tensor<8x8x127x63xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<8x8x256x128xf32>\n    %1 = bufferization.to_memref %arg2 : memref<8x8x127x63xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<8x8x127x63xf32>\n    memref.copy %1, %alloc : memref<8x8x127x63xf32> to memref<8x8x127x63xf32>\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 8 {\n        affine.for %arg5 = 0 to 127 {\n          affine.for %arg6 = 0 to 63 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<8x8x256x128xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x8x127x63xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x8x127x63xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<8x8x127x63xf32>\n    return %2 : tensor<8x8x127x63xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<8x8x127x63xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<8x8x256x128xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<8x8x256x128xf32>) -> tensor<8x8x256x128xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<8x8x127x63xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<8x8x127x63xf32>) -> tensor<8x8x127x63xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x8x256x128xf32>, tensor<3x3xf32>) outs (%init: tensor<8x8x127x63xf32>) -> tensor<8x8x127x63xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<8x8x127x63xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<8x8x127x63xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 8, 1], ["%arg4", 0, 8, 1], ["%arg5", 0, 127, 1], ["%arg6", 0, 63, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 10903055}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x32x64x256xf32>, tensor<3x3xf32>) outs (%init: tensor<256x32x31x127xf32>) -> tensor<256x32x31x127xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x32x64x256xf32>, tensor<3x3xf32>) outs (%init: tensor<256x32x31x127xf32>) -> tensor<256x32x31x127xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x32x64x256xf32>, %filter: tensor<3x3xf32>, %init: tensor<256x32x31x127xf32>) -> tensor<256x32x31x127xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x32x64x256xf32>, tensor<3x3xf32>) outs (%init: tensor<256x32x31x127xf32>) -> tensor<256x32x31x127xf32>\n  return %ret : tensor<256x32x31x127xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x32x64x256xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<256x32x31x127xf32>) -> tensor<256x32x31x127xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<256x32x64x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<256x32x31x127xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x32x31x127xf32>\n    memref.copy %1, %alloc : memref<256x32x31x127xf32> to memref<256x32x31x127xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 31 {\n          affine.for %arg6 = 0 to 127 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<256x32x64x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x32x31x127xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x32x31x127xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<256x32x31x127xf32>\n    return %2 : tensor<256x32x31x127xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x32x31x127xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x32x64x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x32x64x256xf32>) -> tensor<256x32x64x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x32x31x127xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x32x31x127xf32>) -> tensor<256x32x31x127xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x32x64x256xf32>, tensor<3x3xf32>) outs (%init: tensor<256x32x31x127xf32>) -> tensor<256x32x31x127xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x32x31x127xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x32x31x127xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 31, 1], ["%arg6", 0, 127, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 699328212}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x256x64x512xf32>, tensor<7x7xf32>) outs (%init: tensor<4x256x29x253xf32>) -> tensor<4x256x29x253xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x256x64x512xf32>, tensor<7x7xf32>) outs (%init: tensor<4x256x29x253xf32>) -> tensor<4x256x29x253xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<4x256x64x512xf32>, %filter: tensor<7x7xf32>, %init: tensor<4x256x29x253xf32>) -> tensor<4x256x29x253xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x256x64x512xf32>, tensor<7x7xf32>) outs (%init: tensor<4x256x29x253xf32>) -> tensor<4x256x29x253xf32>\n  return %ret : tensor<4x256x29x253xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<4x256x64x512xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<4x256x29x253xf32>) -> tensor<4x256x29x253xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<4x256x64x512xf32>\n    %1 = bufferization.to_memref %arg2 : memref<4x256x29x253xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<4x256x29x253xf32>\n    memref.copy %1, %alloc : memref<4x256x29x253xf32> to memref<4x256x29x253xf32>\n    affine.for %arg3 = 0 to 4 {\n      affine.for %arg4 = 0 to 256 {\n        affine.for %arg5 = 0 to 29 {\n          affine.for %arg6 = 0 to 253 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<4x256x64x512xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x256x29x253xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x256x29x253xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<4x256x29x253xf32>\n    return %2 : tensor<4x256x29x253xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<4x256x29x253xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<4x256x64x512xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<4x256x64x512xf32>) -> tensor<4x256x64x512xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<4x256x29x253xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<4x256x29x253xf32>) -> tensor<4x256x29x253xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x256x64x512xf32>, tensor<7x7xf32>) outs (%init: tensor<4x256x29x253xf32>) -> tensor<4x256x29x253xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<4x256x29x253xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<4x256x29x253xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 4, 1], ["%arg4", 0, 256, 1], ["%arg5", 0, 29, 1], ["%arg6", 0, 253, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 1466527299}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x512x256x128xf32>, tensor<7x7xf32>) outs (%init: tensor<4x512x125x61xf32>) -> tensor<4x512x125x61xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x512x256x128xf32>, tensor<7x7xf32>) outs (%init: tensor<4x512x125x61xf32>) -> tensor<4x512x125x61xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<4x512x256x128xf32>, %filter: tensor<7x7xf32>, %init: tensor<4x512x125x61xf32>) -> tensor<4x512x125x61xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x512x256x128xf32>, tensor<7x7xf32>) outs (%init: tensor<4x512x125x61xf32>) -> tensor<4x512x125x61xf32>\n  return %ret : tensor<4x512x125x61xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<4x512x256x128xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<4x512x125x61xf32>) -> tensor<4x512x125x61xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<4x512x256x128xf32>\n    %1 = bufferization.to_memref %arg2 : memref<4x512x125x61xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<4x512x125x61xf32>\n    memref.copy %1, %alloc : memref<4x512x125x61xf32> to memref<4x512x125x61xf32>\n    affine.for %arg3 = 0 to 4 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 125 {\n          affine.for %arg6 = 0 to 61 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<4x512x256x128xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x512x125x61xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<4x512x125x61xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<4x512x125x61xf32>\n    return %2 : tensor<4x512x125x61xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<4x512x125x61xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<4x512x256x128xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<4x512x256x128xf32>) -> tensor<4x512x256x128xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<4x512x125x61xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<4x512x125x61xf32>) -> tensor<4x512x125x61xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<4x512x256x128xf32>, tensor<7x7xf32>) outs (%init: tensor<4x512x125x61xf32>) -> tensor<4x512x125x61xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<4x512x125x61xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<4x512x125x61xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 4, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 125, 1], ["%arg6", 0, 61, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 3046511886}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x16x32x512xf32>, tensor<3x3xf32>) outs (%init: tensor<16x16x15x255xf32>) -> tensor<16x16x15x255xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x16x32x512xf32>, tensor<3x3xf32>) outs (%init: tensor<16x16x15x255xf32>) -> tensor<16x16x15x255xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<16x16x32x512xf32>, %filter: tensor<3x3xf32>, %init: tensor<16x16x15x255xf32>) -> tensor<16x16x15x255xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x16x32x512xf32>, tensor<3x3xf32>) outs (%init: tensor<16x16x15x255xf32>) -> tensor<16x16x15x255xf32>\n  return %ret : tensor<16x16x15x255xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<16x16x32x512xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<16x16x15x255xf32>) -> tensor<16x16x15x255xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<16x16x32x512xf32>\n    %1 = bufferization.to_memref %arg2 : memref<16x16x15x255xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x16x15x255xf32>\n    memref.copy %1, %alloc : memref<16x16x15x255xf32> to memref<16x16x15x255xf32>\n    affine.for %arg3 = 0 to 16 {\n      affine.for %arg4 = 0 to 16 {\n        affine.for %arg5 = 0 to 15 {\n          affine.for %arg6 = 0 to 255 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<16x16x32x512xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x16x15x255xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x16x15x255xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<16x16x15x255xf32>\n    return %2 : tensor<16x16x15x255xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x16x15x255xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<16x16x32x512xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<16x16x32x512xf32>) -> tensor<16x16x32x512xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<16x16x15x255xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<16x16x15x255xf32>) -> tensor<16x16x15x255xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x16x32x512xf32>, tensor<3x3xf32>) outs (%init: tensor<16x16x15x255xf32>) -> tensor<16x16x15x255xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x16x15x255xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x16x15x255xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 16, 1], ["%arg4", 0, 16, 1], ["%arg5", 0, 15, 1], ["%arg6", 0, 255, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 20728587}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x32x256x256xf32>, tensor<7x7xf32>) outs (%init: tensor<64x32x125x125xf32>) -> tensor<64x32x125x125xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x32x256x256xf32>, tensor<7x7xf32>) outs (%init: tensor<64x32x125x125xf32>) -> tensor<64x32x125x125xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x32x256x256xf32>, %filter: tensor<7x7xf32>, %init: tensor<64x32x125x125xf32>) -> tensor<64x32x125x125xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x32x256x256xf32>, tensor<7x7xf32>) outs (%init: tensor<64x32x125x125xf32>) -> tensor<64x32x125x125xf32>\n  return %ret : tensor<64x32x125x125xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x32x256x256xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<64x32x125x125xf32>) -> tensor<64x32x125x125xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<64x32x256x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<64x32x125x125xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x32x125x125xf32>\n    memref.copy %1, %alloc : memref<64x32x125x125xf32> to memref<64x32x125x125xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 32 {\n        affine.for %arg5 = 0 to 125 {\n          affine.for %arg6 = 0 to 125 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<64x32x256x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x32x125x125xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x32x125x125xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<64x32x125x125xf32>\n    return %2 : tensor<64x32x125x125xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<64x32x125x125xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<64x32x256x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<64x32x256x256xf32>) -> tensor<64x32x256x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<64x32x125x125xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<64x32x125x125xf32>) -> tensor<64x32x125x125xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x32x256x256xf32>, tensor<7x7xf32>) outs (%init: tensor<64x32x125x125xf32>) -> tensor<64x32x125x125xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<64x32x125x125xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<64x32x125x125xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 32, 1], ["%arg5", 0, 125, 1], ["%arg6", 0, 125, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 6249934880}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x512x256x128xf32>, tensor<1x1xf32>) outs (%init: tensor<64x512x256x128xf32>) -> tensor<64x512x256x128xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x512x256x128xf32>, tensor<1x1xf32>) outs (%init: tensor<64x512x256x128xf32>) -> tensor<64x512x256x128xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x512x256x128xf32>, %filter: tensor<1x1xf32>, %init: tensor<64x512x256x128xf32>) -> tensor<64x512x256x128xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x512x256x128xf32>, tensor<1x1xf32>) outs (%init: tensor<64x512x256x128xf32>) -> tensor<64x512x256x128xf32>\n  return %ret : tensor<64x512x256x128xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x512x256x128xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<64x512x256x128xf32>) -> tensor<64x512x256x128xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<64x512x256x128xf32>\n    %1 = bufferization.to_memref %arg2 : memref<64x512x256x128xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x512x256x128xf32>\n    memref.copy %1, %alloc : memref<64x512x256x128xf32> to memref<64x512x256x128xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 256 {\n          affine.for %arg6 = 0 to 128 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<64x512x256x128xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x512x256x128xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x512x256x128xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<64x512x256x128xf32>\n    return %2 : tensor<64x512x256x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<64x512x256x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<64x512x256x128xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<64x512x256x128xf32>) -> tensor<64x512x256x128xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<64x512x256x128xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<64x512x256x128xf32>) -> tensor<64x512x256x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<64x512x256x128xf32>, tensor<1x1xf32>) outs (%init: tensor<64x512x256x128xf32>) -> tensor<64x512x256x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<64x512x256x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<64x512x256x128xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 256, 1], ["%arg6", 0, 128, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 1609856266}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x16x256x64xf32>, tensor<5x5xf32>) outs (%init: tensor<64x16x126x30xf32>) -> tensor<64x16x126x30xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x16x256x64xf32>, tensor<5x5xf32>) outs (%init: tensor<64x16x126x30xf32>) -> tensor<64x16x126x30xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x16x256x64xf32>, %filter: tensor<5x5xf32>, %init: tensor<64x16x126x30xf32>) -> tensor<64x16x126x30xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x16x256x64xf32>, tensor<5x5xf32>) outs (%init: tensor<64x16x126x30xf32>) -> tensor<64x16x126x30xf32>\n  return %ret : tensor<64x16x126x30xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x16x256x64xf32>, %arg1: tensor<5x5xf32>, %arg2: tensor<64x16x126x30xf32>) -> tensor<64x16x126x30xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<64x16x256x64xf32>\n    %1 = bufferization.to_memref %arg2 : memref<64x16x126x30xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x16x126x30xf32>\n    memref.copy %1, %alloc : memref<64x16x126x30xf32> to memref<64x16x126x30xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 16 {\n        affine.for %arg5 = 0 to 126 {\n          affine.for %arg6 = 0 to 30 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<64x16x256x64xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x16x126x30xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x16x126x30xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<64x16x126x30xf32>\n    return %2 : tensor<64x16x126x30xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<64x16x126x30xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<64x16x256x64xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<64x16x256x64xf32>) -> tensor<64x16x256x64xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<5x5xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<5x5xf32>) -> tensor<5x5xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<64x16x126x30xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<64x16x126x30xf32>) -> tensor<64x16x126x30xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x16x256x64xf32>, tensor<5x5xf32>) outs (%init: tensor<64x16x126x30xf32>) -> tensor<64x16x126x30xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<64x16x126x30xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<64x16x126x30xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 16, 1], ["%arg5", 0, 126, 1], ["%arg6", 0, 30, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 323807956}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x128x256x128xf32>, tensor<5x5xf32>) outs (%init: tensor<16x128x126x62xf32>) -> tensor<16x128x126x62xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x128x256x128xf32>, tensor<5x5xf32>) outs (%init: tensor<16x128x126x62xf32>) -> tensor<16x128x126x62xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<16x128x256x128xf32>, %filter: tensor<5x5xf32>, %init: tensor<16x128x126x62xf32>) -> tensor<16x128x126x62xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x128x256x128xf32>, tensor<5x5xf32>) outs (%init: tensor<16x128x126x62xf32>) -> tensor<16x128x126x62xf32>\n  return %ret : tensor<16x128x126x62xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<16x128x256x128xf32>, %arg1: tensor<5x5xf32>, %arg2: tensor<16x128x126x62xf32>) -> tensor<16x128x126x62xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<16x128x256x128xf32>\n    %1 = bufferization.to_memref %arg2 : memref<16x128x126x62xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x128x126x62xf32>\n    memref.copy %1, %alloc : memref<16x128x126x62xf32> to memref<16x128x126x62xf32>\n    affine.for %arg3 = 0 to 16 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 126 {\n          affine.for %arg6 = 0 to 62 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<16x128x256x128xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x128x126x62xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x128x126x62xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<16x128x126x62xf32>\n    return %2 : tensor<16x128x126x62xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x128x126x62xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<16x128x256x128xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<16x128x256x128xf32>) -> tensor<16x128x256x128xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<5x5xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<5x5xf32>) -> tensor<5x5xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<16x128x126x62xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<16x128x126x62xf32>) -> tensor<16x128x126x62xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x128x256x128xf32>, tensor<5x5xf32>) outs (%init: tensor<16x128x126x62xf32>) -> tensor<16x128x126x62xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x128x126x62xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x128x126x62xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 16, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 126, 1], ["%arg6", 0, 62, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 1335204133}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x512x256x256xf32>, tensor<1x1xf32>) outs (%init: tensor<32x512x256x256xf32>) -> tensor<32x512x256x256xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x512x256x256xf32>, tensor<1x1xf32>) outs (%init: tensor<32x512x256x256xf32>) -> tensor<32x512x256x256xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x512x256x256xf32>, %filter: tensor<1x1xf32>, %init: tensor<32x512x256x256xf32>) -> tensor<32x512x256x256xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x512x256x256xf32>, tensor<1x1xf32>) outs (%init: tensor<32x512x256x256xf32>) -> tensor<32x512x256x256xf32>\n  return %ret : tensor<32x512x256x256xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x512x256x256xf32>, %arg1: tensor<1x1xf32>, %arg2: tensor<32x512x256x256xf32>) -> tensor<32x512x256x256xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<32x512x256x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<32x512x256x256xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x512x256x256xf32>\n    memref.copy %1, %alloc : memref<32x512x256x256xf32> to memref<32x512x256x256xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 256 {\n          affine.for %arg6 = 0 to 256 {\n            affine.for %arg7 = 0 to 1 {\n              affine.for %arg8 = 0 to 1 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<32x512x256x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x512x256x256xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x512x256x256xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<32x512x256x256xf32>\n    return %2 : tensor<32x512x256x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<32x512x256x256xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<32x512x256x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<32x512x256x256xf32>) -> tensor<32x512x256x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<1x1xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<1x1xf32>) -> tensor<1x1xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<32x512x256x256xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<32x512x256x256xf32>) -> tensor<32x512x256x256xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x512x256x256xf32>, tensor<1x1xf32>) outs (%init: tensor<32x512x256x256xf32>) -> tensor<32x512x256x256xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<32x512x256x256xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<32x512x256x256xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 256, 1], ["%arg6", 0, 256, 1], ["%arg7", 0, 1, 1], ["%arg8", 0, 1, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 1564673402}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x128x256x32xf32>, tensor<5x5xf32>) outs (%init: tensor<256x128x126x14xf32>) -> tensor<256x128x126x14xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x128x256x32xf32>, tensor<5x5xf32>) outs (%init: tensor<256x128x126x14xf32>) -> tensor<256x128x126x14xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x128x256x32xf32>, %filter: tensor<5x5xf32>, %init: tensor<256x128x126x14xf32>) -> tensor<256x128x126x14xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x128x256x32xf32>, tensor<5x5xf32>) outs (%init: tensor<256x128x126x14xf32>) -> tensor<256x128x126x14xf32>\n  return %ret : tensor<256x128x126x14xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x128x256x32xf32>, %arg1: tensor<5x5xf32>, %arg2: tensor<256x128x126x14xf32>) -> tensor<256x128x126x14xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<256x128x256x32xf32>\n    %1 = bufferization.to_memref %arg2 : memref<256x128x126x14xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x128x126x14xf32>\n    memref.copy %1, %alloc : memref<256x128x126x14xf32> to memref<256x128x126x14xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 126 {\n          affine.for %arg6 = 0 to 14 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<256x128x256x32xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x128x126x14xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x128x126x14xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<256x128x126x14xf32>\n    return %2 : tensor<256x128x126x14xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x128x126x14xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x128x256x32xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x128x256x32xf32>) -> tensor<256x128x256x32xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<5x5xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<5x5xf32>) -> tensor<5x5xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x128x126x14xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x128x126x14xf32>) -> tensor<256x128x126x14xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x128x256x32xf32>, tensor<5x5xf32>) outs (%init: tensor<256x128x126x14xf32>) -> tensor<256x128x126x14xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x128x126x14xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x128x126x14xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 126, 1], ["%arg6", 0, 14, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 4869896023}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x8x32x512xf32>, tensor<7x7xf32>) outs (%init: tensor<8x8x26x506xf32>) -> tensor<8x8x26x506xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x8x32x512xf32>, tensor<7x7xf32>) outs (%init: tensor<8x8x26x506xf32>) -> tensor<8x8x26x506xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<8x8x32x512xf32>, %filter: tensor<7x7xf32>, %init: tensor<8x8x26x506xf32>) -> tensor<8x8x26x506xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x8x32x512xf32>, tensor<7x7xf32>) outs (%init: tensor<8x8x26x506xf32>) -> tensor<8x8x26x506xf32>\n  return %ret : tensor<8x8x26x506xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<8x8x32x512xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<8x8x26x506xf32>) -> tensor<8x8x26x506xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<8x8x32x512xf32>\n    %1 = bufferization.to_memref %arg2 : memref<8x8x26x506xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<8x8x26x506xf32>\n    memref.copy %1, %alloc : memref<8x8x26x506xf32> to memref<8x8x26x506xf32>\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 8 {\n        affine.for %arg5 = 0 to 26 {\n          affine.for %arg6 = 0 to 506 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<8x8x32x512xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x8x26x506xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x8x26x506xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<8x8x26x506xf32>\n    return %2 : tensor<8x8x26x506xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<8x8x26x506xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<8x8x32x512xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<8x8x32x512xf32>) -> tensor<8x8x32x512xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<8x8x26x506xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<8x8x26x506xf32>) -> tensor<8x8x26x506xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x8x32x512xf32>, tensor<7x7xf32>) outs (%init: tensor<8x8x26x506xf32>) -> tensor<8x8x26x506xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<8x8x26x506xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<8x8x26x506xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 8, 1], ["%arg4", 0, 8, 1], ["%arg5", 0, 26, 1], ["%arg6", 0, 506, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 164157447}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x512x128x256xf32>, tensor<5x5xf32>) outs (%init: tensor<16x512x62x126xf32>) -> tensor<16x512x62x126xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x512x128x256xf32>, tensor<5x5xf32>) outs (%init: tensor<16x512x62x126xf32>) -> tensor<16x512x62x126xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<16x512x128x256xf32>, %filter: tensor<5x5xf32>, %init: tensor<16x512x62x126xf32>) -> tensor<16x512x62x126xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x512x128x256xf32>, tensor<5x5xf32>) outs (%init: tensor<16x512x62x126xf32>) -> tensor<16x512x62x126xf32>\n  return %ret : tensor<16x512x62x126xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<16x512x128x256xf32>, %arg1: tensor<5x5xf32>, %arg2: tensor<16x512x62x126xf32>) -> tensor<16x512x62x126xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<16x512x128x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<16x512x62x126xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<16x512x62x126xf32>\n    memref.copy %1, %alloc : memref<16x512x62x126xf32> to memref<16x512x62x126xf32>\n    affine.for %arg3 = 0 to 16 {\n      affine.for %arg4 = 0 to 512 {\n        affine.for %arg5 = 0 to 62 {\n          affine.for %arg6 = 0 to 126 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<16x512x128x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x512x62x126xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<16x512x62x126xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<16x512x62x126xf32>\n    return %2 : tensor<16x512x62x126xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<16x512x62x126xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<16x512x128x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<16x512x128x256xf32>) -> tensor<16x512x128x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<5x5xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<5x5xf32>) -> tensor<5x5xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<16x512x62x126xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<16x512x62x126xf32>) -> tensor<16x512x62x126xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<16x512x128x256xf32>, tensor<5x5xf32>) outs (%init: tensor<16x512x62x126xf32>) -> tensor<16x512x62x126xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<16x512x62x126xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<16x512x62x126xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 16, 1], ["%arg4", 0, 512, 1], ["%arg5", 0, 62, 1], ["%arg6", 0, 126, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 5346951462}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x8x256x64xf32>, tensor<7x7xf32>) outs (%init: tensor<64x8x125x29xf32>) -> tensor<64x8x125x29xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x8x256x64xf32>, tensor<7x7xf32>) outs (%init: tensor<64x8x125x29xf32>) -> tensor<64x8x125x29xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<64x8x256x64xf32>, %filter: tensor<7x7xf32>, %init: tensor<64x8x125x29xf32>) -> tensor<64x8x125x29xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x8x256x64xf32>, tensor<7x7xf32>) outs (%init: tensor<64x8x125x29xf32>) -> tensor<64x8x125x29xf32>\n  return %ret : tensor<64x8x125x29xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<64x8x256x64xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<64x8x125x29xf32>) -> tensor<64x8x125x29xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<64x8x256x64xf32>\n    %1 = bufferization.to_memref %arg2 : memref<64x8x125x29xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<64x8x125x29xf32>\n    memref.copy %1, %alloc : memref<64x8x125x29xf32> to memref<64x8x125x29xf32>\n    affine.for %arg3 = 0 to 64 {\n      affine.for %arg4 = 0 to 8 {\n        affine.for %arg5 = 0 to 125 {\n          affine.for %arg6 = 0 to 29 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<64x8x256x64xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x8x125x29xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<64x8x125x29xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<64x8x125x29xf32>\n    return %2 : tensor<64x8x125x29xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<64x8x125x29xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<64x8x256x64xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<64x8x256x64xf32>) -> tensor<64x8x256x64xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<64x8x125x29xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<64x8x125x29xf32>) -> tensor<64x8x125x29xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<64x8x256x64xf32>, tensor<7x7xf32>) outs (%init: tensor<64x8x125x29xf32>) -> tensor<64x8x125x29xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<64x8x125x29xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<64x8x125x29xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 64, 1], ["%arg4", 0, 8, 1], ["%arg5", 0, 125, 1], ["%arg6", 0, 29, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 369729554}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x8x32x64xf32>, tensor<5x5xf32>) outs (%init: tensor<8x8x28x60xf32>) -> tensor<8x8x28x60xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x8x32x64xf32>, tensor<5x5xf32>) outs (%init: tensor<8x8x28x60xf32>) -> tensor<8x8x28x60xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<8x8x32x64xf32>, %filter: tensor<5x5xf32>, %init: tensor<8x8x28x60xf32>) -> tensor<8x8x28x60xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x8x32x64xf32>, tensor<5x5xf32>) outs (%init: tensor<8x8x28x60xf32>) -> tensor<8x8x28x60xf32>\n  return %ret : tensor<8x8x28x60xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<8x8x32x64xf32>, %arg1: tensor<5x5xf32>, %arg2: tensor<8x8x28x60xf32>) -> tensor<8x8x28x60xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<8x8x32x64xf32>\n    %1 = bufferization.to_memref %arg2 : memref<8x8x28x60xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<8x8x28x60xf32>\n    memref.copy %1, %alloc : memref<8x8x28x60xf32> to memref<8x8x28x60xf32>\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 8 {\n        affine.for %arg5 = 0 to 28 {\n          affine.for %arg6 = 0 to 60 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<8x8x32x64xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x8x28x60xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x8x28x60xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<8x8x28x60xf32>\n    return %2 : tensor<8x8x28x60xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<8x8x28x60xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<8x8x32x64xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<8x8x32x64xf32>) -> tensor<8x8x32x64xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<5x5xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<5x5xf32>) -> tensor<5x5xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<8x8x28x60xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<8x8x28x60xf32>) -> tensor<8x8x28x60xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x8x32x64xf32>, tensor<5x5xf32>) outs (%init: tensor<8x8x28x60xf32>) -> tensor<8x8x28x60xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<8x8x28x60xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<8x8x28x60xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 8, 1], ["%arg4", 0, 8, 1], ["%arg5", 0, 28, 1], ["%arg6", 0, 60, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 8886717}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x1024x32x256xf32>, tensor<3x3xf32>) outs (%init: tensor<256x1024x15x127xf32>) -> tensor<256x1024x15x127xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x1024x32x256xf32>, tensor<3x3xf32>) outs (%init: tensor<256x1024x15x127xf32>) -> tensor<256x1024x15x127xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<256x1024x32x256xf32>, %filter: tensor<3x3xf32>, %init: tensor<256x1024x15x127xf32>) -> tensor<256x1024x15x127xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x1024x32x256xf32>, tensor<3x3xf32>) outs (%init: tensor<256x1024x15x127xf32>) -> tensor<256x1024x15x127xf32>\n  return %ret : tensor<256x1024x15x127xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<256x1024x32x256xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<256x1024x15x127xf32>) -> tensor<256x1024x15x127xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<256x1024x32x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<256x1024x15x127xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<256x1024x15x127xf32>\n    memref.copy %1, %alloc : memref<256x1024x15x127xf32> to memref<256x1024x15x127xf32>\n    affine.for %arg3 = 0 to 256 {\n      affine.for %arg4 = 0 to 1024 {\n        affine.for %arg5 = 0 to 15 {\n          affine.for %arg6 = 0 to 127 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<256x1024x32x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1024x15x127xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<256x1024x15x127xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<256x1024x15x127xf32>\n    return %2 : tensor<256x1024x15x127xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<256x1024x15x127xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<256x1024x32x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<256x1024x32x256xf32>) -> tensor<256x1024x32x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<256x1024x15x127xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<256x1024x15x127xf32>) -> tensor<256x1024x15x127xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<256x1024x32x256xf32>, tensor<3x3xf32>) outs (%init: tensor<256x1024x15x127xf32>) -> tensor<256x1024x15x127xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<256x1024x15x127xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<256x1024x15x127xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 256, 1], ["%arg4", 0, 1024, 1], ["%arg5", 0, 15, 1], ["%arg6", 0, 127, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 10860194111}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x8x128x128xf32>, tensor<3x3xf32>) outs (%init: tensor<8x8x63x63xf32>) -> tensor<8x8x63x63xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x8x128x128xf32>, tensor<3x3xf32>) outs (%init: tensor<8x8x63x63xf32>) -> tensor<8x8x63x63xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<8x8x128x128xf32>, %filter: tensor<3x3xf32>, %init: tensor<8x8x63x63xf32>) -> tensor<8x8x63x63xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x8x128x128xf32>, tensor<3x3xf32>) outs (%init: tensor<8x8x63x63xf32>) -> tensor<8x8x63x63xf32>\n  return %ret : tensor<8x8x63x63xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 * 2 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<8x8x128x128xf32>, %arg1: tensor<3x3xf32>, %arg2: tensor<8x8x63x63xf32>) -> tensor<8x8x63x63xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<8x8x128x128xf32>\n    %1 = bufferization.to_memref %arg2 : memref<8x8x63x63xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<8x8x63x63xf32>\n    memref.copy %1, %alloc : memref<8x8x63x63xf32> to memref<8x8x63x63xf32>\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 8 {\n        affine.for %arg5 = 0 to 63 {\n          affine.for %arg6 = 0 to 63 {\n            affine.for %arg7 = 0 to 3 {\n              affine.for %arg8 = 0 to 3 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<8x8x128x128xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x8x63x63xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x8x63x63xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<8x8x63x63xf32>\n    return %2 : tensor<8x8x63x63xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<8x8x63x63xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<8x8x128x128xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<8x8x128x128xf32>) -> tensor<8x8x128x128xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<3x3xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<3x3xf32>) -> tensor<3x3xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<8x8x63x63xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<8x8x63x63xf32>) -> tensor<8x8x63x63xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<2> : tensor<2xi64>} ins (%input, %filter: tensor<8x8x128x128xf32>, tensor<3x3xf32>) outs (%init: tensor<8x8x63x63xf32>) -> tensor<8x8x63x63xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<8x8x63x63xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<8x8x63x63xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 8, 1], ["%arg4", 0, 8, 1], ["%arg5", 0, 63, 1], ["%arg6", 0, 63, 1], ["%arg7", 0, 3, 1], ["%arg8", 0, 3, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 * 2 + %arg7", "%arg6 * 2 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 5405171}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x128x128x256xf32>, tensor<5x5xf32>) outs (%init: tensor<32x128x124x252xf32>) -> tensor<32x128x124x252xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x128x128x256xf32>, tensor<5x5xf32>) outs (%init: tensor<32x128x124x252xf32>) -> tensor<32x128x124x252xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<32x128x128x256xf32>, %filter: tensor<5x5xf32>, %init: tensor<32x128x124x252xf32>) -> tensor<32x128x124x252xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x128x128x256xf32>, tensor<5x5xf32>) outs (%init: tensor<32x128x124x252xf32>) -> tensor<32x128x124x252xf32>\n  return %ret : tensor<32x128x124x252xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<32x128x128x256xf32>, %arg1: tensor<5x5xf32>, %arg2: tensor<32x128x124x252xf32>) -> tensor<32x128x124x252xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<32x128x128x256xf32>\n    %1 = bufferization.to_memref %arg2 : memref<32x128x124x252xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<32x128x124x252xf32>\n    memref.copy %1, %alloc : memref<32x128x124x252xf32> to memref<32x128x124x252xf32>\n    affine.for %arg3 = 0 to 32 {\n      affine.for %arg4 = 0 to 128 {\n        affine.for %arg5 = 0 to 124 {\n          affine.for %arg6 = 0 to 252 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<32x128x128x256xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x128x124x252xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<32x128x124x252xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<32x128x124x252xf32>\n    return %2 : tensor<32x128x124x252xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<32x128x124x252xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<32x128x128x256xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<32x128x128x256xf32>) -> tensor<32x128x128x256xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<5x5xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<5x5xf32>) -> tensor<5x5xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<32x128x124x252xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<32x128x124x252xf32>) -> tensor<32x128x124x252xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<32x128x128x256xf32>, tensor<5x5xf32>) outs (%init: tensor<32x128x124x252xf32>) -> tensor<32x128x124x252xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<32x128x124x252xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<32x128x124x252xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 32, 1], ["%arg4", 0, 128, 1], ["%arg5", 0, 124, 1], ["%arg6", 0, 252, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 10589651134}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x16x512x128xf32>, tensor<5x5xf32>) outs (%init: tensor<8x16x508x124xf32>) -> tensor<8x16x508x124xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x16x512x128xf32>, tensor<5x5xf32>) outs (%init: tensor<8x16x508x124xf32>) -> tensor<8x16x508x124xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<8x16x512x128xf32>, %filter: tensor<5x5xf32>, %init: tensor<8x16x508x124xf32>) -> tensor<8x16x508x124xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x16x512x128xf32>, tensor<5x5xf32>) outs (%init: tensor<8x16x508x124xf32>) -> tensor<8x16x508x124xf32>\n  return %ret : tensor<8x16x508x124xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<8x16x512x128xf32>, %arg1: tensor<5x5xf32>, %arg2: tensor<8x16x508x124xf32>) -> tensor<8x16x508x124xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<8x16x512x128xf32>\n    %1 = bufferization.to_memref %arg2 : memref<8x16x508x124xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<8x16x508x124xf32>\n    memref.copy %1, %alloc : memref<8x16x508x124xf32> to memref<8x16x508x124xf32>\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 16 {\n        affine.for %arg5 = 0 to 508 {\n          affine.for %arg6 = 0 to 124 {\n            affine.for %arg7 = 0 to 5 {\n              affine.for %arg8 = 0 to 5 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<8x16x512x128xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x16x508x124xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x16x508x124xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<8x16x508x124xf32>\n    return %2 : tensor<8x16x508x124xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<8x16x508x124xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<8x16x512x128xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<8x16x512x128xf32>) -> tensor<8x16x512x128xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<5x5xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<5x5xf32>) -> tensor<5x5xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<8x16x508x124xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<8x16x508x124xf32>) -> tensor<8x16x508x124xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x16x512x128xf32>, tensor<5x5xf32>) outs (%init: tensor<8x16x508x124xf32>) -> tensor<8x16x508x124xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<8x16x508x124xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<8x16x508x124xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 8, 1], ["%arg4", 0, 16, 1], ["%arg5", 0, 508, 1], ["%arg6", 0, 124, 1], ["%arg7", 0, 5, 1], ["%arg8", 0, 5, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 667609555}, "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x64x256x128xf32>, tensor<7x7xf32>) outs (%init: tensor<8x64x250x122xf32>) -> tensor<8x64x250x122xf32>": {"operation": "linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x64x256x128xf32>, tensor<7x7xf32>) outs (%init: tensor<8x64x250x122xf32>) -> tensor<8x64x250x122xf32>", "wrapped_operation": "func.func @func_call(%input: tensor<8x64x256x128xf32>, %filter: tensor<7x7xf32>, %init: tensor<8x64x250x122xf32>) -> tensor<8x64x250x122xf32> {\n  %ret = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x64x256x128xf32>, tensor<7x7xf32>) outs (%init: tensor<8x64x250x122xf32>) -> tensor<8x64x250x122xf32>\n  return %ret : tensor<8x64x250x122xf32>\n}", "lowered_operation": "#map = affine_map<(d0, d1) -> (d0 + d1)>\nmodule {\n  func.func @func_call(%arg0: tensor<8x64x256x128xf32>, %arg1: tensor<7x7xf32>, %arg2: tensor<8x64x250x122xf32>) -> tensor<8x64x250x122xf32> {\n    %0 = bufferization.to_memref %arg0 : memref<8x64x256x128xf32>\n    %1 = bufferization.to_memref %arg2 : memref<8x64x250x122xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<8x64x250x122xf32>\n    memref.copy %1, %alloc : memref<8x64x250x122xf32> to memref<8x64x250x122xf32>\n    affine.for %arg3 = 0 to 8 {\n      affine.for %arg4 = 0 to 64 {\n        affine.for %arg5 = 0 to 250 {\n          affine.for %arg6 = 0 to 122 {\n            affine.for %arg7 = 0 to 7 {\n              affine.for %arg8 = 0 to 7 {\n                %3 = affine.apply #map(%arg5, %arg7)\n                %4 = affine.apply #map(%arg6, %arg8)\n                %5 = affine.load %0[%arg3, %arg4, %3, %4] : memref<8x64x256x128xf32>\n                %6 = affine.load %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x64x250x122xf32>\n                %7 = arith.maximumf %6, %5 : f32\n                affine.store %7, %alloc[%arg3, %arg4, %arg5, %arg6] : memref<8x64x250x122xf32>\n              }\n            }\n          }\n        }\n      }\n    }\n    %2 = bufferization.to_tensor %alloc : memref<8x64x250x122xf32>\n    return %2 : tensor<8x64x250x122xf32>\n  }\n}\n\n", "transform_wrapped_operation": "module attributes {torch.debug_module_name = \"Net\"} {\nfunc.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<8x64x250x122xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_input = bufferization.alloc_tensor() : tensor<8x64x256x128xf32>\n%input = linalg.fill ins(%val : f32) outs(%tmp_input : tensor<8x64x256x128xf32>) -> tensor<8x64x256x128xf32>\n%tmp_filter = bufferization.alloc_tensor() : tensor<7x7xf32>\n%filter = linalg.fill ins(%val : f32) outs(%tmp_filter : tensor<7x7xf32>) -> tensor<7x7xf32>\n%tmp_init = bufferization.alloc_tensor() : tensor<8x64x250x122xf32>\n%init = linalg.fill ins(%val : f32) outs(%tmp_init : tensor<8x64x250x122xf32>) -> tensor<8x64x250x122xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.pooling_nchw_max {dilations = dense<1> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} ins (%input, %filter: tensor<8x64x256x128xf32>, tensor<7x7xf32>) outs (%init: tensor<8x64x250x122xf32>) -> tensor<8x64x250x122xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<8x64x250x122xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<8x64x250x122xf32>\n    }\n    return\n}\n}\n", "loops_data": {"nested_loops": [["%arg3", 0, 8, 1], ["%arg4", 0, 64, 1], ["%arg5", 0, 250, 1], ["%arg6", 0, 122, 1], ["%arg7", 0, 7, 1], ["%arg8", 0, 7, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg5 + %arg7", "%arg6 + %arg8"], ["%arg3", "%arg4", "%arg5", "%arg6"]], "store_data": []}, "execution_time": 3045710991}}