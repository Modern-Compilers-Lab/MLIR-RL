{"linalg.matmul ins(%arg0, %arg1 : tensor<1200x1500xf32>, tensor<1500x1000xf32>) outs(%arg2 : tensor<1200x1000xf32>) -> tensor<1200x1000xf32>_0": {"operation": "linalg.matmul ins(%arg0, %arg1 : tensor<1200x1500xf32>, tensor<1500x1000xf32>) outs(%arg2 : tensor<1200x1000xf32>) -> tensor<1200x1000xf32>", "wrapped_operation": "func.func @func_call(%arg0: tensor<1200x1500xf32>, %arg1: tensor<1500x1000xf32>, %arg2: tensor<1200x1000xf32>) -> tensor<1200x1000xf32> {\n  %ret = linalg.matmul ins(%arg0, %arg1 : tensor<1200x1500xf32>, tensor<1500x1000xf32>) outs(%arg2 : tensor<1200x1000xf32>) -> tensor<1200x1000xf32>\n  return %ret : tensor<1200x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<1200x1500xf32>, %arg1: tensor<1500x1000xf32>, %arg2: tensor<1200x1000xf32>) -> tensor<1200x1000xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1500x1000xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1200x1500xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1200x1000xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1200x1000xf32>\n    memref.copy %2, %alloc : memref<1200x1000xf32> to memref<1200x1000xf32>\n    affine.for %arg3 = 0 to 1200 {\n      affine.for %arg4 = 0 to 1000 {\n        affine.for %arg5 = 0 to 1500 {\n          %4 = affine.load %1[%arg3, %arg5] : memref<1200x1500xf32>\n          %5 = affine.load %0[%arg5, %arg4] : memref<1500x1000xf32>\n          %6 = affine.load %alloc[%arg3, %arg4] : memref<1200x1000xf32>\n          %7 = arith.mulf %4, %5 : f32\n          %8 = arith.addf %6, %7 : f32\n          affine.store %8, %alloc[%arg3, %arg4] : memref<1200x1000xf32>\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1200x1000xf32>\n    return %3 : tensor<1200x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printI64(i64)\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @func_call(%arg0: tensor<1200x1500xf32>, %arg1: tensor<1500x1000xf32>, %arg2: tensor<1200x1000xf32>) -> tensor<1200x1000xf32> {\n  %ret = linalg.matmul ins(%arg0, %arg1 : tensor<1200x1500xf32>, tensor<1500x1000xf32>) outs(%arg2 : tensor<1200x1000xf32>) -> tensor<1200x1000xf32>\n  return %ret : tensor<1200x1000xf32>\n}\n\nfunc.func @main() -> i64 {\n  %c1000 = arith.constant 1000 : index\n  %c1200 = arith.constant 1200 : index\n  %c1500 = arith.constant 1500 : index\n  %arg0_temp = bufferization.alloc_tensor(%c1200, %c1500) : tensor<?x?xf32>\n  %arg0 = tensor.cast %arg0_temp : tensor<?x?xf32> to tensor<1200x1500xf32>\n  %arg1_temp = bufferization.alloc_tensor(%c1500, %c1000) : tensor<?x?xf32>\n  %arg1 = tensor.cast %arg1_temp : tensor<?x?xf32> to tensor<1500x1000xf32>\n  %arg2_temp = bufferization.alloc_tensor(%c1200, %c1000) : tensor<?x?xf32>\n  %arg2 = tensor.cast %arg2_temp : tensor<?x?xf32> to tensor<1200x1000xf32>\n  %t0 = func.call @nanoTime() : () -> (i64)\n  %ret_arg = func.call @func_call(%arg0, %arg1, %arg2) : (tensor<1200x1500xf32>, tensor<1500x1000xf32>, tensor<1200x1000xf32>) -> (tensor<1200x1000xf32>)  %t1 = func.call @nanoTime() : () -> (i64)\n  %delta = arith.subi %t1, %t0 : i64\n  return %delta : i64\n}", "loops_data": {"nested_loops": [["%arg3", 0, 1200, 1], ["%arg4", 0, 1000, 1], ["%arg5", 0, 1500, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg5"], ["%arg5", "%arg4"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}}}