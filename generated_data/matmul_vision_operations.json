[["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x130536x768xf32>) -> tensor<1x130536x768xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x130536x768xf32>) -> tensor<1x130536x768xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x130536x768xf32>) -> tensor<1x130536x768xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x130536x768xf32>) -> tensor<1x130536x768xf32>\n  return %ret : tensor<1x130536x768xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x130536x768xf32>) -> tensor<1x130536x768xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x130536x768xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 130536 {\n        affine.for %arg4 = 0 to 768 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x130536x768xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x130536x768xf32>\n    return %0 : tensor<1x130536x768xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x130536x768xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x130536x768xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x130536x768xf32>) -> tensor<1x130536x768xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x130536x768xf32>) -> tensor<1x130536x768xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x130536x768xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x130536x768xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 130536, 1], ["%arg4", 0, 768, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 57728633}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x32634x768xf32>) -> tensor<1x32634x768xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x32634x768xf32>) -> tensor<1x32634x768xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x32634x768xf32>) -> tensor<1x32634x768xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x32634x768xf32>) -> tensor<1x32634x768xf32>\n  return %ret : tensor<1x32634x768xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x32634x768xf32>) -> tensor<1x32634x768xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x32634x768xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 32634 {\n        affine.for %arg4 = 0 to 768 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x32634x768xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x32634x768xf32>\n    return %0 : tensor<1x32634x768xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x32634x768xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x32634x768xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x32634x768xf32>) -> tensor<1x32634x768xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x32634x768xf32>) -> tensor<1x32634x768xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x32634x768xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x32634x768xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 32634, 1], ["%arg4", 0, 768, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 14345564}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x2088576x96xf32>) -> tensor<1x2088576x96xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x2088576x96xf32>) -> tensor<1x2088576x96xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x2088576x96xf32>) -> tensor<1x2088576x96xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x2088576x96xf32>) -> tensor<1x2088576x96xf32>\n  return %ret : tensor<1x2088576x96xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x2088576x96xf32>) -> tensor<1x2088576x96xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x2088576x96xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 2088576 {\n        affine.for %arg4 = 0 to 96 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x2088576x96xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x2088576x96xf32>\n    return %0 : tensor<1x2088576x96xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x2088576x96xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x2088576x96xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x2088576x96xf32>) -> tensor<1x2088576x96xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x2088576x96xf32>) -> tensor<1x2088576x96xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x2088576x96xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x2088576x96xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 2088576, 1], ["%arg4", 0, 96, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 119608180}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 666 {\n        affine.for %arg4 = 0 to 1000 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x666x1000xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %0 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 666, 1], ["%arg4", 0, 1000, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 325890}], ["linalg.batch_matmul ins(%1, %3 : tensor<1x666x1664xf32>, tensor<1x1664x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.batch_matmul ins(%1, %3 : tensor<1x666x1664xf32>, tensor<1x1664x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<1x666x1664xf32>, %3: tensor<1x1664x1000xf32>, %5: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.batch_matmul ins(%1, %3 : tensor<1x666x1664xf32>, tensor<1x1664x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<1x666x1664xf32>, %arg1: tensor<1x1664x1000xf32>, %arg2: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1664x1000xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1x666x1664xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1x666x1000xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    memref.copy %2, %alloc : memref<1x666x1000xf32> to memref<1x666x1000xf32>\n    affine.for %arg3 = 0 to 1 {\n      affine.for %arg4 = 0 to 666 {\n        affine.for %arg5 = 0 to 1000 {\n          affine.for %arg6 = 0 to 1664 {\n            %4 = affine.load %1[%arg3, %arg4, %arg6] : memref<1x666x1664xf32>\n            %5 = affine.load %0[%arg3, %arg6, %arg5] : memref<1x1664x1000xf32>\n            %6 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n            %7 = arith.mulf %4, %5 : f32\n            %8 = arith.addf %6, %7 : f32\n            affine.store %8, %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %3 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<1x666x1664xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<1x666x1664xf32>) -> tensor<1x666x1664xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1664x1000xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1664x1000xf32>) -> tensor<1x1664x1000xf32>\n%tmp_5 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%5 = linalg.fill ins(%val : f32) outs(%tmp_5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.batch_matmul ins(%1, %3 : tensor<1x666x1664xf32>, tensor<1x1664x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 1, 1], ["%arg4", 0, 666, 1], ["%arg5", 0, 1000, 1], ["%arg6", 0, 1664, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg6"], ["%arg3", "%arg6", "%arg5"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}, "execution_time": 4189094937}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 666 {\n        affine.for %arg4 = 0 to 1000 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x666x1000xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %0 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 666, 1], ["%arg4", 0, 1000, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 330300}], ["linalg.batch_matmul ins(%1, %3 : tensor<1x666x384xf32>, tensor<1x384x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.batch_matmul ins(%1, %3 : tensor<1x666x384xf32>, tensor<1x384x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<1x666x384xf32>, %3: tensor<1x384x1000xf32>, %5: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.batch_matmul ins(%1, %3 : tensor<1x666x384xf32>, tensor<1x384x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<1x666x384xf32>, %arg1: tensor<1x384x1000xf32>, %arg2: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x384x1000xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1x666x384xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1x666x1000xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    memref.copy %2, %alloc : memref<1x666x1000xf32> to memref<1x666x1000xf32>\n    affine.for %arg3 = 0 to 1 {\n      affine.for %arg4 = 0 to 666 {\n        affine.for %arg5 = 0 to 1000 {\n          affine.for %arg6 = 0 to 384 {\n            %4 = affine.load %1[%arg3, %arg4, %arg6] : memref<1x666x384xf32>\n            %5 = affine.load %0[%arg3, %arg6, %arg5] : memref<1x384x1000xf32>\n            %6 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n            %7 = arith.mulf %4, %5 : f32\n            %8 = arith.addf %6, %7 : f32\n            affine.store %8, %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %3 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<1x666x384xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<1x666x384xf32>) -> tensor<1x666x384xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x384x1000xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x384x1000xf32>) -> tensor<1x384x1000xf32>\n%tmp_5 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%5 = linalg.fill ins(%val : f32) outs(%tmp_5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.batch_matmul ins(%1, %3 : tensor<1x666x384xf32>, tensor<1x384x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 1, 1], ["%arg4", 0, 666, 1], ["%arg5", 0, 1000, 1], ["%arg6", 0, 384, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg6"], ["%arg3", "%arg6", "%arg5"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}, "execution_time": 936663045}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x32634x1024xf32>) -> tensor<1x32634x1024xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x32634x1024xf32>) -> tensor<1x32634x1024xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x32634x1024xf32>) -> tensor<1x32634x1024xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x32634x1024xf32>) -> tensor<1x32634x1024xf32>\n  return %ret : tensor<1x32634x1024xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x32634x1024xf32>) -> tensor<1x32634x1024xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x32634x1024xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 32634 {\n        affine.for %arg4 = 0 to 1024 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x32634x1024xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x32634x1024xf32>\n    return %0 : tensor<1x32634x1024xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x32634x1024xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x32634x1024xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x32634x1024xf32>) -> tensor<1x32634x1024xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x32634x1024xf32>) -> tensor<1x32634x1024xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x32634x1024xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x32634x1024xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 32634, 1], ["%arg4", 0, 1024, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 19198985}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 666 {\n        affine.for %arg4 = 0 to 1000 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x666x1000xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %0 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 666, 1], ["%arg4", 0, 1000, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 331663}], ["linalg.batch_matmul ins(%1, %3 : tensor<1x666x888xf32>, tensor<1x888x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.batch_matmul ins(%1, %3 : tensor<1x666x888xf32>, tensor<1x888x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<1x666x888xf32>, %3: tensor<1x888x1000xf32>, %5: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.batch_matmul ins(%1, %3 : tensor<1x666x888xf32>, tensor<1x888x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<1x666x888xf32>, %arg1: tensor<1x888x1000xf32>, %arg2: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x888x1000xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1x666x888xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1x666x1000xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    memref.copy %2, %alloc : memref<1x666x1000xf32> to memref<1x666x1000xf32>\n    affine.for %arg3 = 0 to 1 {\n      affine.for %arg4 = 0 to 666 {\n        affine.for %arg5 = 0 to 1000 {\n          affine.for %arg6 = 0 to 888 {\n            %4 = affine.load %1[%arg3, %arg4, %arg6] : memref<1x666x888xf32>\n            %5 = affine.load %0[%arg3, %arg6, %arg5] : memref<1x888x1000xf32>\n            %6 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n            %7 = arith.mulf %4, %5 : f32\n            %8 = arith.addf %6, %7 : f32\n            affine.store %8, %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %3 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<1x666x888xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<1x666x888xf32>) -> tensor<1x666x888xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x888x1000xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x888x1000xf32>) -> tensor<1x888x1000xf32>\n%tmp_5 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%5 = linalg.fill ins(%val : f32) outs(%tmp_5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.batch_matmul ins(%1, %3 : tensor<1x666x888xf32>, tensor<1x888x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 1, 1], ["%arg4", 0, 666, 1], ["%arg5", 0, 1000, 1], ["%arg6", 0, 888, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg6"], ["%arg3", "%arg6", "%arg5"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}, "execution_time": 2221582738}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x522144x1024xf32>) -> tensor<1x522144x1024xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x522144x1024xf32>) -> tensor<1x522144x1024xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x522144x1024xf32>) -> tensor<1x522144x1024xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x522144x1024xf32>) -> tensor<1x522144x1024xf32>\n  return %ret : tensor<1x522144x1024xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x522144x1024xf32>) -> tensor<1x522144x1024xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x522144x1024xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 522144 {\n        affine.for %arg4 = 0 to 1024 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x522144x1024xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x522144x1024xf32>\n    return %0 : tensor<1x522144x1024xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x522144x1024xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x522144x1024xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x522144x1024xf32>) -> tensor<1x522144x1024xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x522144x1024xf32>) -> tensor<1x522144x1024xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x522144x1024xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x522144x1024xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 522144, 1], ["%arg4", 0, 1024, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 307800727}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 666 {\n        affine.for %arg4 = 0 to 1000 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x666x1000xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %0 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 666, 1], ["%arg4", 0, 1000, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 330675}], ["linalg.batch_matmul ins(%1, %3 : tensor<1x666x528xf32>, tensor<1x528x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.batch_matmul ins(%1, %3 : tensor<1x666x528xf32>, tensor<1x528x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<1x666x528xf32>, %3: tensor<1x528x1000xf32>, %5: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.batch_matmul ins(%1, %3 : tensor<1x666x528xf32>, tensor<1x528x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<1x666x528xf32>, %arg1: tensor<1x528x1000xf32>, %arg2: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x528x1000xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1x666x528xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1x666x1000xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    memref.copy %2, %alloc : memref<1x666x1000xf32> to memref<1x666x1000xf32>\n    affine.for %arg3 = 0 to 1 {\n      affine.for %arg4 = 0 to 666 {\n        affine.for %arg5 = 0 to 1000 {\n          affine.for %arg6 = 0 to 528 {\n            %4 = affine.load %1[%arg3, %arg4, %arg6] : memref<1x666x528xf32>\n            %5 = affine.load %0[%arg3, %arg6, %arg5] : memref<1x528x1000xf32>\n            %6 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n            %7 = arith.mulf %4, %5 : f32\n            %8 = arith.addf %6, %7 : f32\n            affine.store %8, %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %3 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<1x666x528xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<1x666x528xf32>) -> tensor<1x666x528xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x528x1000xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x528x1000xf32>) -> tensor<1x528x1000xf32>\n%tmp_5 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%5 = linalg.fill ins(%val : f32) outs(%tmp_5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.batch_matmul ins(%1, %3 : tensor<1x666x528xf32>, tensor<1x528x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 1, 1], ["%arg4", 0, 666, 1], ["%arg5", 0, 1000, 1], ["%arg6", 0, 528, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg6"], ["%arg3", "%arg6", "%arg5"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}, "execution_time": 1304457982}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x2088576x128xf32>) -> tensor<1x2088576x128xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x2088576x128xf32>) -> tensor<1x2088576x128xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x2088576x128xf32>) -> tensor<1x2088576x128xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x2088576x128xf32>) -> tensor<1x2088576x128xf32>\n  return %ret : tensor<1x2088576x128xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x2088576x128xf32>) -> tensor<1x2088576x128xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x2088576x128xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 2088576 {\n        affine.for %arg4 = 0 to 128 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x2088576x128xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x2088576x128xf32>\n    return %0 : tensor<1x2088576x128xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x2088576x128xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x2088576x128xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x2088576x128xf32>) -> tensor<1x2088576x128xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x2088576x128xf32>) -> tensor<1x2088576x128xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x2088576x128xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x2088576x128xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 2088576, 1], ["%arg4", 0, 128, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 156182874}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 666 {\n        affine.for %arg4 = 0 to 1000 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x666x1000xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %0 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 666, 1], ["%arg4", 0, 1000, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 331017}], ["linalg.batch_matmul ins(%1, %3 : tensor<1x666x1920xf32>, tensor<1x1920x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.batch_matmul ins(%1, %3 : tensor<1x666x1920xf32>, tensor<1x1920x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<1x666x1920xf32>, %3: tensor<1x1920x1000xf32>, %5: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.batch_matmul ins(%1, %3 : tensor<1x666x1920xf32>, tensor<1x1920x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<1x666x1920xf32>, %arg1: tensor<1x1920x1000xf32>, %arg2: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1920x1000xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1x666x1920xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1x666x1000xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    memref.copy %2, %alloc : memref<1x666x1000xf32> to memref<1x666x1000xf32>\n    affine.for %arg3 = 0 to 1 {\n      affine.for %arg4 = 0 to 666 {\n        affine.for %arg5 = 0 to 1000 {\n          affine.for %arg6 = 0 to 1920 {\n            %4 = affine.load %1[%arg3, %arg4, %arg6] : memref<1x666x1920xf32>\n            %5 = affine.load %0[%arg3, %arg6, %arg5] : memref<1x1920x1000xf32>\n            %6 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n            %7 = arith.mulf %4, %5 : f32\n            %8 = arith.addf %6, %7 : f32\n            affine.store %8, %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %3 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<1x666x1920xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<1x666x1920xf32>) -> tensor<1x666x1920xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1920x1000xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1920x1000xf32>) -> tensor<1x1920x1000xf32>\n%tmp_5 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%5 = linalg.fill ins(%val : f32) outs(%tmp_5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.batch_matmul ins(%1, %3 : tensor<1x666x1920xf32>, tensor<1x1920x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 1, 1], ["%arg4", 0, 666, 1], ["%arg5", 0, 1000, 1], ["%arg6", 0, 1920, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg6"], ["%arg3", "%arg6", "%arg5"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}, "execution_time": 4847515850}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x32634x8192xf32>) -> tensor<1x32634x8192xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x32634x8192xf32>) -> tensor<1x32634x8192xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x32634x8192xf32>) -> tensor<1x32634x8192xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x32634x8192xf32>) -> tensor<1x32634x8192xf32>\n  return %ret : tensor<1x32634x8192xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x32634x8192xf32>) -> tensor<1x32634x8192xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x32634x8192xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 32634 {\n        affine.for %arg4 = 0 to 8192 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x32634x8192xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x32634x8192xf32>\n    return %0 : tensor<1x32634x8192xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x32634x8192xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x32634x8192xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x32634x8192xf32>) -> tensor<1x32634x8192xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x32634x8192xf32>) -> tensor<1x32634x8192xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x32634x8192xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x32634x8192xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 32634, 1], ["%arg4", 0, 8192, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 153837159}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 666 {\n        affine.for %arg4 = 0 to 1000 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x666x1000xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %0 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 666, 1], ["%arg4", 0, 1000, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 332429}], ["linalg.batch_matmul ins(%1, %3 : tensor<1x666x608xf32>, tensor<1x608x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.batch_matmul ins(%1, %3 : tensor<1x666x608xf32>, tensor<1x608x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<1x666x608xf32>, %3: tensor<1x608x1000xf32>, %5: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.batch_matmul ins(%1, %3 : tensor<1x666x608xf32>, tensor<1x608x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<1x666x608xf32>, %arg1: tensor<1x608x1000xf32>, %arg2: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x608x1000xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1x666x608xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1x666x1000xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    memref.copy %2, %alloc : memref<1x666x1000xf32> to memref<1x666x1000xf32>\n    affine.for %arg3 = 0 to 1 {\n      affine.for %arg4 = 0 to 666 {\n        affine.for %arg5 = 0 to 1000 {\n          affine.for %arg6 = 0 to 608 {\n            %4 = affine.load %1[%arg3, %arg4, %arg6] : memref<1x666x608xf32>\n            %5 = affine.load %0[%arg3, %arg6, %arg5] : memref<1x608x1000xf32>\n            %6 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n            %7 = arith.mulf %4, %5 : f32\n            %8 = arith.addf %6, %7 : f32\n            affine.store %8, %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %3 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<1x666x608xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<1x666x608xf32>) -> tensor<1x666x608xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x608x1000xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x608x1000xf32>) -> tensor<1x608x1000xf32>\n%tmp_5 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%5 = linalg.fill ins(%val : f32) outs(%tmp_5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.batch_matmul ins(%1, %3 : tensor<1x666x608xf32>, tensor<1x608x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 1, 1], ["%arg4", 0, 666, 1], ["%arg5", 0, 1000, 1], ["%arg6", 0, 608, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg6"], ["%arg3", "%arg6", "%arg5"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}, "execution_time": 1508552361}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x130536x2048xf32>) -> tensor<1x130536x2048xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x130536x2048xf32>) -> tensor<1x130536x2048xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x130536x2048xf32>) -> tensor<1x130536x2048xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x130536x2048xf32>) -> tensor<1x130536x2048xf32>\n  return %ret : tensor<1x130536x2048xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x130536x2048xf32>) -> tensor<1x130536x2048xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x130536x2048xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 130536 {\n        affine.for %arg4 = 0 to 2048 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x130536x2048xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x130536x2048xf32>\n    return %0 : tensor<1x130536x2048xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x130536x2048xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x130536x2048xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x130536x2048xf32>) -> tensor<1x130536x2048xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x130536x2048xf32>) -> tensor<1x130536x2048xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x130536x2048xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x130536x2048xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 130536, 1], ["%arg4", 0, 2048, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 153464620}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x2088576x512xf32>) -> tensor<1x2088576x512xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x2088576x512xf32>) -> tensor<1x2088576x512xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x2088576x512xf32>) -> tensor<1x2088576x512xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x2088576x512xf32>) -> tensor<1x2088576x512xf32>\n  return %ret : tensor<1x2088576x512xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x2088576x512xf32>) -> tensor<1x2088576x512xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x2088576x512xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 2088576 {\n        affine.for %arg4 = 0 to 512 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x2088576x512xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x2088576x512xf32>\n    return %0 : tensor<1x2088576x512xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x2088576x512xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x2088576x512xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x2088576x512xf32>) -> tensor<1x2088576x512xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x2088576x512xf32>) -> tensor<1x2088576x512xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x2088576x512xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x2088576x512xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 2088576, 1], ["%arg4", 0, 512, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 617432034}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 666 {\n        affine.for %arg4 = 0 to 1000 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x666x1000xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %0 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 666, 1], ["%arg4", 0, 1000, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 326341}], ["linalg.batch_matmul ins(%1, %3 : tensor<1x666x1280xf32>, tensor<1x1280x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.batch_matmul ins(%1, %3 : tensor<1x666x1280xf32>, tensor<1x1280x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<1x666x1280xf32>, %3: tensor<1x1280x1000xf32>, %5: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.batch_matmul ins(%1, %3 : tensor<1x666x1280xf32>, tensor<1x1280x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<1x666x1280xf32>, %arg1: tensor<1x1280x1000xf32>, %arg2: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1280x1000xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1x666x1280xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1x666x1000xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    memref.copy %2, %alloc : memref<1x666x1000xf32> to memref<1x666x1000xf32>\n    affine.for %arg3 = 0 to 1 {\n      affine.for %arg4 = 0 to 666 {\n        affine.for %arg5 = 0 to 1000 {\n          affine.for %arg6 = 0 to 1280 {\n            %4 = affine.load %1[%arg3, %arg4, %arg6] : memref<1x666x1280xf32>\n            %5 = affine.load %0[%arg3, %arg6, %arg5] : memref<1x1280x1000xf32>\n            %6 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n            %7 = arith.mulf %4, %5 : f32\n            %8 = arith.addf %6, %7 : f32\n            affine.store %8, %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %3 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<1x666x1280xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<1x666x1280xf32>) -> tensor<1x666x1280xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1280x1000xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1280x1000xf32>) -> tensor<1x1280x1000xf32>\n%tmp_5 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%5 = linalg.fill ins(%val : f32) outs(%tmp_5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.batch_matmul ins(%1, %3 : tensor<1x666x1280xf32>, tensor<1x1280x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 1, 1], ["%arg4", 0, 666, 1], ["%arg5", 0, 1000, 1], ["%arg6", 0, 1280, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg6"], ["%arg3", "%arg6", "%arg5"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}, "execution_time": 3215107522}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 666 {\n        affine.for %arg4 = 0 to 1000 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x666x1000xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %0 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 666, 1], ["%arg4", 0, 1000, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 326525}], ["linalg.batch_matmul ins(%1, %3 : tensor<1x666x1624xf32>, tensor<1x1624x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.batch_matmul ins(%1, %3 : tensor<1x666x1624xf32>, tensor<1x1624x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<1x666x1624xf32>, %3: tensor<1x1624x1000xf32>, %5: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.batch_matmul ins(%1, %3 : tensor<1x666x1624xf32>, tensor<1x1624x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<1x666x1624xf32>, %arg1: tensor<1x1624x1000xf32>, %arg2: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1624x1000xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1x666x1624xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1x666x1000xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    memref.copy %2, %alloc : memref<1x666x1000xf32> to memref<1x666x1000xf32>\n    affine.for %arg3 = 0 to 1 {\n      affine.for %arg4 = 0 to 666 {\n        affine.for %arg5 = 0 to 1000 {\n          affine.for %arg6 = 0 to 1624 {\n            %4 = affine.load %1[%arg3, %arg4, %arg6] : memref<1x666x1624xf32>\n            %5 = affine.load %0[%arg3, %arg6, %arg5] : memref<1x1624x1000xf32>\n            %6 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n            %7 = arith.mulf %4, %5 : f32\n            %8 = arith.addf %6, %7 : f32\n            affine.store %8, %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %3 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<1x666x1624xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<1x666x1624xf32>) -> tensor<1x666x1624xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1624x1000xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1624x1000xf32>) -> tensor<1x1624x1000xf32>\n%tmp_5 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%5 = linalg.fill ins(%val : f32) outs(%tmp_5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.batch_matmul ins(%1, %3 : tensor<1x666x1624xf32>, tensor<1x1624x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 1, 1], ["%arg4", 0, 666, 1], ["%arg5", 0, 1000, 1], ["%arg6", 0, 1624, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg6"], ["%arg3", "%arg6", "%arg5"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}, "execution_time": 4093743220}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 666 {\n        affine.for %arg4 = 0 to 1000 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x666x1000xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %0 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 666, 1], ["%arg4", 0, 1000, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 326523}], ["linalg.batch_matmul ins(%1, %3 : tensor<1x666x1296xf32>, tensor<1x1296x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.batch_matmul ins(%1, %3 : tensor<1x666x1296xf32>, tensor<1x1296x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<1x666x1296xf32>, %3: tensor<1x1296x1000xf32>, %5: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.batch_matmul ins(%1, %3 : tensor<1x666x1296xf32>, tensor<1x1296x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<1x666x1296xf32>, %arg1: tensor<1x1296x1000xf32>, %arg2: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1296x1000xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1x666x1296xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1x666x1000xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    memref.copy %2, %alloc : memref<1x666x1000xf32> to memref<1x666x1000xf32>\n    affine.for %arg3 = 0 to 1 {\n      affine.for %arg4 = 0 to 666 {\n        affine.for %arg5 = 0 to 1000 {\n          affine.for %arg6 = 0 to 1296 {\n            %4 = affine.load %1[%arg3, %arg4, %arg6] : memref<1x666x1296xf32>\n            %5 = affine.load %0[%arg3, %arg6, %arg5] : memref<1x1296x1000xf32>\n            %6 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n            %7 = arith.mulf %4, %5 : f32\n            %8 = arith.addf %6, %7 : f32\n            affine.store %8, %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %3 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<1x666x1296xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<1x666x1296xf32>) -> tensor<1x666x1296xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1296x1000xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1296x1000xf32>) -> tensor<1x1296x1000xf32>\n%tmp_5 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%5 = linalg.fill ins(%val : f32) outs(%tmp_5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.batch_matmul ins(%1, %3 : tensor<1x666x1296xf32>, tensor<1x1296x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 1, 1], ["%arg4", 0, 666, 1], ["%arg5", 0, 1000, 1], ["%arg6", 0, 1296, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg6"], ["%arg3", "%arg6", "%arg5"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}, "execution_time": 3259053517}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 666 {\n        affine.for %arg4 = 0 to 1000 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x666x1000xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %0 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 666, 1], ["%arg4", 0, 1000, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 326969}], ["linalg.batch_matmul ins(%1, %3 : tensor<1x666x1088xf32>, tensor<1x1088x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.batch_matmul ins(%1, %3 : tensor<1x666x1088xf32>, tensor<1x1088x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<1x666x1088xf32>, %3: tensor<1x1088x1000xf32>, %5: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.batch_matmul ins(%1, %3 : tensor<1x666x1088xf32>, tensor<1x1088x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<1x666x1088xf32>, %arg1: tensor<1x1088x1000xf32>, %arg2: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1088x1000xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1x666x1088xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1x666x1000xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    memref.copy %2, %alloc : memref<1x666x1000xf32> to memref<1x666x1000xf32>\n    affine.for %arg3 = 0 to 1 {\n      affine.for %arg4 = 0 to 666 {\n        affine.for %arg5 = 0 to 1000 {\n          affine.for %arg6 = 0 to 1088 {\n            %4 = affine.load %1[%arg3, %arg4, %arg6] : memref<1x666x1088xf32>\n            %5 = affine.load %0[%arg3, %arg6, %arg5] : memref<1x1088x1000xf32>\n            %6 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n            %7 = arith.mulf %4, %5 : f32\n            %8 = arith.addf %6, %7 : f32\n            affine.store %8, %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %3 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<1x666x1088xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<1x666x1088xf32>) -> tensor<1x666x1088xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1088x1000xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1088x1000xf32>) -> tensor<1x1088x1000xf32>\n%tmp_5 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%5 = linalg.fill ins(%val : f32) outs(%tmp_5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.batch_matmul ins(%1, %3 : tensor<1x666x1088xf32>, tensor<1x1088x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 1, 1], ["%arg4", 0, 666, 1], ["%arg5", 0, 1000, 1], ["%arg6", 0, 1088, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg6"], ["%arg3", "%arg6", "%arg5"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}, "execution_time": 2864327537}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x2088576x384xf32>) -> tensor<1x2088576x384xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x2088576x384xf32>) -> tensor<1x2088576x384xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x2088576x384xf32>) -> tensor<1x2088576x384xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x2088576x384xf32>) -> tensor<1x2088576x384xf32>\n  return %ret : tensor<1x2088576x384xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x2088576x384xf32>) -> tensor<1x2088576x384xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x2088576x384xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 2088576 {\n        affine.for %arg4 = 0 to 384 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x2088576x384xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x2088576x384xf32>\n    return %0 : tensor<1x2088576x384xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x2088576x384xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x2088576x384xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x2088576x384xf32>) -> tensor<1x2088576x384xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x2088576x384xf32>) -> tensor<1x2088576x384xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x2088576x384xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x2088576x384xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 2088576, 1], ["%arg4", 0, 384, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 481070763}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x522144x256xf32>) -> tensor<1x522144x256xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x522144x256xf32>) -> tensor<1x522144x256xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x522144x256xf32>) -> tensor<1x522144x256xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x522144x256xf32>) -> tensor<1x522144x256xf32>\n  return %ret : tensor<1x522144x256xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x522144x256xf32>) -> tensor<1x522144x256xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x522144x256xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 522144 {\n        affine.for %arg4 = 0 to 256 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x522144x256xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x522144x256xf32>\n    return %0 : tensor<1x522144x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x522144x256xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x522144x256xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x522144x256xf32>) -> tensor<1x522144x256xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x522144x256xf32>) -> tensor<1x522144x256xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x522144x256xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x522144x256xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 522144, 1], ["%arg4", 0, 256, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 80059123}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x2088576x1024xf32>) -> tensor<1x2088576x1024xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x2088576x1024xf32>) -> tensor<1x2088576x1024xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x2088576x1024xf32>) -> tensor<1x2088576x1024xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x2088576x1024xf32>) -> tensor<1x2088576x1024xf32>\n  return %ret : tensor<1x2088576x1024xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x2088576x1024xf32>) -> tensor<1x2088576x1024xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x2088576x1024xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 2088576 {\n        affine.for %arg4 = 0 to 1024 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x2088576x1024xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x2088576x1024xf32>\n    return %0 : tensor<1x2088576x1024xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x2088576x1024xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x2088576x1024xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x2088576x1024xf32>) -> tensor<1x2088576x1024xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x2088576x1024xf32>) -> tensor<1x2088576x1024xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x2088576x1024xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x2088576x1024xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 2088576, 1], ["%arg4", 0, 1024, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 1270710779}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 666 {\n        affine.for %arg4 = 0 to 1000 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x666x1000xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %0 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 666, 1], ["%arg4", 0, 1000, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 326910}], ["linalg.batch_matmul ins(%1, %3 : tensor<1x666x1360xf32>, tensor<1x1360x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.batch_matmul ins(%1, %3 : tensor<1x666x1360xf32>, tensor<1x1360x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<1x666x1360xf32>, %3: tensor<1x1360x1000xf32>, %5: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.batch_matmul ins(%1, %3 : tensor<1x666x1360xf32>, tensor<1x1360x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<1x666x1360xf32>, %arg1: tensor<1x1360x1000xf32>, %arg2: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1360x1000xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1x666x1360xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1x666x1000xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    memref.copy %2, %alloc : memref<1x666x1000xf32> to memref<1x666x1000xf32>\n    affine.for %arg3 = 0 to 1 {\n      affine.for %arg4 = 0 to 666 {\n        affine.for %arg5 = 0 to 1000 {\n          affine.for %arg6 = 0 to 1360 {\n            %4 = affine.load %1[%arg3, %arg4, %arg6] : memref<1x666x1360xf32>\n            %5 = affine.load %0[%arg3, %arg6, %arg5] : memref<1x1360x1000xf32>\n            %6 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n            %7 = arith.mulf %4, %5 : f32\n            %8 = arith.addf %6, %7 : f32\n            affine.store %8, %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %3 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<1x666x1360xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<1x666x1360xf32>) -> tensor<1x666x1360xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1360x1000xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1360x1000xf32>) -> tensor<1x1360x1000xf32>\n%tmp_5 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%5 = linalg.fill ins(%val : f32) outs(%tmp_5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.batch_matmul ins(%1, %3 : tensor<1x666x1360xf32>, tensor<1x1360x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 1, 1], ["%arg4", 0, 666, 1], ["%arg5", 0, 1000, 1], ["%arg6", 0, 1360, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg6"], ["%arg3", "%arg6", "%arg5"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}, "execution_time": 3446028308}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x522144x768xf32>) -> tensor<1x522144x768xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x522144x768xf32>) -> tensor<1x522144x768xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x522144x768xf32>) -> tensor<1x522144x768xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x522144x768xf32>) -> tensor<1x522144x768xf32>\n  return %ret : tensor<1x522144x768xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x522144x768xf32>) -> tensor<1x522144x768xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x522144x768xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 522144 {\n        affine.for %arg4 = 0 to 768 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x522144x768xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x522144x768xf32>\n    return %0 : tensor<1x522144x768xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x522144x768xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x522144x768xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x522144x768xf32>) -> tensor<1x522144x768xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x522144x768xf32>) -> tensor<1x522144x768xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x522144x768xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x522144x768xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 522144, 1], ["%arg4", 0, 768, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 229888581}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x4096xf32>) -> tensor<1x666x4096xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x4096xf32>) -> tensor<1x666x4096xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x666x4096xf32>) -> tensor<1x666x4096xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x4096xf32>) -> tensor<1x666x4096xf32>\n  return %ret : tensor<1x666x4096xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x666x4096xf32>) -> tensor<1x666x4096xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x4096xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 666 {\n        affine.for %arg4 = 0 to 4096 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x666x4096xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x666x4096xf32>\n    return %0 : tensor<1x666x4096xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x4096xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x666x4096xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x666x4096xf32>) -> tensor<1x666x4096xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x4096xf32>) -> tensor<1x666x4096xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x4096xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x4096xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 666, 1], ["%arg4", 0, 4096, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 1387169}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 666 {\n        affine.for %arg4 = 0 to 1000 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x666x1000xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %0 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 666, 1], ["%arg4", 0, 1000, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 331663}], ["linalg.batch_matmul ins(%1, %3 : tensor<1x666x2016xf32>, tensor<1x2016x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.batch_matmul ins(%1, %3 : tensor<1x666x2016xf32>, tensor<1x2016x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<1x666x2016xf32>, %3: tensor<1x2016x1000xf32>, %5: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.batch_matmul ins(%1, %3 : tensor<1x666x2016xf32>, tensor<1x2016x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<1x666x2016xf32>, %arg1: tensor<1x2016x1000xf32>, %arg2: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x2016x1000xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1x666x2016xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1x666x1000xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    memref.copy %2, %alloc : memref<1x666x1000xf32> to memref<1x666x1000xf32>\n    affine.for %arg3 = 0 to 1 {\n      affine.for %arg4 = 0 to 666 {\n        affine.for %arg5 = 0 to 1000 {\n          affine.for %arg6 = 0 to 2016 {\n            %4 = affine.load %1[%arg3, %arg4, %arg6] : memref<1x666x2016xf32>\n            %5 = affine.load %0[%arg3, %arg6, %arg5] : memref<1x2016x1000xf32>\n            %6 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n            %7 = arith.mulf %4, %5 : f32\n            %8 = arith.addf %6, %7 : f32\n            affine.store %8, %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %3 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<1x666x2016xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<1x666x2016xf32>) -> tensor<1x666x2016xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x2016x1000xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x2016x1000xf32>) -> tensor<1x2016x1000xf32>\n%tmp_5 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%5 = linalg.fill ins(%val : f32) outs(%tmp_5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.batch_matmul ins(%1, %3 : tensor<1x666x2016xf32>, tensor<1x2016x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 1, 1], ["%arg4", 0, 666, 1], ["%arg5", 0, 1000, 1], ["%arg6", 0, 2016, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg6"], ["%arg3", "%arg6", "%arg5"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}, "execution_time": 5395259155}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x522144x1536xf32>) -> tensor<1x522144x1536xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x522144x1536xf32>) -> tensor<1x522144x1536xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x522144x1536xf32>) -> tensor<1x522144x1536xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x522144x1536xf32>) -> tensor<1x522144x1536xf32>\n  return %ret : tensor<1x522144x1536xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x522144x1536xf32>) -> tensor<1x522144x1536xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x522144x1536xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 522144 {\n        affine.for %arg4 = 0 to 1536 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x522144x1536xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x522144x1536xf32>\n    return %0 : tensor<1x522144x1536xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x522144x1536xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x522144x1536xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x522144x1536xf32>) -> tensor<1x522144x1536xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x522144x1536xf32>) -> tensor<1x522144x1536xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x522144x1536xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x522144x1536xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 522144, 1], ["%arg4", 0, 1536, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 473529595}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 666 {\n        affine.for %arg4 = 0 to 1000 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x666x1000xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %0 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 666, 1], ["%arg4", 0, 1000, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 330872}], ["linalg.batch_matmul ins(%1, %3 : tensor<1x666x672xf32>, tensor<1x672x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.batch_matmul ins(%1, %3 : tensor<1x666x672xf32>, tensor<1x672x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<1x666x672xf32>, %3: tensor<1x672x1000xf32>, %5: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.batch_matmul ins(%1, %3 : tensor<1x666x672xf32>, tensor<1x672x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<1x666x672xf32>, %arg1: tensor<1x672x1000xf32>, %arg2: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x672x1000xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1x666x672xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1x666x1000xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    memref.copy %2, %alloc : memref<1x666x1000xf32> to memref<1x666x1000xf32>\n    affine.for %arg3 = 0 to 1 {\n      affine.for %arg4 = 0 to 666 {\n        affine.for %arg5 = 0 to 1000 {\n          affine.for %arg6 = 0 to 672 {\n            %4 = affine.load %1[%arg3, %arg4, %arg6] : memref<1x666x672xf32>\n            %5 = affine.load %0[%arg3, %arg6, %arg5] : memref<1x672x1000xf32>\n            %6 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n            %7 = arith.mulf %4, %5 : f32\n            %8 = arith.addf %6, %7 : f32\n            affine.store %8, %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %3 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<1x666x672xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<1x666x672xf32>) -> tensor<1x666x672xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x672x1000xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x672x1000xf32>) -> tensor<1x672x1000xf32>\n%tmp_5 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%5 = linalg.fill ins(%val : f32) outs(%tmp_5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.batch_matmul ins(%1, %3 : tensor<1x666x672xf32>, tensor<1x672x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 1, 1], ["%arg4", 0, 666, 1], ["%arg5", 0, 1000, 1], ["%arg6", 0, 672, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg6"], ["%arg3", "%arg6", "%arg5"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}, "execution_time": 1710627928}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x522144x192xf32>) -> tensor<1x522144x192xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x522144x192xf32>) -> tensor<1x522144x192xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x522144x192xf32>) -> tensor<1x522144x192xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x522144x192xf32>) -> tensor<1x522144x192xf32>\n  return %ret : tensor<1x522144x192xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x522144x192xf32>) -> tensor<1x522144x192xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x522144x192xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 522144 {\n        affine.for %arg4 = 0 to 192 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x522144x192xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x522144x192xf32>\n    return %0 : tensor<1x522144x192xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x522144x192xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x522144x192xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x522144x192xf32>) -> tensor<1x522144x192xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x522144x192xf32>) -> tensor<1x522144x192xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x522144x192xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x522144x192xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 522144, 1], ["%arg4", 0, 192, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 59499958}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 666 {\n        affine.for %arg4 = 0 to 1000 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x666x1000xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %0 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 666, 1], ["%arg4", 0, 1000, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 336187}], ["linalg.batch_matmul ins(%1, %3 : tensor<1x666x1056xf32>, tensor<1x1056x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.batch_matmul ins(%1, %3 : tensor<1x666x1056xf32>, tensor<1x1056x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<1x666x1056xf32>, %3: tensor<1x1056x1000xf32>, %5: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.batch_matmul ins(%1, %3 : tensor<1x666x1056xf32>, tensor<1x1056x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<1x666x1056xf32>, %arg1: tensor<1x1056x1000xf32>, %arg2: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1056x1000xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1x666x1056xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1x666x1000xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    memref.copy %2, %alloc : memref<1x666x1000xf32> to memref<1x666x1000xf32>\n    affine.for %arg3 = 0 to 1 {\n      affine.for %arg4 = 0 to 666 {\n        affine.for %arg5 = 0 to 1000 {\n          affine.for %arg6 = 0 to 1056 {\n            %4 = affine.load %1[%arg3, %arg4, %arg6] : memref<1x666x1056xf32>\n            %5 = affine.load %0[%arg3, %arg6, %arg5] : memref<1x1056x1000xf32>\n            %6 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n            %7 = arith.mulf %4, %5 : f32\n            %8 = arith.addf %6, %7 : f32\n            affine.store %8, %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %3 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<1x666x1056xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<1x666x1056xf32>) -> tensor<1x666x1056xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1056x1000xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1056x1000xf32>) -> tensor<1x1056x1000xf32>\n%tmp_5 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%5 = linalg.fill ins(%val : f32) outs(%tmp_5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.batch_matmul ins(%1, %3 : tensor<1x666x1056xf32>, tensor<1x1056x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 1, 1], ["%arg4", 0, 666, 1], ["%arg5", 0, 1000, 1], ["%arg6", 0, 1056, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg6"], ["%arg3", "%arg6", "%arg5"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}, "execution_time": 2749874217}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 666 {\n        affine.for %arg4 = 0 to 1000 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x666x1000xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %0 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 666, 1], ["%arg4", 0, 1000, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 331144}], ["linalg.batch_matmul ins(%1, %3 : tensor<1x666x4032xf32>, tensor<1x4032x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.batch_matmul ins(%1, %3 : tensor<1x666x4032xf32>, tensor<1x4032x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<1x666x4032xf32>, %3: tensor<1x4032x1000xf32>, %5: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.batch_matmul ins(%1, %3 : tensor<1x666x4032xf32>, tensor<1x4032x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<1x666x4032xf32>, %arg1: tensor<1x4032x1000xf32>, %arg2: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x4032x1000xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1x666x4032xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1x666x1000xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    memref.copy %2, %alloc : memref<1x666x1000xf32> to memref<1x666x1000xf32>\n    affine.for %arg3 = 0 to 1 {\n      affine.for %arg4 = 0 to 666 {\n        affine.for %arg5 = 0 to 1000 {\n          affine.for %arg6 = 0 to 4032 {\n            %4 = affine.load %1[%arg3, %arg4, %arg6] : memref<1x666x4032xf32>\n            %5 = affine.load %0[%arg3, %arg6, %arg5] : memref<1x4032x1000xf32>\n            %6 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n            %7 = arith.mulf %4, %5 : f32\n            %8 = arith.addf %6, %7 : f32\n            affine.store %8, %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %3 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<1x666x4032xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<1x666x4032xf32>) -> tensor<1x666x4032xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x4032x1000xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x4032x1000xf32>) -> tensor<1x4032x1000xf32>\n%tmp_5 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%5 = linalg.fill ins(%val : f32) outs(%tmp_5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.batch_matmul ins(%1, %3 : tensor<1x666x4032xf32>, tensor<1x4032x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 1, 1], ["%arg4", 0, 666, 1], ["%arg5", 0, 1000, 1], ["%arg6", 0, 4032, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg6"], ["%arg3", "%arg6", "%arg5"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}, "execution_time": 10565808702}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x32634x6144xf32>) -> tensor<1x32634x6144xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x32634x6144xf32>) -> tensor<1x32634x6144xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x32634x6144xf32>) -> tensor<1x32634x6144xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x32634x6144xf32>) -> tensor<1x32634x6144xf32>\n  return %ret : tensor<1x32634x6144xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x32634x6144xf32>) -> tensor<1x32634x6144xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x32634x6144xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 32634 {\n        affine.for %arg4 = 0 to 6144 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x32634x6144xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x32634x6144xf32>\n    return %0 : tensor<1x32634x6144xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x32634x6144xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x32634x6144xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x32634x6144xf32>) -> tensor<1x32634x6144xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x32634x6144xf32>) -> tensor<1x32634x6144xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x32634x6144xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x32634x6144xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 32634, 1], ["%arg4", 0, 6144, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 118822893}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 666 {\n        affine.for %arg4 = 0 to 1000 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x666x1000xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %0 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 666, 1], ["%arg4", 0, 1000, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 338454}], ["linalg.batch_matmul ins(%1, %3 : tensor<1x666x2048xf32>, tensor<1x2048x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.batch_matmul ins(%1, %3 : tensor<1x666x2048xf32>, tensor<1x2048x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<1x666x2048xf32>, %3: tensor<1x2048x1000xf32>, %5: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.batch_matmul ins(%1, %3 : tensor<1x666x2048xf32>, tensor<1x2048x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<1x666x2048xf32>, %arg1: tensor<1x2048x1000xf32>, %arg2: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x2048x1000xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1x666x2048xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1x666x1000xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    memref.copy %2, %alloc : memref<1x666x1000xf32> to memref<1x666x1000xf32>\n    affine.for %arg3 = 0 to 1 {\n      affine.for %arg4 = 0 to 666 {\n        affine.for %arg5 = 0 to 1000 {\n          affine.for %arg6 = 0 to 2048 {\n            %4 = affine.load %1[%arg3, %arg4, %arg6] : memref<1x666x2048xf32>\n            %5 = affine.load %0[%arg3, %arg6, %arg5] : memref<1x2048x1000xf32>\n            %6 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n            %7 = arith.mulf %4, %5 : f32\n            %8 = arith.addf %6, %7 : f32\n            affine.store %8, %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %3 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<1x666x2048xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<1x666x2048xf32>) -> tensor<1x666x2048xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x2048x1000xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x2048x1000xf32>) -> tensor<1x2048x1000xf32>\n%tmp_5 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%5 = linalg.fill ins(%val : f32) outs(%tmp_5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.batch_matmul ins(%1, %3 : tensor<1x666x2048xf32>, tensor<1x2048x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 1, 1], ["%arg4", 0, 666, 1], ["%arg5", 0, 1000, 1], ["%arg6", 0, 2048, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg6"], ["%arg3", "%arg6", "%arg5"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}, "execution_time": 5518512942}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x130536x1536xf32>) -> tensor<1x130536x1536xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x130536x1536xf32>) -> tensor<1x130536x1536xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x130536x1536xf32>) -> tensor<1x130536x1536xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x130536x1536xf32>) -> tensor<1x130536x1536xf32>\n  return %ret : tensor<1x130536x1536xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x130536x1536xf32>) -> tensor<1x130536x1536xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x130536x1536xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 130536 {\n        affine.for %arg4 = 0 to 1536 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x130536x1536xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x130536x1536xf32>\n    return %0 : tensor<1x130536x1536xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x130536x1536xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x130536x1536xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x130536x1536xf32>) -> tensor<1x130536x1536xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x130536x1536xf32>) -> tensor<1x130536x1536xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x130536x1536xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x130536x1536xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 130536, 1], ["%arg4", 0, 1536, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 117897909}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x32634x1536xf32>) -> tensor<1x32634x1536xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x32634x1536xf32>) -> tensor<1x32634x1536xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x32634x1536xf32>) -> tensor<1x32634x1536xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x32634x1536xf32>) -> tensor<1x32634x1536xf32>\n  return %ret : tensor<1x32634x1536xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x32634x1536xf32>) -> tensor<1x32634x1536xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x32634x1536xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 32634 {\n        affine.for %arg4 = 0 to 1536 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x32634x1536xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x32634x1536xf32>\n    return %0 : tensor<1x32634x1536xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x32634x1536xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x32634x1536xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x32634x1536xf32>) -> tensor<1x32634x1536xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x32634x1536xf32>) -> tensor<1x32634x1536xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x32634x1536xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x32634x1536xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 32634, 1], ["%arg4", 0, 1536, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 29655170}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x2088576x192xf32>) -> tensor<1x2088576x192xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x2088576x192xf32>) -> tensor<1x2088576x192xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x2088576x192xf32>) -> tensor<1x2088576x192xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x2088576x192xf32>) -> tensor<1x2088576x192xf32>\n  return %ret : tensor<1x2088576x192xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x2088576x192xf32>) -> tensor<1x2088576x192xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x2088576x192xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 2088576 {\n        affine.for %arg4 = 0 to 192 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x2088576x192xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x2088576x192xf32>\n    return %0 : tensor<1x2088576x192xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x2088576x192xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x2088576x192xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x2088576x192xf32>) -> tensor<1x2088576x192xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x2088576x192xf32>) -> tensor<1x2088576x192xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x2088576x192xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x2088576x192xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 2088576, 1], ["%arg4", 0, 192, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 232080470}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 666 {\n        affine.for %arg4 = 0 to 1000 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x666x1000xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %0 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 666, 1], ["%arg4", 0, 1000, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 332235}], ["linalg.batch_matmul ins(%1, %3 : tensor<1x666x2240xf32>, tensor<1x2240x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.batch_matmul ins(%1, %3 : tensor<1x666x2240xf32>, tensor<1x2240x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<1x666x2240xf32>, %3: tensor<1x2240x1000xf32>, %5: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.batch_matmul ins(%1, %3 : tensor<1x666x2240xf32>, tensor<1x2240x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<1x666x2240xf32>, %arg1: tensor<1x2240x1000xf32>, %arg2: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x2240x1000xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1x666x2240xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1x666x1000xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    memref.copy %2, %alloc : memref<1x666x1000xf32> to memref<1x666x1000xf32>\n    affine.for %arg3 = 0 to 1 {\n      affine.for %arg4 = 0 to 666 {\n        affine.for %arg5 = 0 to 1000 {\n          affine.for %arg6 = 0 to 2240 {\n            %4 = affine.load %1[%arg3, %arg4, %arg6] : memref<1x666x2240xf32>\n            %5 = affine.load %0[%arg3, %arg6, %arg5] : memref<1x2240x1000xf32>\n            %6 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n            %7 = arith.mulf %4, %5 : f32\n            %8 = arith.addf %6, %7 : f32\n            affine.store %8, %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %3 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<1x666x2240xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<1x666x2240xf32>) -> tensor<1x666x2240xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x2240x1000xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x2240x1000xf32>) -> tensor<1x2240x1000xf32>\n%tmp_5 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%5 = linalg.fill ins(%val : f32) outs(%tmp_5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.batch_matmul ins(%1, %3 : tensor<1x666x2240xf32>, tensor<1x2240x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 1, 1], ["%arg4", 0, 666, 1], ["%arg5", 0, 1000, 1], ["%arg6", 0, 2240, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg6"], ["%arg3", "%arg6", "%arg5"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}, "execution_time": 6102380243}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x522144x512xf32>) -> tensor<1x522144x512xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x522144x512xf32>) -> tensor<1x522144x512xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x522144x512xf32>) -> tensor<1x522144x512xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x522144x512xf32>) -> tensor<1x522144x512xf32>\n  return %ret : tensor<1x522144x512xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x522144x512xf32>) -> tensor<1x522144x512xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x522144x512xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 522144 {\n        affine.for %arg4 = 0 to 512 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x522144x512xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x522144x512xf32>\n    return %0 : tensor<1x522144x512xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x522144x512xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x522144x512xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x522144x512xf32>) -> tensor<1x522144x512xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x522144x512xf32>) -> tensor<1x522144x512xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x522144x512xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x522144x512xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 522144, 1], ["%arg4", 0, 512, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 154303320}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 666 {\n        affine.for %arg4 = 0 to 1000 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x666x1000xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %0 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 666, 1], ["%arg4", 0, 1000, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 331733}], ["linalg.batch_matmul ins(%1, %3 : tensor<1x666x912xf32>, tensor<1x912x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.batch_matmul ins(%1, %3 : tensor<1x666x912xf32>, tensor<1x912x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<1x666x912xf32>, %3: tensor<1x912x1000xf32>, %5: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.batch_matmul ins(%1, %3 : tensor<1x666x912xf32>, tensor<1x912x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<1x666x912xf32>, %arg1: tensor<1x912x1000xf32>, %arg2: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x912x1000xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1x666x912xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1x666x1000xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    memref.copy %2, %alloc : memref<1x666x1000xf32> to memref<1x666x1000xf32>\n    affine.for %arg3 = 0 to 1 {\n      affine.for %arg4 = 0 to 666 {\n        affine.for %arg5 = 0 to 1000 {\n          affine.for %arg6 = 0 to 912 {\n            %4 = affine.load %1[%arg3, %arg4, %arg6] : memref<1x666x912xf32>\n            %5 = affine.load %0[%arg3, %arg6, %arg5] : memref<1x912x1000xf32>\n            %6 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n            %7 = arith.mulf %4, %5 : f32\n            %8 = arith.addf %6, %7 : f32\n            affine.store %8, %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %3 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<1x666x912xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<1x666x912xf32>) -> tensor<1x666x912xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x912x1000xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x912x1000xf32>) -> tensor<1x912x1000xf32>\n%tmp_5 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%5 = linalg.fill ins(%val : f32) outs(%tmp_5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.batch_matmul ins(%1, %3 : tensor<1x666x912xf32>, tensor<1x912x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 1, 1], ["%arg4", 0, 666, 1], ["%arg5", 0, 1000, 1], ["%arg6", 0, 912, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg6"], ["%arg3", "%arg6", "%arg5"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}, "execution_time": 2304422558}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 666 {\n        affine.for %arg4 = 0 to 1000 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x666x1000xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %0 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 666, 1], ["%arg4", 0, 1000, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 336440}], ["linalg.batch_matmul ins(%1, %3 : tensor<1x666x440xf32>, tensor<1x440x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.batch_matmul ins(%1, %3 : tensor<1x666x440xf32>, tensor<1x440x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<1x666x440xf32>, %3: tensor<1x440x1000xf32>, %5: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.batch_matmul ins(%1, %3 : tensor<1x666x440xf32>, tensor<1x440x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<1x666x440xf32>, %arg1: tensor<1x440x1000xf32>, %arg2: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x440x1000xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1x666x440xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1x666x1000xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    memref.copy %2, %alloc : memref<1x666x1000xf32> to memref<1x666x1000xf32>\n    affine.for %arg3 = 0 to 1 {\n      affine.for %arg4 = 0 to 666 {\n        affine.for %arg5 = 0 to 1000 {\n          affine.for %arg6 = 0 to 440 {\n            %4 = affine.load %1[%arg3, %arg4, %arg6] : memref<1x666x440xf32>\n            %5 = affine.load %0[%arg3, %arg6, %arg5] : memref<1x440x1000xf32>\n            %6 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n            %7 = arith.mulf %4, %5 : f32\n            %8 = arith.addf %6, %7 : f32\n            affine.store %8, %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %3 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<1x666x440xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<1x666x440xf32>) -> tensor<1x666x440xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x440x1000xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x440x1000xf32>) -> tensor<1x440x1000xf32>\n%tmp_5 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%5 = linalg.fill ins(%val : f32) outs(%tmp_5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.batch_matmul ins(%1, %3 : tensor<1x666x440xf32>, tensor<1x440x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 1, 1], ["%arg4", 0, 666, 1], ["%arg5", 0, 1000, 1], ["%arg6", 0, 440, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg6"], ["%arg3", "%arg6", "%arg5"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}, "execution_time": 1084950067}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x32634x2048xf32>) -> tensor<1x32634x2048xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x32634x2048xf32>) -> tensor<1x32634x2048xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x32634x2048xf32>) -> tensor<1x32634x2048xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x32634x2048xf32>) -> tensor<1x32634x2048xf32>\n  return %ret : tensor<1x32634x2048xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x32634x2048xf32>) -> tensor<1x32634x2048xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x32634x2048xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 32634 {\n        affine.for %arg4 = 0 to 2048 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x32634x2048xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x32634x2048xf32>\n    return %0 : tensor<1x32634x2048xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x32634x2048xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x32634x2048xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x32634x2048xf32>) -> tensor<1x32634x2048xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x32634x2048xf32>) -> tensor<1x32634x2048xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x32634x2048xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x32634x2048xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 32634, 1], ["%arg4", 0, 2048, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 38548332}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x130536x3072xf32>) -> tensor<1x130536x3072xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x130536x3072xf32>) -> tensor<1x130536x3072xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x130536x3072xf32>) -> tensor<1x130536x3072xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x130536x3072xf32>) -> tensor<1x130536x3072xf32>\n  return %ret : tensor<1x130536x3072xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x130536x3072xf32>) -> tensor<1x130536x3072xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x130536x3072xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 130536 {\n        affine.for %arg4 = 0 to 3072 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x130536x3072xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x130536x3072xf32>\n    return %0 : tensor<1x130536x3072xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x130536x3072xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x130536x3072xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x130536x3072xf32>) -> tensor<1x130536x3072xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x130536x3072xf32>) -> tensor<1x130536x3072xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x130536x3072xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x130536x3072xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 130536, 1], ["%arg4", 0, 3072, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 230257458}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x2088576x256xf32>) -> tensor<1x2088576x256xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x2088576x256xf32>) -> tensor<1x2088576x256xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x2088576x256xf32>) -> tensor<1x2088576x256xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x2088576x256xf32>) -> tensor<1x2088576x256xf32>\n  return %ret : tensor<1x2088576x256xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x2088576x256xf32>) -> tensor<1x2088576x256xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x2088576x256xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 2088576 {\n        affine.for %arg4 = 0 to 256 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x2088576x256xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x2088576x256xf32>\n    return %0 : tensor<1x2088576x256xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x2088576x256xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x2088576x256xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x2088576x256xf32>) -> tensor<1x2088576x256xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x2088576x256xf32>) -> tensor<1x2088576x256xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x2088576x256xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x2088576x256xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 2088576, 1], ["%arg4", 0, 256, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 308408464}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x2088576x768xf32>) -> tensor<1x2088576x768xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x2088576x768xf32>) -> tensor<1x2088576x768xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x2088576x768xf32>) -> tensor<1x2088576x768xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x2088576x768xf32>) -> tensor<1x2088576x768xf32>\n  return %ret : tensor<1x2088576x768xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x2088576x768xf32>) -> tensor<1x2088576x768xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x2088576x768xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 2088576 {\n        affine.for %arg4 = 0 to 768 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x2088576x768xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x2088576x768xf32>\n    return %0 : tensor<1x2088576x768xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x2088576x768xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x2088576x768xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x2088576x768xf32>) -> tensor<1x2088576x768xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x2088576x768xf32>) -> tensor<1x2088576x768xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x2088576x768xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x2088576x768xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 2088576, 1], ["%arg4", 0, 768, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 930367160}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 666 {\n        affine.for %arg4 = 0 to 1000 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x666x1000xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %0 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 666, 1], ["%arg4", 0, 1000, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 331932}], ["linalg.batch_matmul ins(%1, %3 : tensor<1x666x3024xf32>, tensor<1x3024x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.batch_matmul ins(%1, %3 : tensor<1x666x3024xf32>, tensor<1x3024x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<1x666x3024xf32>, %3: tensor<1x3024x1000xf32>, %5: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.batch_matmul ins(%1, %3 : tensor<1x666x3024xf32>, tensor<1x3024x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<1x666x3024xf32>, %arg1: tensor<1x3024x1000xf32>, %arg2: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x3024x1000xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1x666x3024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1x666x1000xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    memref.copy %2, %alloc : memref<1x666x1000xf32> to memref<1x666x1000xf32>\n    affine.for %arg3 = 0 to 1 {\n      affine.for %arg4 = 0 to 666 {\n        affine.for %arg5 = 0 to 1000 {\n          affine.for %arg6 = 0 to 3024 {\n            %4 = affine.load %1[%arg3, %arg4, %arg6] : memref<1x666x3024xf32>\n            %5 = affine.load %0[%arg3, %arg6, %arg5] : memref<1x3024x1000xf32>\n            %6 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n            %7 = arith.mulf %4, %5 : f32\n            %8 = arith.addf %6, %7 : f32\n            affine.store %8, %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %3 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<1x666x3024xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<1x666x3024xf32>) -> tensor<1x666x3024xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x3024x1000xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x3024x1000xf32>) -> tensor<1x3024x1000xf32>\n%tmp_5 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%5 = linalg.fill ins(%val : f32) outs(%tmp_5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.batch_matmul ins(%1, %3 : tensor<1x666x3024xf32>, tensor<1x3024x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 1, 1], ["%arg4", 0, 666, 1], ["%arg5", 0, 1000, 1], ["%arg6", 0, 3024, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg6"], ["%arg3", "%arg6", "%arg5"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}, "execution_time": 8463945146}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x130536x512xf32>) -> tensor<1x130536x512xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x130536x512xf32>) -> tensor<1x130536x512xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x130536x512xf32>) -> tensor<1x130536x512xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x130536x512xf32>) -> tensor<1x130536x512xf32>\n  return %ret : tensor<1x130536x512xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x130536x512xf32>) -> tensor<1x130536x512xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x130536x512xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 130536 {\n        affine.for %arg4 = 0 to 512 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x130536x512xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x130536x512xf32>\n    return %0 : tensor<1x130536x512xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x130536x512xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x130536x512xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x130536x512xf32>) -> tensor<1x130536x512xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x130536x512xf32>) -> tensor<1x130536x512xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x130536x512xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x130536x512xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 130536, 1], ["%arg4", 0, 512, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 39406657}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x4096xf32>) -> tensor<1x666x4096xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x4096xf32>) -> tensor<1x666x4096xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x666x4096xf32>) -> tensor<1x666x4096xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x4096xf32>) -> tensor<1x666x4096xf32>\n  return %ret : tensor<1x666x4096xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x666x4096xf32>) -> tensor<1x666x4096xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x4096xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 666 {\n        affine.for %arg4 = 0 to 4096 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x666x4096xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x666x4096xf32>\n    return %0 : tensor<1x666x4096xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x4096xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x666x4096xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x666x4096xf32>) -> tensor<1x666x4096xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x4096xf32>) -> tensor<1x666x4096xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x4096xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x4096xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 666, 1], ["%arg4", 0, 4096, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 1380230}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 666 {\n        affine.for %arg4 = 0 to 1000 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x666x1000xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %0 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 666, 1], ["%arg4", 0, 1000, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 346249}], ["linalg.batch_matmul ins(%1, %3 : tensor<1x666x2520xf32>, tensor<1x2520x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.batch_matmul ins(%1, %3 : tensor<1x666x2520xf32>, tensor<1x2520x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<1x666x2520xf32>, %3: tensor<1x2520x1000xf32>, %5: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.batch_matmul ins(%1, %3 : tensor<1x666x2520xf32>, tensor<1x2520x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<1x666x2520xf32>, %arg1: tensor<1x2520x1000xf32>, %arg2: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x2520x1000xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1x666x2520xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1x666x1000xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    memref.copy %2, %alloc : memref<1x666x1000xf32> to memref<1x666x1000xf32>\n    affine.for %arg3 = 0 to 1 {\n      affine.for %arg4 = 0 to 666 {\n        affine.for %arg5 = 0 to 1000 {\n          affine.for %arg6 = 0 to 2520 {\n            %4 = affine.load %1[%arg3, %arg4, %arg6] : memref<1x666x2520xf32>\n            %5 = affine.load %0[%arg3, %arg6, %arg5] : memref<1x2520x1000xf32>\n            %6 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n            %7 = arith.mulf %4, %5 : f32\n            %8 = arith.addf %6, %7 : f32\n            affine.store %8, %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %3 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<1x666x2520xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<1x666x2520xf32>) -> tensor<1x666x2520xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x2520x1000xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x2520x1000xf32>) -> tensor<1x2520x1000xf32>\n%tmp_5 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%5 = linalg.fill ins(%val : f32) outs(%tmp_5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.batch_matmul ins(%1, %3 : tensor<1x666x2520xf32>, tensor<1x2520x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 1, 1], ["%arg4", 0, 666, 1], ["%arg5", 0, 1000, 1], ["%arg6", 0, 2520, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg6"], ["%arg3", "%arg6", "%arg5"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}, "execution_time": 6649878056}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x130536x1024xf32>) -> tensor<1x130536x1024xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x130536x1024xf32>) -> tensor<1x130536x1024xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x130536x1024xf32>) -> tensor<1x130536x1024xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x130536x1024xf32>) -> tensor<1x130536x1024xf32>\n  return %ret : tensor<1x130536x1024xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x130536x1024xf32>) -> tensor<1x130536x1024xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x130536x1024xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 130536 {\n        affine.for %arg4 = 0 to 1024 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x130536x1024xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x130536x1024xf32>\n    return %0 : tensor<1x130536x1024xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x130536x1024xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x130536x1024xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x130536x1024xf32>) -> tensor<1x130536x1024xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x130536x1024xf32>) -> tensor<1x130536x1024xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x130536x1024xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x130536x1024xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 130536, 1], ["%arg4", 0, 1024, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 77102334}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x130536x384xf32>) -> tensor<1x130536x384xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x130536x384xf32>) -> tensor<1x130536x384xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x130536x384xf32>) -> tensor<1x130536x384xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x130536x384xf32>) -> tensor<1x130536x384xf32>\n  return %ret : tensor<1x130536x384xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x130536x384xf32>) -> tensor<1x130536x384xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x130536x384xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 130536 {\n        affine.for %arg4 = 0 to 384 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x130536x384xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x130536x384xf32>\n    return %0 : tensor<1x130536x384xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x130536x384xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x130536x384xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x130536x384xf32>) -> tensor<1x130536x384xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x130536x384xf32>) -> tensor<1x130536x384xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x130536x384xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x130536x384xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 130536, 1], ["%arg4", 0, 384, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 29006469}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 666 {\n        affine.for %arg4 = 0 to 1000 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x666x1000xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %0 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 666, 1], ["%arg4", 0, 1000, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 331085}], ["linalg.batch_matmul ins(%1, %3 : tensor<1x666x368xf32>, tensor<1x368x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.batch_matmul ins(%1, %3 : tensor<1x666x368xf32>, tensor<1x368x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<1x666x368xf32>, %3: tensor<1x368x1000xf32>, %5: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.batch_matmul ins(%1, %3 : tensor<1x666x368xf32>, tensor<1x368x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<1x666x368xf32>, %arg1: tensor<1x368x1000xf32>, %arg2: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x368x1000xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1x666x368xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1x666x1000xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    memref.copy %2, %alloc : memref<1x666x1000xf32> to memref<1x666x1000xf32>\n    affine.for %arg3 = 0 to 1 {\n      affine.for %arg4 = 0 to 666 {\n        affine.for %arg5 = 0 to 1000 {\n          affine.for %arg6 = 0 to 368 {\n            %4 = affine.load %1[%arg3, %arg4, %arg6] : memref<1x666x368xf32>\n            %5 = affine.load %0[%arg3, %arg6, %arg5] : memref<1x368x1000xf32>\n            %6 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n            %7 = arith.mulf %4, %5 : f32\n            %8 = arith.addf %6, %7 : f32\n            affine.store %8, %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %3 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<1x666x368xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<1x666x368xf32>) -> tensor<1x666x368xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x368x1000xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x368x1000xf32>) -> tensor<1x368x1000xf32>\n%tmp_5 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%5 = linalg.fill ins(%val : f32) outs(%tmp_5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.batch_matmul ins(%1, %3 : tensor<1x666x368xf32>, tensor<1x368x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 1, 1], ["%arg4", 0, 666, 1], ["%arg5", 0, 1000, 1], ["%arg6", 0, 368, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg6"], ["%arg3", "%arg6", "%arg5"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}, "execution_time": 930495858}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x32634x3072xf32>) -> tensor<1x32634x3072xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x32634x3072xf32>) -> tensor<1x32634x3072xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x32634x3072xf32>) -> tensor<1x32634x3072xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x32634x3072xf32>) -> tensor<1x32634x3072xf32>\n  return %ret : tensor<1x32634x3072xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x32634x3072xf32>) -> tensor<1x32634x3072xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x32634x3072xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 32634 {\n        affine.for %arg4 = 0 to 3072 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x32634x3072xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x32634x3072xf32>\n    return %0 : tensor<1x32634x3072xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x32634x3072xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x32634x3072xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x32634x3072xf32>) -> tensor<1x32634x3072xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x32634x3072xf32>) -> tensor<1x32634x3072xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x32634x3072xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x32634x3072xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 32634, 1], ["%arg4", 0, 3072, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 56770217}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 666 {\n        affine.for %arg4 = 0 to 1000 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x666x1000xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %0 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 666, 1], ["%arg4", 0, 1000, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 340588}], ["linalg.batch_matmul ins(%1, %3 : tensor<1x666x1024xf32>, tensor<1x1024x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.batch_matmul ins(%1, %3 : tensor<1x666x1024xf32>, tensor<1x1024x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<1x666x1024xf32>, %3: tensor<1x1024x1000xf32>, %5: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.batch_matmul ins(%1, %3 : tensor<1x666x1024xf32>, tensor<1x1024x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<1x666x1024xf32>, %arg1: tensor<1x1024x1000xf32>, %arg2: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1024x1000xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1x666x1024xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1x666x1000xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    memref.copy %2, %alloc : memref<1x666x1000xf32> to memref<1x666x1000xf32>\n    affine.for %arg3 = 0 to 1 {\n      affine.for %arg4 = 0 to 666 {\n        affine.for %arg5 = 0 to 1000 {\n          affine.for %arg6 = 0 to 1024 {\n            %4 = affine.load %1[%arg3, %arg4, %arg6] : memref<1x666x1024xf32>\n            %5 = affine.load %0[%arg3, %arg6, %arg5] : memref<1x1024x1000xf32>\n            %6 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n            %7 = arith.mulf %4, %5 : f32\n            %8 = arith.addf %6, %7 : f32\n            affine.store %8, %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %3 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<1x666x1024xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<1x666x1024xf32>) -> tensor<1x666x1024xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1024x1000xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1024x1000xf32>) -> tensor<1x1024x1000xf32>\n%tmp_5 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%5 = linalg.fill ins(%val : f32) outs(%tmp_5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.batch_matmul ins(%1, %3 : tensor<1x666x1024xf32>, tensor<1x1024x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 1, 1], ["%arg4", 0, 666, 1], ["%arg5", 0, 1000, 1], ["%arg6", 0, 1024, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg6"], ["%arg3", "%arg6", "%arg5"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}, "execution_time": 2655454309}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 666 {\n        affine.for %arg4 = 0 to 1000 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x666x1000xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %0 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 666, 1], ["%arg4", 0, 1000, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 333766}], ["linalg.batch_matmul ins(%1, %3 : tensor<1x666x1536xf32>, tensor<1x1536x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.batch_matmul ins(%1, %3 : tensor<1x666x1536xf32>, tensor<1x1536x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<1x666x1536xf32>, %3: tensor<1x1536x1000xf32>, %5: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.batch_matmul ins(%1, %3 : tensor<1x666x1536xf32>, tensor<1x1536x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<1x666x1536xf32>, %arg1: tensor<1x1536x1000xf32>, %arg2: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1536x1000xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1x666x1536xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1x666x1000xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    memref.copy %2, %alloc : memref<1x666x1000xf32> to memref<1x666x1000xf32>\n    affine.for %arg3 = 0 to 1 {\n      affine.for %arg4 = 0 to 666 {\n        affine.for %arg5 = 0 to 1000 {\n          affine.for %arg6 = 0 to 1536 {\n            %4 = affine.load %1[%arg3, %arg4, %arg6] : memref<1x666x1536xf32>\n            %5 = affine.load %0[%arg3, %arg6, %arg5] : memref<1x1536x1000xf32>\n            %6 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n            %7 = arith.mulf %4, %5 : f32\n            %8 = arith.addf %6, %7 : f32\n            affine.store %8, %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %3 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<1x666x1536xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<1x666x1536xf32>) -> tensor<1x666x1536xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1536x1000xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1536x1000xf32>) -> tensor<1x1536x1000xf32>\n%tmp_5 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%5 = linalg.fill ins(%val : f32) outs(%tmp_5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.batch_matmul ins(%1, %3 : tensor<1x666x1536xf32>, tensor<1x1536x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 1, 1], ["%arg4", 0, 666, 1], ["%arg5", 0, 1000, 1], ["%arg6", 0, 1536, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg6"], ["%arg3", "%arg6", "%arg5"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}, "execution_time": 4096351838}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x522144x2048xf32>) -> tensor<1x522144x2048xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x522144x2048xf32>) -> tensor<1x522144x2048xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x522144x2048xf32>) -> tensor<1x522144x2048xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x522144x2048xf32>) -> tensor<1x522144x2048xf32>\n  return %ret : tensor<1x522144x2048xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x522144x2048xf32>) -> tensor<1x522144x2048xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x522144x2048xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 522144 {\n        affine.for %arg4 = 0 to 2048 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x522144x2048xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x522144x2048xf32>\n    return %0 : tensor<1x522144x2048xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x522144x2048xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x522144x2048xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x522144x2048xf32>) -> tensor<1x522144x2048xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x522144x2048xf32>) -> tensor<1x522144x2048xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x522144x2048xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x522144x2048xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 522144, 1], ["%arg4", 0, 2048, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 617020674}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x130536x4096xf32>) -> tensor<1x130536x4096xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x130536x4096xf32>) -> tensor<1x130536x4096xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x130536x4096xf32>) -> tensor<1x130536x4096xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x130536x4096xf32>) -> tensor<1x130536x4096xf32>\n  return %ret : tensor<1x130536x4096xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x130536x4096xf32>) -> tensor<1x130536x4096xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x130536x4096xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 130536 {\n        affine.for %arg4 = 0 to 4096 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x130536x4096xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x130536x4096xf32>\n    return %0 : tensor<1x130536x4096xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x130536x4096xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x130536x4096xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x130536x4096xf32>) -> tensor<1x130536x4096xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x130536x4096xf32>) -> tensor<1x130536x4096xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x130536x4096xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x130536x4096xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 130536, 1], ["%arg4", 0, 4096, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 307742775}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 666 {\n        affine.for %arg4 = 0 to 1000 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x666x1000xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %0 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 666, 1], ["%arg4", 0, 1000, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 339784}], ["linalg.batch_matmul ins(%1, %3 : tensor<1x666x3712xf32>, tensor<1x3712x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.batch_matmul ins(%1, %3 : tensor<1x666x3712xf32>, tensor<1x3712x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<1x666x3712xf32>, %3: tensor<1x3712x1000xf32>, %5: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.batch_matmul ins(%1, %3 : tensor<1x666x3712xf32>, tensor<1x3712x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<1x666x3712xf32>, %arg1: tensor<1x3712x1000xf32>, %arg2: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x3712x1000xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1x666x3712xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1x666x1000xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    memref.copy %2, %alloc : memref<1x666x1000xf32> to memref<1x666x1000xf32>\n    affine.for %arg3 = 0 to 1 {\n      affine.for %arg4 = 0 to 666 {\n        affine.for %arg5 = 0 to 1000 {\n          affine.for %arg6 = 0 to 3712 {\n            %4 = affine.load %1[%arg3, %arg4, %arg6] : memref<1x666x3712xf32>\n            %5 = affine.load %0[%arg3, %arg6, %arg5] : memref<1x3712x1000xf32>\n            %6 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n            %7 = arith.mulf %4, %5 : f32\n            %8 = arith.addf %6, %7 : f32\n            affine.store %8, %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %3 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<1x666x3712xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<1x666x3712xf32>) -> tensor<1x666x3712xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x3712x1000xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x3712x1000xf32>) -> tensor<1x3712x1000xf32>\n%tmp_5 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%5 = linalg.fill ins(%val : f32) outs(%tmp_5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.batch_matmul ins(%1, %3 : tensor<1x666x3712xf32>, tensor<1x3712x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 1, 1], ["%arg4", 0, 666, 1], ["%arg5", 0, 1000, 1], ["%arg6", 0, 3712, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg6"], ["%arg3", "%arg6", "%arg5"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}, "execution_time": 10756571045}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 666 {\n        affine.for %arg4 = 0 to 1000 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x666x1000xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %0 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 666, 1], ["%arg4", 0, 1000, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 346184}], ["linalg.batch_matmul ins(%1, %3 : tensor<1x666x768xf32>, tensor<1x768x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.batch_matmul ins(%1, %3 : tensor<1x666x768xf32>, tensor<1x768x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<1x666x768xf32>, %3: tensor<1x768x1000xf32>, %5: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.batch_matmul ins(%1, %3 : tensor<1x666x768xf32>, tensor<1x768x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<1x666x768xf32>, %arg1: tensor<1x768x1000xf32>, %arg2: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x768x1000xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1x666x768xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1x666x1000xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    memref.copy %2, %alloc : memref<1x666x1000xf32> to memref<1x666x1000xf32>\n    affine.for %arg3 = 0 to 1 {\n      affine.for %arg4 = 0 to 666 {\n        affine.for %arg5 = 0 to 1000 {\n          affine.for %arg6 = 0 to 768 {\n            %4 = affine.load %1[%arg3, %arg4, %arg6] : memref<1x666x768xf32>\n            %5 = affine.load %0[%arg3, %arg6, %arg5] : memref<1x768x1000xf32>\n            %6 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n            %7 = arith.mulf %4, %5 : f32\n            %8 = arith.addf %6, %7 : f32\n            affine.store %8, %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %3 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<1x666x768xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<1x666x768xf32>) -> tensor<1x666x768xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x768x1000xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x768x1000xf32>) -> tensor<1x768x1000xf32>\n%tmp_5 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%5 = linalg.fill ins(%val : f32) outs(%tmp_5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.batch_matmul ins(%1, %3 : tensor<1x666x768xf32>, tensor<1x768x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 1, 1], ["%arg4", 0, 666, 1], ["%arg5", 0, 1000, 1], ["%arg6", 0, 768, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg6"], ["%arg3", "%arg6", "%arg5"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}, "execution_time": 1991808456}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x32634x4096xf32>) -> tensor<1x32634x4096xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x32634x4096xf32>) -> tensor<1x32634x4096xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x32634x4096xf32>) -> tensor<1x32634x4096xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x32634x4096xf32>) -> tensor<1x32634x4096xf32>\n  return %ret : tensor<1x32634x4096xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x32634x4096xf32>) -> tensor<1x32634x4096xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x32634x4096xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 32634 {\n        affine.for %arg4 = 0 to 4096 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x32634x4096xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x32634x4096xf32>\n    return %0 : tensor<1x32634x4096xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x32634x4096xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x32634x4096xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x32634x4096xf32>) -> tensor<1x32634x4096xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x32634x4096xf32>) -> tensor<1x32634x4096xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x32634x4096xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x32634x4096xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 32634, 1], ["%arg4", 0, 4096, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 77445693}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 666 {\n        affine.for %arg4 = 0 to 1000 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x666x1000xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %0 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 666, 1], ["%arg4", 0, 1000, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 339517}], ["linalg.batch_matmul ins(%1, %3 : tensor<1x666x1512xf32>, tensor<1x1512x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.batch_matmul ins(%1, %3 : tensor<1x666x1512xf32>, tensor<1x1512x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<1x666x1512xf32>, %3: tensor<1x1512x1000xf32>, %5: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.batch_matmul ins(%1, %3 : tensor<1x666x1512xf32>, tensor<1x1512x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<1x666x1512xf32>, %arg1: tensor<1x1512x1000xf32>, %arg2: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1512x1000xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1x666x1512xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1x666x1000xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    memref.copy %2, %alloc : memref<1x666x1000xf32> to memref<1x666x1000xf32>\n    affine.for %arg3 = 0 to 1 {\n      affine.for %arg4 = 0 to 666 {\n        affine.for %arg5 = 0 to 1000 {\n          affine.for %arg6 = 0 to 1512 {\n            %4 = affine.load %1[%arg3, %arg4, %arg6] : memref<1x666x1512xf32>\n            %5 = affine.load %0[%arg3, %arg6, %arg5] : memref<1x1512x1000xf32>\n            %6 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n            %7 = arith.mulf %4, %5 : f32\n            %8 = arith.addf %6, %7 : f32\n            affine.store %8, %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %3 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<1x666x1512xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<1x666x1512xf32>) -> tensor<1x666x1512xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1512x1000xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1512x1000xf32>) -> tensor<1x1512x1000xf32>\n%tmp_5 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%5 = linalg.fill ins(%val : f32) outs(%tmp_5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.batch_matmul ins(%1, %3 : tensor<1x666x1512xf32>, tensor<1x1512x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 1, 1], ["%arg4", 0, 666, 1], ["%arg5", 0, 1000, 1], ["%arg6", 0, 1512, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg6"], ["%arg3", "%arg6", "%arg5"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}, "execution_time": 4222744384}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x522144x384xf32>) -> tensor<1x522144x384xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x522144x384xf32>) -> tensor<1x522144x384xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x522144x384xf32>) -> tensor<1x522144x384xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x522144x384xf32>) -> tensor<1x522144x384xf32>\n  return %ret : tensor<1x522144x384xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x522144x384xf32>) -> tensor<1x522144x384xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x522144x384xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 522144 {\n        affine.for %arg4 = 0 to 384 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x522144x384xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x522144x384xf32>\n    return %0 : tensor<1x522144x384xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x522144x384xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x522144x384xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x522144x384xf32>) -> tensor<1x522144x384xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x522144x384xf32>) -> tensor<1x522144x384xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x522144x384xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x522144x384xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 522144, 1], ["%arg4", 0, 384, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 115887752}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 666 {\n        affine.for %arg4 = 0 to 1000 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x666x1000xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %0 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 666, 1], ["%arg4", 0, 1000, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 352934}], ["linalg.batch_matmul ins(%1, %3 : tensor<1x666x1008xf32>, tensor<1x1008x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.batch_matmul ins(%1, %3 : tensor<1x666x1008xf32>, tensor<1x1008x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<1x666x1008xf32>, %3: tensor<1x1008x1000xf32>, %5: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.batch_matmul ins(%1, %3 : tensor<1x666x1008xf32>, tensor<1x1008x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<1x666x1008xf32>, %arg1: tensor<1x1008x1000xf32>, %arg2: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x1008x1000xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1x666x1008xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1x666x1000xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    memref.copy %2, %alloc : memref<1x666x1000xf32> to memref<1x666x1000xf32>\n    affine.for %arg3 = 0 to 1 {\n      affine.for %arg4 = 0 to 666 {\n        affine.for %arg5 = 0 to 1000 {\n          affine.for %arg6 = 0 to 1008 {\n            %4 = affine.load %1[%arg3, %arg4, %arg6] : memref<1x666x1008xf32>\n            %5 = affine.load %0[%arg3, %arg6, %arg5] : memref<1x1008x1000xf32>\n            %6 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n            %7 = arith.mulf %4, %5 : f32\n            %8 = arith.addf %6, %7 : f32\n            affine.store %8, %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %3 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<1x666x1008xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<1x666x1008xf32>) -> tensor<1x666x1008xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x1008x1000xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x1008x1000xf32>) -> tensor<1x1008x1000xf32>\n%tmp_5 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%5 = linalg.fill ins(%val : f32) outs(%tmp_5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.batch_matmul ins(%1, %3 : tensor<1x666x1008xf32>, tensor<1x1008x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 1, 1], ["%arg4", 0, 666, 1], ["%arg5", 0, 1000, 1], ["%arg6", 0, 1008, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg6"], ["%arg3", "%arg6", "%arg5"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}, "execution_time": 2757481140}], ["linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%cst_0: f32, %4: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: f32, %arg1: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    affine.for %arg2 = 0 to 1 {\n      affine.for %arg3 = 0 to 666 {\n        affine.for %arg4 = 0 to 1000 {\n          affine.store %arg0, %alloc[%arg2, %arg3, %arg4] : memref<1x666x1000xf32>\n        }\n      }\n    }\n    %0 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %0 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%cst_0 = arith.constant 2.00000e+00 : f32\n%tmp_4 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%4 = linalg.fill ins(%val : f32) outs(%tmp_4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg2", 0, 1, 1], ["%arg3", 0, 666, 1], ["%arg4", 0, 1000, 1]], "op_count": {"+": 0, "-": 0, "*": 0, "/": 0, "exp": 0}, "load_data": [], "store_data": ["%arg2", "%arg3", "%arg4"]}, "execution_time": 352676}], ["linalg.batch_matmul ins(%1, %3 : tensor<1x666x4096xf32>, tensor<1x4096x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", {"operation": "linalg.batch_matmul ins(%1, %3 : tensor<1x666x4096xf32>, tensor<1x4096x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>", "wrapped_operation": "func.func @func_call(%1: tensor<1x666x4096xf32>, %3: tensor<1x4096x1000xf32>, %5: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n  %ret = linalg.batch_matmul ins(%1, %3 : tensor<1x666x4096xf32>, tensor<1x4096x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n  return %ret : tensor<1x666x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<1x666x4096xf32>, %arg1: tensor<1x4096x1000xf32>, %arg2: tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1x4096x1000xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1x666x4096xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1x666x1000xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1x666x1000xf32>\n    memref.copy %2, %alloc : memref<1x666x1000xf32> to memref<1x666x1000xf32>\n    affine.for %arg3 = 0 to 1 {\n      affine.for %arg4 = 0 to 666 {\n        affine.for %arg5 = 0 to 1000 {\n          affine.for %arg6 = 0 to 4096 {\n            %4 = affine.load %1[%arg3, %arg4, %arg6] : memref<1x666x4096xf32>\n            %5 = affine.load %0[%arg3, %arg6, %arg5] : memref<1x4096x1000xf32>\n            %6 = affine.load %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n            %7 = arith.mulf %4, %5 : f32\n            %8 = arith.addf %6, %7 : f32\n            affine.store %8, %alloc[%arg3, %arg4, %arg5] : memref<1x666x1000xf32>\n          }\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1x666x1000xf32>\n    return %3 : tensor<1x666x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1x666x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%tmp_1 = bufferization.alloc_tensor() : tensor<1x666x4096xf32>\n%1 = linalg.fill ins(%val : f32) outs(%tmp_1 : tensor<1x666x4096xf32>) -> tensor<1x666x4096xf32>\n%tmp_3 = bufferization.alloc_tensor() : tensor<1x4096x1000xf32>\n%3 = linalg.fill ins(%val : f32) outs(%tmp_3 : tensor<1x4096x1000xf32>) -> tensor<1x4096x1000xf32>\n%tmp_5 = bufferization.alloc_tensor() : tensor<1x666x1000xf32>\n%5 = linalg.fill ins(%val : f32) outs(%tmp_5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%return_arg = linalg.batch_matmul ins(%1, %3 : tensor<1x666x4096xf32>, tensor<1x4096x1000xf32>) outs(%5 : tensor<1x666x1000xf32>) -> tensor<1x666x1000xf32>\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %return_arg : tensor<1x666x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1x666x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 1, 1], ["%arg4", 0, 666, 1], ["%arg5", 0, 1000, 1], ["%arg6", 0, 4096, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg4", "%arg6"], ["%arg3", "%arg6", "%arg5"], ["%arg3", "%arg4", "%arg5"]], "store_data": ["%arg3", "%arg4", "%arg5"]}, "execution_time": 14852865606}]]