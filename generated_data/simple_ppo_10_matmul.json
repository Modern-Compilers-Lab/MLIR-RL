[["linalg.matmul ins(%arg0, %arg1: tensor<500x500xf32>, tensor<500x500xf32>) outs(%arg2: tensor<500x500xf32>) -> tensor<500x500xf32>", {"operation": "linalg.matmul ins(%arg0, %arg1: tensor<500x500xf32>, tensor<500x500xf32>) outs(%arg2: tensor<500x500xf32>) -> tensor<500x500xf32>", "wrapped_operation": "func.func @func_call(%arg0: tensor<500x500xf32>, %arg1: tensor<500x500xf32>, %arg2: tensor<500x500xf32>) -> tensor<500x500xf32> {\n  %ret = linalg.matmul ins(%arg0, %arg1: tensor<500x500xf32>, tensor<500x500xf32>) outs(%arg2: tensor<500x500xf32>) -> tensor<500x500xf32>\n  return %ret : tensor<500x500xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<500x500xf32>, %arg1: tensor<500x500xf32>, %arg2: tensor<500x500xf32>) -> tensor<500x500xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<500x500xf32>\n    %1 = bufferization.to_memref %arg0 : memref<500x500xf32>\n    %2 = bufferization.to_memref %arg2 : memref<500x500xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<500x500xf32>\n    memref.copy %2, %alloc : memref<500x500xf32> to memref<500x500xf32>\n    affine.for %arg3 = 0 to 500 {\n      affine.for %arg4 = 0 to 500 {\n        affine.for %arg5 = 0 to 500 {\n          %4 = affine.load %1[%arg3, %arg5] : memref<500x500xf32>\n          %5 = affine.load %0[%arg5, %arg4] : memref<500x500xf32>\n          %6 = affine.load %alloc[%arg3, %arg4] : memref<500x500xf32>\n          %7 = arith.mulf %4, %5 : f32\n          %8 = arith.addf %6, %7 : f32\n          affine.store %8, %alloc[%arg3, %arg4] : memref<500x500xf32>\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<500x500xf32>\n    return %3 : tensor<500x500xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<500x500xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%out = bufferization.alloc_tensor() : tensor<500x500xf32>\n%A = linalg.fill ins(%val : f32) outs(%out : tensor<500x500xf32>) -> tensor<500x500xf32>\n%out1 = bufferization.alloc_tensor() : tensor<500x500xf32>\n%B = linalg.fill ins(%val : f32) outs(%out1 : tensor<500x500xf32>) -> tensor<500x500xf32>\n%out2 = bufferization.alloc_tensor() : tensor<500x500xf32>\n%C = linalg.fill ins(%zero : f32) outs(%out2 : tensor<500x500xf32>) -> tensor<500x500xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%D = linalg.matmul ins(%A, %B: tensor<500x500xf32>, tensor<500x500xf32>) outs(%C: tensor<500x500xf32>) -> tensor<500x500xf32>\n\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %D : tensor<500x500xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<500x500xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 500, 1], ["%arg4", 0, 500, 1], ["%arg5", 0, 500, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg5"], ["%arg5", "%arg4"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}, "execution_time": 462136215.0}], ["linalg.matmul ins(%arg0, %arg1: tensor<1000x1000xf32>, tensor<1000x1000xf32>) outs(%arg2: tensor<1000x1000xf32>) -> tensor<1000x1000xf32>", {"operation": "linalg.matmul ins(%arg0, %arg1: tensor<1000x1000xf32>, tensor<1000x1000xf32>) outs(%arg2: tensor<1000x1000xf32>) -> tensor<1000x1000xf32>", "wrapped_operation": "func.func @func_call(%arg0: tensor<1000x1000xf32>, %arg1: tensor<1000x1000xf32>, %arg2: tensor<1000x1000xf32>) -> tensor<1000x1000xf32> {\n  %ret = linalg.matmul ins(%arg0, %arg1: tensor<1000x1000xf32>, tensor<1000x1000xf32>) outs(%arg2: tensor<1000x1000xf32>) -> tensor<1000x1000xf32>\n  return %ret : tensor<1000x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<1000x1000xf32>, %arg1: tensor<1000x1000xf32>, %arg2: tensor<1000x1000xf32>) -> tensor<1000x1000xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1000x1000xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1000x1000xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1000x1000xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1000x1000xf32>\n    memref.copy %2, %alloc : memref<1000x1000xf32> to memref<1000x1000xf32>\n    affine.for %arg3 = 0 to 1000 {\n      affine.for %arg4 = 0 to 1000 {\n        affine.for %arg5 = 0 to 1000 {\n          %4 = affine.load %1[%arg3, %arg5] : memref<1000x1000xf32>\n          %5 = affine.load %0[%arg5, %arg4] : memref<1000x1000xf32>\n          %6 = affine.load %alloc[%arg3, %arg4] : memref<1000x1000xf32>\n          %7 = arith.mulf %4, %5 : f32\n          %8 = arith.addf %6, %7 : f32\n          affine.store %8, %alloc[%arg3, %arg4] : memref<1000x1000xf32>\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1000x1000xf32>\n    return %3 : tensor<1000x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1000x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%out = bufferization.alloc_tensor() : tensor<1000x1000xf32>\n%A = linalg.fill ins(%val : f32) outs(%out : tensor<1000x1000xf32>) -> tensor<1000x1000xf32>\n%out1 = bufferization.alloc_tensor() : tensor<1000x1000xf32>\n%B = linalg.fill ins(%val : f32) outs(%out1 : tensor<1000x1000xf32>) -> tensor<1000x1000xf32>\n%out2 = bufferization.alloc_tensor() : tensor<1000x1000xf32>\n%C = linalg.fill ins(%zero : f32) outs(%out2 : tensor<1000x1000xf32>) -> tensor<1000x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%D = linalg.matmul ins(%A, %B: tensor<1000x1000xf32>, tensor<1000x1000xf32>) outs(%C: tensor<1000x1000xf32>) -> tensor<1000x1000xf32>\n\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %D : tensor<1000x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1000x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 1000, 1], ["%arg4", 0, 1000, 1], ["%arg5", 0, 1000, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg5"], ["%arg5", "%arg4"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}, "execution_time": 3759466851.5}], ["linalg.matmul ins(%arg0, %arg1: tensor<600x1200xf32>, tensor<1200x600xf32>) outs(%arg2: tensor<600x600xf32>) -> tensor<600x600xf32>", {"operation": "linalg.matmul ins(%arg0, %arg1: tensor<600x1200xf32>, tensor<1200x600xf32>) outs(%arg2: tensor<600x600xf32>) -> tensor<600x600xf32>", "wrapped_operation": "func.func @func_call(%arg0: tensor<600x1200xf32>, %arg1: tensor<1200x600xf32>, %arg2: tensor<600x600xf32>) -> tensor<600x600xf32> {\n  %ret = linalg.matmul ins(%arg0, %arg1: tensor<600x1200xf32>, tensor<1200x600xf32>) outs(%arg2: tensor<600x600xf32>) -> tensor<600x600xf32>\n  return %ret : tensor<600x600xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<600x1200xf32>, %arg1: tensor<1200x600xf32>, %arg2: tensor<600x600xf32>) -> tensor<600x600xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1200x600xf32>\n    %1 = bufferization.to_memref %arg0 : memref<600x1200xf32>\n    %2 = bufferization.to_memref %arg2 : memref<600x600xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<600x600xf32>\n    memref.copy %2, %alloc : memref<600x600xf32> to memref<600x600xf32>\n    affine.for %arg3 = 0 to 600 {\n      affine.for %arg4 = 0 to 600 {\n        affine.for %arg5 = 0 to 1200 {\n          %4 = affine.load %1[%arg3, %arg5] : memref<600x1200xf32>\n          %5 = affine.load %0[%arg5, %arg4] : memref<1200x600xf32>\n          %6 = affine.load %alloc[%arg3, %arg4] : memref<600x600xf32>\n          %7 = arith.mulf %4, %5 : f32\n          %8 = arith.addf %6, %7 : f32\n          affine.store %8, %alloc[%arg3, %arg4] : memref<600x600xf32>\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<600x600xf32>\n    return %3 : tensor<600x600xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<600x600xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%out = bufferization.alloc_tensor() : tensor<600x1200xf32>\n%A = linalg.fill ins(%val : f32) outs(%out : tensor<600x1200xf32>) -> tensor<600x1200xf32>\n%out1 = bufferization.alloc_tensor() : tensor<1200x600xf32>\n%B = linalg.fill ins(%val : f32) outs(%out1 : tensor<1200x600xf32>) -> tensor<1200x600xf32>\n%out2 = bufferization.alloc_tensor() : tensor<600x600xf32>\n%C = linalg.fill ins(%zero : f32) outs(%out2 : tensor<600x600xf32>) -> tensor<600x600xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%D = linalg.matmul ins(%A, %B: tensor<600x1200xf32>, tensor<1200x600xf32>) outs(%C: tensor<600x600xf32>) -> tensor<600x600xf32>\n\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %D : tensor<600x600xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<600x600xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 600, 1], ["%arg4", 0, 600, 1], ["%arg5", 0, 1200, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg5"], ["%arg5", "%arg4"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}, "execution_time": 1625773612.5}], ["linalg.matmul ins(%arg0, %arg1: tensor<1300x500xf32>, tensor<500x1300xf32>) outs(%arg2: tensor<1300x1300xf32>) -> tensor<1300x1300xf32>", {"operation": "linalg.matmul ins(%arg0, %arg1: tensor<1300x500xf32>, tensor<500x1300xf32>) outs(%arg2: tensor<1300x1300xf32>) -> tensor<1300x1300xf32>", "wrapped_operation": "func.func @func_call(%arg0: tensor<1300x500xf32>, %arg1: tensor<500x1300xf32>, %arg2: tensor<1300x1300xf32>) -> tensor<1300x1300xf32> {\n  %ret = linalg.matmul ins(%arg0, %arg1: tensor<1300x500xf32>, tensor<500x1300xf32>) outs(%arg2: tensor<1300x1300xf32>) -> tensor<1300x1300xf32>\n  return %ret : tensor<1300x1300xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<1300x500xf32>, %arg1: tensor<500x1300xf32>, %arg2: tensor<1300x1300xf32>) -> tensor<1300x1300xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<500x1300xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1300x500xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1300x1300xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1300x1300xf32>\n    memref.copy %2, %alloc : memref<1300x1300xf32> to memref<1300x1300xf32>\n    affine.for %arg3 = 0 to 1300 {\n      affine.for %arg4 = 0 to 1300 {\n        affine.for %arg5 = 0 to 500 {\n          %4 = affine.load %1[%arg3, %arg5] : memref<1300x500xf32>\n          %5 = affine.load %0[%arg5, %arg4] : memref<500x1300xf32>\n          %6 = affine.load %alloc[%arg3, %arg4] : memref<1300x1300xf32>\n          %7 = arith.mulf %4, %5 : f32\n          %8 = arith.addf %6, %7 : f32\n          affine.store %8, %alloc[%arg3, %arg4] : memref<1300x1300xf32>\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1300x1300xf32>\n    return %3 : tensor<1300x1300xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1300x1300xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%out = bufferization.alloc_tensor() : tensor<1300x500xf32>\n%A = linalg.fill ins(%val : f32) outs(%out : tensor<1300x500xf32>) -> tensor<1300x500xf32>\n%out1 = bufferization.alloc_tensor() : tensor<500x1300xf32>\n%B = linalg.fill ins(%val : f32) outs(%out1 : tensor<500x1300xf32>) -> tensor<500x1300xf32>\n%out2 = bufferization.alloc_tensor() : tensor<1300x1300xf32>\n%C = linalg.fill ins(%zero : f32) outs(%out2 : tensor<1300x1300xf32>) -> tensor<1300x1300xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%D = linalg.matmul ins(%A, %B: tensor<1300x500xf32>, tensor<500x1300xf32>) outs(%C: tensor<1300x1300xf32>) -> tensor<1300x1300xf32>\n\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %D : tensor<1300x1300xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1300x1300xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 1300, 1], ["%arg4", 0, 1300, 1], ["%arg5", 0, 500, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg5"], ["%arg5", "%arg4"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}, "execution_time": 3125618350.0}], ["linalg.matmul ins(%arg0, %arg1: tensor<800x600xf32>, tensor<600x800xf32>) outs(%arg2: tensor<800x800xf32>) -> tensor<800x800xf32>", {"operation": "linalg.matmul ins(%arg0, %arg1: tensor<800x600xf32>, tensor<600x800xf32>) outs(%arg2: tensor<800x800xf32>) -> tensor<800x800xf32>", "wrapped_operation": "func.func @func_call(%arg0: tensor<800x600xf32>, %arg1: tensor<600x800xf32>, %arg2: tensor<800x800xf32>) -> tensor<800x800xf32> {\n  %ret = linalg.matmul ins(%arg0, %arg1: tensor<800x600xf32>, tensor<600x800xf32>) outs(%arg2: tensor<800x800xf32>) -> tensor<800x800xf32>\n  return %ret : tensor<800x800xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<800x600xf32>, %arg1: tensor<600x800xf32>, %arg2: tensor<800x800xf32>) -> tensor<800x800xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<600x800xf32>\n    %1 = bufferization.to_memref %arg0 : memref<800x600xf32>\n    %2 = bufferization.to_memref %arg2 : memref<800x800xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<800x800xf32>\n    memref.copy %2, %alloc : memref<800x800xf32> to memref<800x800xf32>\n    affine.for %arg3 = 0 to 800 {\n      affine.for %arg4 = 0 to 800 {\n        affine.for %arg5 = 0 to 600 {\n          %4 = affine.load %1[%arg3, %arg5] : memref<800x600xf32>\n          %5 = affine.load %0[%arg5, %arg4] : memref<600x800xf32>\n          %6 = affine.load %alloc[%arg3, %arg4] : memref<800x800xf32>\n          %7 = arith.mulf %4, %5 : f32\n          %8 = arith.addf %6, %7 : f32\n          affine.store %8, %alloc[%arg3, %arg4] : memref<800x800xf32>\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<800x800xf32>\n    return %3 : tensor<800x800xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<800x800xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%out = bufferization.alloc_tensor() : tensor<800x600xf32>\n%A = linalg.fill ins(%val : f32) outs(%out : tensor<800x600xf32>) -> tensor<800x600xf32>\n%out1 = bufferization.alloc_tensor() : tensor<600x800xf32>\n%B = linalg.fill ins(%val : f32) outs(%out1 : tensor<600x800xf32>) -> tensor<600x800xf32>\n%out2 = bufferization.alloc_tensor() : tensor<800x800xf32>\n%C = linalg.fill ins(%zero : f32) outs(%out2 : tensor<800x800xf32>) -> tensor<800x800xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%D = linalg.matmul ins(%A, %B: tensor<800x600xf32>, tensor<600x800xf32>) outs(%C: tensor<800x800xf32>) -> tensor<800x800xf32>\n\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %D : tensor<800x800xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<800x800xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 800, 1], ["%arg4", 0, 800, 1], ["%arg5", 0, 600, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg5"], ["%arg5", "%arg4"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}, "execution_time": 1429887369.5}], ["linalg.matmul ins(%arg0, %arg1: tensor<1000x200xf32>, tensor<200x1500xf32>) outs(%arg2: tensor<1000x1500xf32>) -> tensor<1000x1500xf32>", {"operation": "linalg.matmul ins(%arg0, %arg1: tensor<1000x200xf32>, tensor<200x1500xf32>) outs(%arg2: tensor<1000x1500xf32>) -> tensor<1000x1500xf32>", "wrapped_operation": "func.func @func_call(%arg0: tensor<1000x200xf32>, %arg1: tensor<200x1500xf32>, %arg2: tensor<1000x1500xf32>) -> tensor<1000x1500xf32> {\n  %ret = linalg.matmul ins(%arg0, %arg1: tensor<1000x200xf32>, tensor<200x1500xf32>) outs(%arg2: tensor<1000x1500xf32>) -> tensor<1000x1500xf32>\n  return %ret : tensor<1000x1500xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<1000x200xf32>, %arg1: tensor<200x1500xf32>, %arg2: tensor<1000x1500xf32>) -> tensor<1000x1500xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<200x1500xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1000x200xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1000x1500xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1000x1500xf32>\n    memref.copy %2, %alloc : memref<1000x1500xf32> to memref<1000x1500xf32>\n    affine.for %arg3 = 0 to 1000 {\n      affine.for %arg4 = 0 to 1500 {\n        affine.for %arg5 = 0 to 200 {\n          %4 = affine.load %1[%arg3, %arg5] : memref<1000x200xf32>\n          %5 = affine.load %0[%arg5, %arg4] : memref<200x1500xf32>\n          %6 = affine.load %alloc[%arg3, %arg4] : memref<1000x1500xf32>\n          %7 = arith.mulf %4, %5 : f32\n          %8 = arith.addf %6, %7 : f32\n          affine.store %8, %alloc[%arg3, %arg4] : memref<1000x1500xf32>\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1000x1500xf32>\n    return %3 : tensor<1000x1500xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1000x1500xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%out = bufferization.alloc_tensor() : tensor<1000x200xf32>\n%A = linalg.fill ins(%val : f32) outs(%out : tensor<1000x200xf32>) -> tensor<1000x200xf32>\n%out1 = bufferization.alloc_tensor() : tensor<200x1500xf32>\n%B = linalg.fill ins(%val : f32) outs(%out1 : tensor<200x1500xf32>) -> tensor<200x1500xf32>\n%out2 = bufferization.alloc_tensor() : tensor<1000x1500xf32>\n%C = linalg.fill ins(%zero : f32) outs(%out2 : tensor<1000x1500xf32>) -> tensor<1000x1500xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%D = linalg.matmul ins(%A, %B: tensor<1000x200xf32>, tensor<200x1500xf32>) outs(%C: tensor<1000x1500xf32>) -> tensor<1000x1500xf32>\n\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %D : tensor<1000x1500xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1000x1500xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 1000, 1], ["%arg4", 0, 1500, 1], ["%arg5", 0, 200, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg5"], ["%arg5", "%arg4"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}, "execution_time": 1065382853.5}], ["linalg.matmul ins(%arg0, %arg1: tensor<1200x200xf32>, tensor<200x1000xf32>) outs(%arg2: tensor<1200x1000xf32>) -> tensor<1200x1000xf32>", {"operation": "linalg.matmul ins(%arg0, %arg1: tensor<1200x200xf32>, tensor<200x1000xf32>) outs(%arg2: tensor<1200x1000xf32>) -> tensor<1200x1000xf32>", "wrapped_operation": "func.func @func_call(%arg0: tensor<1200x200xf32>, %arg1: tensor<200x1000xf32>, %arg2: tensor<1200x1000xf32>) -> tensor<1200x1000xf32> {\n  %ret = linalg.matmul ins(%arg0, %arg1: tensor<1200x200xf32>, tensor<200x1000xf32>) outs(%arg2: tensor<1200x1000xf32>) -> tensor<1200x1000xf32>\n  return %ret : tensor<1200x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<1200x200xf32>, %arg1: tensor<200x1000xf32>, %arg2: tensor<1200x1000xf32>) -> tensor<1200x1000xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<200x1000xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1200x200xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1200x1000xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1200x1000xf32>\n    memref.copy %2, %alloc : memref<1200x1000xf32> to memref<1200x1000xf32>\n    affine.for %arg3 = 0 to 1200 {\n      affine.for %arg4 = 0 to 1000 {\n        affine.for %arg5 = 0 to 200 {\n          %4 = affine.load %1[%arg3, %arg5] : memref<1200x200xf32>\n          %5 = affine.load %0[%arg5, %arg4] : memref<200x1000xf32>\n          %6 = affine.load %alloc[%arg3, %arg4] : memref<1200x1000xf32>\n          %7 = arith.mulf %4, %5 : f32\n          %8 = arith.addf %6, %7 : f32\n          affine.store %8, %alloc[%arg3, %arg4] : memref<1200x1000xf32>\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1200x1000xf32>\n    return %3 : tensor<1200x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1200x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%out = bufferization.alloc_tensor() : tensor<1200x200xf32>\n%A = linalg.fill ins(%val : f32) outs(%out : tensor<1200x200xf32>) -> tensor<1200x200xf32>\n%out1 = bufferization.alloc_tensor() : tensor<200x1000xf32>\n%B = linalg.fill ins(%val : f32) outs(%out1 : tensor<200x1000xf32>) -> tensor<200x1000xf32>\n%out2 = bufferization.alloc_tensor() : tensor<1200x1000xf32>\n%C = linalg.fill ins(%zero : f32) outs(%out2 : tensor<1200x1000xf32>) -> tensor<1200x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%D = linalg.matmul ins(%A, %B: tensor<1200x200xf32>, tensor<200x1000xf32>) outs(%C: tensor<1200x1000xf32>) -> tensor<1200x1000xf32>\n\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %D : tensor<1200x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1200x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 1200, 1], ["%arg4", 0, 1000, 1], ["%arg5", 0, 200, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg5"], ["%arg5", "%arg4"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}, "execution_time": 851288274.5}], ["linalg.matmul ins(%arg0, %arg1: tensor<1200x1500xf32>, tensor<1500x1000xf32>) outs(%arg2: tensor<1200x1000xf32>) -> tensor<1200x1000xf32>", {"operation": "linalg.matmul ins(%arg0, %arg1: tensor<1200x1500xf32>, tensor<1500x1000xf32>) outs(%arg2: tensor<1200x1000xf32>) -> tensor<1200x1000xf32>", "wrapped_operation": "func.func @func_call(%arg0: tensor<1200x1500xf32>, %arg1: tensor<1500x1000xf32>, %arg2: tensor<1200x1000xf32>) -> tensor<1200x1000xf32> {\n  %ret = linalg.matmul ins(%arg0, %arg1: tensor<1200x1500xf32>, tensor<1500x1000xf32>) outs(%arg2: tensor<1200x1000xf32>) -> tensor<1200x1000xf32>\n  return %ret : tensor<1200x1000xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<1200x1500xf32>, %arg1: tensor<1500x1000xf32>, %arg2: tensor<1200x1000xf32>) -> tensor<1200x1000xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<1500x1000xf32>\n    %1 = bufferization.to_memref %arg0 : memref<1200x1500xf32>\n    %2 = bufferization.to_memref %arg2 : memref<1200x1000xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<1200x1000xf32>\n    memref.copy %2, %alloc : memref<1200x1000xf32> to memref<1200x1000xf32>\n    affine.for %arg3 = 0 to 1200 {\n      affine.for %arg4 = 0 to 1000 {\n        affine.for %arg5 = 0 to 1500 {\n          %4 = affine.load %1[%arg3, %arg5] : memref<1200x1500xf32>\n          %5 = affine.load %0[%arg5, %arg4] : memref<1500x1000xf32>\n          %6 = affine.load %alloc[%arg3, %arg4] : memref<1200x1000xf32>\n          %7 = arith.mulf %4, %5 : f32\n          %8 = arith.addf %6, %7 : f32\n          affine.store %8, %alloc[%arg3, %arg4] : memref<1200x1000xf32>\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<1200x1000xf32>\n    return %3 : tensor<1200x1000xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<1200x1000xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%out = bufferization.alloc_tensor() : tensor<1200x1500xf32>\n%A = linalg.fill ins(%val : f32) outs(%out : tensor<1200x1500xf32>) -> tensor<1200x1500xf32>\n%out1 = bufferization.alloc_tensor() : tensor<1500x1000xf32>\n%B = linalg.fill ins(%val : f32) outs(%out1 : tensor<1500x1000xf32>) -> tensor<1500x1000xf32>\n%out2 = bufferization.alloc_tensor() : tensor<1200x1000xf32>\n%C = linalg.fill ins(%zero : f32) outs(%out2 : tensor<1200x1000xf32>) -> tensor<1200x1000xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%D = linalg.matmul ins(%A, %B: tensor<1200x1500xf32>, tensor<1500x1000xf32>) outs(%C: tensor<1200x1000xf32>) -> tensor<1200x1000xf32>\n\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %D : tensor<1200x1000xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<1200x1000xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 1200, 1], ["%arg4", 0, 1000, 1], ["%arg5", 0, 1500, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg5"], ["%arg5", "%arg4"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}, "execution_time": 6798548785.0}], ["linalg.matmul ins(%arg0, %arg1: tensor<2000x200xf32>, tensor<200x800xf32>) outs(%arg2: tensor<2000x800xf32>) -> tensor<2000x800xf32>", {"operation": "linalg.matmul ins(%arg0, %arg1: tensor<2000x200xf32>, tensor<200x800xf32>) outs(%arg2: tensor<2000x800xf32>) -> tensor<2000x800xf32>", "wrapped_operation": "func.func @func_call(%arg0: tensor<2000x200xf32>, %arg1: tensor<200x800xf32>, %arg2: tensor<2000x800xf32>) -> tensor<2000x800xf32> {\n  %ret = linalg.matmul ins(%arg0, %arg1: tensor<2000x200xf32>, tensor<200x800xf32>) outs(%arg2: tensor<2000x800xf32>) -> tensor<2000x800xf32>\n  return %ret : tensor<2000x800xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<2000x200xf32>, %arg1: tensor<200x800xf32>, %arg2: tensor<2000x800xf32>) -> tensor<2000x800xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<200x800xf32>\n    %1 = bufferization.to_memref %arg0 : memref<2000x200xf32>\n    %2 = bufferization.to_memref %arg2 : memref<2000x800xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<2000x800xf32>\n    memref.copy %2, %alloc : memref<2000x800xf32> to memref<2000x800xf32>\n    affine.for %arg3 = 0 to 2000 {\n      affine.for %arg4 = 0 to 800 {\n        affine.for %arg5 = 0 to 200 {\n          %4 = affine.load %1[%arg3, %arg5] : memref<2000x200xf32>\n          %5 = affine.load %0[%arg5, %arg4] : memref<200x800xf32>\n          %6 = affine.load %alloc[%arg3, %arg4] : memref<2000x800xf32>\n          %7 = arith.mulf %4, %5 : f32\n          %8 = arith.addf %6, %7 : f32\n          affine.store %8, %alloc[%arg3, %arg4] : memref<2000x800xf32>\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<2000x800xf32>\n    return %3 : tensor<2000x800xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<2000x800xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%out = bufferization.alloc_tensor() : tensor<2000x200xf32>\n%A = linalg.fill ins(%val : f32) outs(%out : tensor<2000x200xf32>) -> tensor<2000x200xf32>\n%out1 = bufferization.alloc_tensor() : tensor<200x800xf32>\n%B = linalg.fill ins(%val : f32) outs(%out1 : tensor<200x800xf32>) -> tensor<200x800xf32>\n%out2 = bufferization.alloc_tensor() : tensor<2000x800xf32>\n%C = linalg.fill ins(%zero : f32) outs(%out2 : tensor<2000x800xf32>) -> tensor<2000x800xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%D = linalg.matmul ins(%A, %B: tensor<2000x200xf32>, tensor<200x800xf32>) outs(%C: tensor<2000x800xf32>) -> tensor<2000x800xf32>\n\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %D : tensor<2000x800xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<2000x800xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 2000, 1], ["%arg4", 0, 800, 1], ["%arg5", 0, 200, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg5"], ["%arg5", "%arg4"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}, "execution_time": 1136681376.0}], ["linalg.matmul ins(%arg0, %arg1: tensor<2400x400xf32>, tensor<400x600xf32>) outs(%arg2: tensor<2400x600xf32>) -> tensor<2400x600xf32>", {"operation": "linalg.matmul ins(%arg0, %arg1: tensor<2400x400xf32>, tensor<400x600xf32>) outs(%arg2: tensor<2400x600xf32>) -> tensor<2400x600xf32>", "wrapped_operation": "func.func @func_call(%arg0: tensor<2400x400xf32>, %arg1: tensor<400x600xf32>, %arg2: tensor<2400x600xf32>) -> tensor<2400x600xf32> {\n  %ret = linalg.matmul ins(%arg0, %arg1: tensor<2400x400xf32>, tensor<400x600xf32>) outs(%arg2: tensor<2400x600xf32>) -> tensor<2400x600xf32>\n  return %ret : tensor<2400x600xf32>\n}", "lowered_operation": "module {\n  func.func @func_call(%arg0: tensor<2400x400xf32>, %arg1: tensor<400x600xf32>, %arg2: tensor<2400x600xf32>) -> tensor<2400x600xf32> {\n    %0 = bufferization.to_memref %arg1 : memref<400x600xf32>\n    %1 = bufferization.to_memref %arg0 : memref<2400x400xf32>\n    %2 = bufferization.to_memref %arg2 : memref<2400x600xf32>\n    %alloc = memref.alloc() {alignment = 64 : i64} : memref<2400x600xf32>\n    memref.copy %2, %alloc : memref<2400x600xf32> to memref<2400x600xf32>\n    affine.for %arg3 = 0 to 2400 {\n      affine.for %arg4 = 0 to 600 {\n        affine.for %arg5 = 0 to 400 {\n          %4 = affine.load %1[%arg3, %arg5] : memref<2400x400xf32>\n          %5 = affine.load %0[%arg5, %arg4] : memref<400x600xf32>\n          %6 = affine.load %alloc[%arg3, %arg4] : memref<2400x600xf32>\n          %7 = arith.mulf %4, %5 : f32\n          %8 = arith.addf %6, %7 : f32\n          affine.store %8, %alloc[%arg3, %arg4] : memref<2400x600xf32>\n        }\n      }\n    }\n    %3 = bufferization.to_tensor %alloc : memref<2400x600xf32>\n    return %3 : tensor<2400x600xf32>\n  }\n}\n\n", "transform_wrapped_operation": "func.func private @nanoTime() -> i64 attributes { llvm.emit_c_interface }\nfunc.func private @printFlops(f64)\nfunc.func private @printI64(i64)\nfunc.func private @printNewline()\nfunc.func private @printMemrefF32(tensor<*xf32>)\n\n\nfunc.func @matmul() -> tensor<2400x600xf32>{\n\n%val = arith.constant 2.00000e+00 : f32\n%zero = arith.constant 0.00000e+00 : f32\n\n%out = bufferization.alloc_tensor() : tensor<2400x400xf32>\n%A = linalg.fill ins(%val : f32) outs(%out : tensor<2400x400xf32>) -> tensor<2400x400xf32>\n%out1 = bufferization.alloc_tensor() : tensor<400x600xf32>\n%B = linalg.fill ins(%val : f32) outs(%out1 : tensor<400x600xf32>) -> tensor<400x600xf32>\n%out2 = bufferization.alloc_tensor() : tensor<2400x600xf32>\n%C = linalg.fill ins(%zero : f32) outs(%out2 : tensor<2400x600xf32>) -> tensor<2400x600xf32>\n\n%t0 = func.call @nanoTime() : () -> (i64)\n\n%D = linalg.matmul ins(%A, %B: tensor<2400x400xf32>, tensor<400x600xf32>) outs(%C: tensor<2400x600xf32>) -> tensor<2400x600xf32>\n\n%t = func.call @nanoTime() : () -> (i64)\n%delta = arith.subi %t, %t0 : i64\n%fp = arith.uitofp %delta : i64 to f64\n// func.call @printFlops(%fp) : (f64) -> ()\nfunc.call @printI64(%delta) : (i64) -> ()\nfunc.call @printNewline() : () -> ()\n\nreturn %D : tensor<2400x600xf32> \n}\n\nfunc.func @main(){\n    %c1 = arith.constant 1: index\n    %c0 = arith.constant 0 : index\n    %n = arith.constant 2: index\n    scf.for %i = %c0 to %n step %c1 {\n    %outputmain = func.call @matmul() : () -> tensor<2400x600xf32>\n    }\n    return\n}\n\n", "loops_data": {"nested_loops": [["%arg3", 0, 2400, 1], ["%arg4", 0, 600, 1], ["%arg5", 0, 400, 1]], "op_count": {"+": 1, "-": 0, "*": 1, "/": 0, "exp": 0}, "load_data": [["%arg3", "%arg5"], ["%arg5", "%arg4"], ["%arg3", "%arg4"]], "store_data": ["%arg3", "%arg4"]}, "execution_time": 2110507318.0}]]