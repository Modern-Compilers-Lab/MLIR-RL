module attributes {tf.versions = {bad_consumers = [], min_consumer = 12 : i32, producer = 1482 : i32}, tf_saved_model.semantics} {
  "tf_saved_model.global_tensor"() {is_mutable, sym_name = "__sm_node26__model.layer-0.kernel", tf_saved_model.exported_names = [], type = tensor<3x3x3x2xf32>, value = dense<[[[[0.176463425, -2.369690e-01], [-0.304605335, -0.0860649049], [-0.165641204, -0.266251624]], [[-0.0499321222, 0.207697511], [0.0934430062, -0.246268898], [-0.361939043, 0.138852894]], [[0.257687092, -0.101477236], [-0.329039872, 0.155028462], [-0.193229228, 0.0859983861]]], [[[-0.325796425, 0.195808649], [0.325243711, -0.115343526], [-0.325931102, -0.357967108]], [[0.284842849, -0.131421775], [-0.265716016, 0.261599243], [0.0133949518, 0.0997334718]], [[0.234076023, -0.225259662], [-0.0445687175, -0.00787910819], [0.183279335, -0.0708890259]]], [[[-0.0425833762, 0.314798653], [-3.829810e-02, -0.136955455], [-0.0789331793, 0.217654169]], [[0.241518974, 0.108961552], [-0.123185113, -0.145454064], [-0.16551584, 0.128013432]], [[-0.0468637645, -0.100344539], [-0.16066356, 0.112919897], [0.244914293, 0.0809878408]]]]> : tensor<3x3x3x2xf32>} : () -> ()
  "tf_saved_model.global_tensor"() {is_mutable, sym_name = "__sm_node27__model.layer-0.bias", tf_saved_model.exported_names = [], type = tensor<2xf32>, value = dense<0.000000e+00> : tensor<2xf32>} : () -> ()
  "tf_saved_model.global_tensor"() {is_mutable, sym_name = "__sm_node41__model.layer-2.kernel", tf_saved_model.exported_names = [], type = tensor<3x3x2x3xf32>, value = dense<[[[[0.100532502, -0.168829441, 0.102570087], [0.212870359, 0.013250947, -0.104474396]], [[-0.262461871, -0.326837718, -0.324725688], [0.279223859, 0.205869734, -0.132944763]], [[0.240372539, -0.343261868, 0.0995227992], [0.317929149, -0.113965303, -0.102093875]]], [[[-0.155890778, -0.134274229, 0.128072381], [0.151166141, -0.111402392, -0.185141876]], [[0.00898438692, 0.238462627, 0.278365254], [-0.299663216, -0.231795728, 0.00850391387]], [[0.0516980886, -0.145300925, 0.334896088], [0.260956109, -0.0887488127, 0.231064856]]], [[[0.343109131, 0.225938737, -0.346620202], [0.311179638, 0.148108304, 0.137205243]], [[-0.357029587, -0.264785469, -0.0252652466], [-0.29172805, 0.0683663189, -0.358008206]], [[-0.0615435839, -0.130657926, 0.176007032], [0.317584574, 0.213780344, -0.204653785]]]]> : tensor<3x3x2x3xf32>} : () -> ()
  "tf_saved_model.global_tensor"() {is_mutable, sym_name = "__sm_node42__model.layer-2.bias", tf_saved_model.exported_names = [], type = tensor<3xf32>, value = dense<0.000000e+00> : tensor<3xf32>} : () -> ()
  "tf_saved_model.global_tensor"() {is_mutable, sym_name = "__sm_node62__model.layer-5.kernel", tf_saved_model.exported_names = [], type = tensor<75x4xf32>, value = dense<"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tensor<75x4xf32>} : () -> ()
  "tf_saved_model.global_tensor"() {is_mutable, sym_name = "__sm_node63__model.layer-5.bias", tf_saved_model.exported_names = [], type = tensor<4xf32>, value = dense<0.000000e+00> : tensor<4xf32>} : () -> ()
  "tf_saved_model.global_tensor"() {is_mutable, sym_name = "__sm_node70__model.layer-6.kernel", tf_saved_model.exported_names = [], type = tensor<4x1xf32>, value = dense<[[0.725377917], [-0.0616925955], [0.417314529], [0.662358046]]> : tensor<4x1xf32>} : () -> ()
  "tf_saved_model.global_tensor"() {is_mutable, sym_name = "__sm_node71__model.layer-6.bias", tf_saved_model.exported_names = [], type = tensor<1xf32>, value = dense<0.000000e+00> : tensor<1xf32>} : () -> ()
  func.func @__inference_my_predict_1640(%arg0: tensor<16x28x28x3xf32> {tf._user_specified_name = "x", tf_saved_model.index_path = [0]}, %arg1: tensor<!tf_type.resource<tensor<3x3x3x2xf32>>> {tf._user_specified_name = "resource", tf_saved_model.bound_input = @"__sm_node26__model.layer-0.kernel"}, %arg2: tensor<!tf_type.resource<tensor<2xf32>>> {tf._user_specified_name = "resource", tf_saved_model.bound_input = @"__sm_node27__model.layer-0.bias"}, %arg3: tensor<!tf_type.resource<tensor<3x3x2x3xf32>>> {tf._user_specified_name = "resource", tf_saved_model.bound_input = @"__sm_node41__model.layer-2.kernel"}, %arg4: tensor<!tf_type.resource<tensor<3xf32>>> {tf._user_specified_name = "resource", tf_saved_model.bound_input = @"__sm_node42__model.layer-2.bias"}, %arg5: tensor<!tf_type.resource<tensor<75x4xf32>>> {tf._user_specified_name = "resource", tf_saved_model.bound_input = @"__sm_node62__model.layer-5.kernel"}, %arg6: tensor<!tf_type.resource<tensor<4xf32>>> {tf._user_specified_name = "resource", tf_saved_model.bound_input = @"__sm_node63__model.layer-5.bias"}, %arg7: tensor<!tf_type.resource<tensor<4x1xf32>>> {tf._user_specified_name = "resource", tf_saved_model.bound_input = @"__sm_node70__model.layer-6.kernel"}, %arg8: tensor<!tf_type.resource<tensor<1xf32>>> {tf._user_specified_name = "resource", tf_saved_model.bound_input = @"__sm_node71__model.layer-6.bias"}) -> (tensor<16x1xf32> {tf_saved_model.index_path = []}) attributes {tf._construction_context = "kEagerRuntime", tf._input_shapes = [#tf_type.shape<16x28x28x3>, #tf_type.shape<>, #tf_type.shape<>, #tf_type.shape<>, #tf_type.shape<>, #tf_type.shape<>, #tf_type.shape<>, #tf_type.shape<>, #tf_type.shape<>], tf.signature.is_stateful, tf_saved_model.exported_names = ["my_predict"]} {
    %0 = "tf.Cast"(%arg1) {Truncate = false} : (tensor<!tf_type.resource<tensor<3x3x3x2xf32>>>) -> tensor<!tf_type.resource>
    %1 = "tf.Cast"(%arg2) {Truncate = false} : (tensor<!tf_type.resource<tensor<2xf32>>>) -> tensor<!tf_type.resource>
    %2 = "tf.Cast"(%arg3) {Truncate = false} : (tensor<!tf_type.resource<tensor<3x3x2x3xf32>>>) -> tensor<!tf_type.resource>
    %3 = "tf.Cast"(%arg4) {Truncate = false} : (tensor<!tf_type.resource<tensor<3xf32>>>) -> tensor<!tf_type.resource>
    %4 = "tf.Cast"(%arg5) {Truncate = false} : (tensor<!tf_type.resource<tensor<75x4xf32>>>) -> tensor<!tf_type.resource>
    %5 = "tf.Cast"(%arg6) {Truncate = false} : (tensor<!tf_type.resource<tensor<4xf32>>>) -> tensor<!tf_type.resource>
    %6 = "tf.Cast"(%arg7) {Truncate = false} : (tensor<!tf_type.resource<tensor<4x1xf32>>>) -> tensor<!tf_type.resource>
    %7 = "tf.Cast"(%arg8) {Truncate = false} : (tensor<!tf_type.resource<tensor<1xf32>>>) -> tensor<!tf_type.resource>
    %8 = tf_executor.graph {
      %outputs, %control = tf_executor.island wraps "tf.Const"() {device = "", value = dense<[-1, 75]> : tensor<2xi32>} : () -> tensor<2xi32>
      %outputs_0, %control_1 = tf_executor.island wraps "tf.ReadVariableOp"(%3) {device = ""} : (tensor<!tf_type.resource>) -> tensor<3xf32>
      %outputs_2, %control_3 = tf_executor.island wraps "tf.ReadVariableOp"(%2) {device = ""} : (tensor<!tf_type.resource>) -> tensor<3x3x2x3xf32>
      %outputs_4, %control_5 = tf_executor.island wraps "tf.ReadVariableOp"(%1) {device = ""} : (tensor<!tf_type.resource>) -> tensor<2xf32>
      %outputs_6, %control_7 = tf_executor.island wraps "tf.ReadVariableOp"(%0) {device = ""} : (tensor<!tf_type.resource>) -> tensor<3x3x3x2xf32>
      %outputs_8, %control_9 = tf_executor.island wraps "tf.ReadVariableOp"(%7) {device = ""} : (tensor<!tf_type.resource>) -> tensor<1xf32>
      %outputs_10, %control_11 = tf_executor.island wraps "tf.ReadVariableOp"(%6) {device = ""} : (tensor<!tf_type.resource>) -> tensor<4x1xf32>
      %outputs_12, %control_13 = tf_executor.island wraps "tf.ReadVariableOp"(%5) {device = ""} : (tensor<!tf_type.resource>) -> tensor<4xf32>
      %outputs_14, %control_15 = tf_executor.island wraps "tf.ReadVariableOp"(%4) {device = ""} : (tensor<!tf_type.resource>) -> tensor<75x4xf32>
      %control_16 = tf_executor.island(%control_7, %control_5, %control_3, %control_1, %control_15, %control_13, %control_11, %control_9) wraps "tf.NoOp"() {device = ""} : () -> ()
      %outputs_17, %control_18 = tf_executor.island wraps "tf.Conv2D"(%arg0, %outputs_6) {data_format = "NHWC", device = "", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = "VALID", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true} : (tensor<16x28x28x3xf32>, tensor<3x3x3x2xf32>) -> tensor<16x26x26x2xf32>
      %outputs_19, %control_20 = tf_executor.island wraps "tf.BiasAdd"(%outputs_17, %outputs_4) {data_format = "NHWC", device = ""} : (tensor<16x26x26x2xf32>, tensor<2xf32>) -> tensor<16x26x26x2xf32>
      %outputs_21, %control_22 = tf_executor.island wraps "tf.Relu"(%outputs_19) {device = ""} : (tensor<16x26x26x2xf32>) -> tensor<16x26x26x2xf32>
      %outputs_23, %control_24 = tf_executor.island wraps "tf.MaxPool"(%outputs_21) {data_format = "NHWC", device = "", explicit_paddings = [], ksize = [1, 2, 2, 1], padding = "VALID", strides = [1, 2, 2, 1]} : (tensor<16x26x26x2xf32>) -> tensor<16x13x13x2xf32>
      %outputs_25, %control_26 = tf_executor.island wraps "tf.Conv2D"(%outputs_23, %outputs_2) {data_format = "NHWC", device = "", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = "VALID", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true} : (tensor<16x13x13x2xf32>, tensor<3x3x2x3xf32>) -> tensor<16x11x11x3xf32>
      %outputs_27, %control_28 = tf_executor.island wraps "tf.BiasAdd"(%outputs_25, %outputs_0) {data_format = "NHWC", device = ""} : (tensor<16x11x11x3xf32>, tensor<3xf32>) -> tensor<16x11x11x3xf32>
      %outputs_29, %control_30 = tf_executor.island wraps "tf.Relu"(%outputs_27) {device = ""} : (tensor<16x11x11x3xf32>) -> tensor<16x11x11x3xf32>
      %outputs_31, %control_32 = tf_executor.island wraps "tf.MaxPool"(%outputs_29) {data_format = "NHWC", device = "", explicit_paddings = [], ksize = [1, 2, 2, 1], padding = "VALID", strides = [1, 2, 2, 1]} : (tensor<16x11x11x3xf32>) -> tensor<16x5x5x3xf32>
      %outputs_33, %control_34 = tf_executor.island wraps "tf.Reshape"(%outputs_31, %outputs) {device = ""} : (tensor<16x5x5x3xf32>, tensor<2xi32>) -> tensor<16x75xf32>
      %outputs_35, %control_36 = tf_executor.island wraps "tf.MatMul"(%outputs_33, %outputs_14) {device = "", transpose_a = false, transpose_b = false} : (tensor<16x75xf32>, tensor<75x4xf32>) -> tensor<16x4xf32>
      %outputs_37, %control_38 = tf_executor.island wraps "tf.BiasAdd"(%outputs_35, %outputs_12) {data_format = "NHWC", device = ""} : (tensor<16x4xf32>, tensor<4xf32>) -> tensor<16x4xf32>
      %outputs_39, %control_40 = tf_executor.island wraps "tf.Relu"(%outputs_37) {device = ""} : (tensor<16x4xf32>) -> tensor<16x4xf32>
      %outputs_41, %control_42 = tf_executor.island wraps "tf.MatMul"(%outputs_39, %outputs_10) {device = "", transpose_a = false, transpose_b = false} : (tensor<16x4xf32>, tensor<4x1xf32>) -> tensor<16x1xf32>
      %outputs_43, %control_44 = tf_executor.island wraps "tf.BiasAdd"(%outputs_41, %outputs_8) {data_format = "NHWC", device = ""} : (tensor<16x1xf32>, tensor<1xf32>) -> tensor<16x1xf32>
      %outputs_45, %control_46 = tf_executor.island wraps "tf.Softmax"(%outputs_43) {device = ""} : (tensor<16x1xf32>) -> tensor<16x1xf32>
      %outputs_47, %control_48 = tf_executor.island(%control_16) wraps "tf.Identity"(%outputs_45) {device = ""} : (tensor<16x1xf32>) -> tensor<16x1xf32>
      tf_executor.fetch %outputs_47, %control_7, %control_5, %control_3, %control_1, %control_15, %control_13, %control_11, %control_9 : tensor<16x1xf32>, !tf_executor.control, !tf_executor.control, !tf_executor.control, !tf_executor.control, !tf_executor.control, !tf_executor.control, !tf_executor.control, !tf_executor.control
    }
    return %8 : tensor<16x1xf32>
  }
  func.func private @__inference__wrapped_model_2220(%arg0: tensor<?x28x28x3xf32> {tf._user_specified_name = "conv2d_input"}, %arg1: tensor<!tf_type.resource> {tf._user_specified_name = "resource"}, %arg2: tensor<!tf_type.resource> {tf._user_specified_name = "resource"}, %arg3: tensor<!tf_type.resource> {tf._user_specified_name = "resource"}, %arg4: tensor<!tf_type.resource> {tf._user_specified_name = "resource"}, %arg5: tensor<!tf_type.resource> {tf._user_specified_name = "resource"}, %arg6: tensor<!tf_type.resource> {tf._user_specified_name = "resource"}, %arg7: tensor<!tf_type.resource> {tf._user_specified_name = "resource"}, %arg8: tensor<!tf_type.resource> {tf._user_specified_name = "resource"}) -> tensor<?x1xf32> attributes {tf._construction_context = "kEagerRuntime", tf._input_shapes = [#tf_type.shape<?x28x28x3>, #tf_type.shape<>, #tf_type.shape<>, #tf_type.shape<>, #tf_type.shape<>, #tf_type.shape<>, #tf_type.shape<>, #tf_type.shape<>, #tf_type.shape<>], tf.signature.is_stateful} {
    %0 = tf_executor.graph {
      %outputs, %control = tf_executor.island wraps "tf.Const"() {device = "", value = dense<[-1, 75]> : tensor<2xi32>} : () -> tensor<2xi32>
      %outputs_0, %control_1 = tf_executor.island wraps "tf.ReadVariableOp"(%arg4) {device = ""} : (tensor<!tf_type.resource>) -> tensor<3xf32>
      %outputs_2, %control_3 = tf_executor.island wraps "tf.ReadVariableOp"(%arg3) {device = ""} : (tensor<!tf_type.resource>) -> tensor<3x3x2x3xf32>
      %outputs_4, %control_5 = tf_executor.island wraps "tf.ReadVariableOp"(%arg2) {device = ""} : (tensor<!tf_type.resource>) -> tensor<2xf32>
      %outputs_6, %control_7 = tf_executor.island wraps "tf.ReadVariableOp"(%arg1) {device = ""} : (tensor<!tf_type.resource>) -> tensor<3x3x3x2xf32>
      %outputs_8, %control_9 = tf_executor.island wraps "tf.Conv2D"(%arg0, %outputs_6) {data_format = "NHWC", device = "", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = "VALID", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true} : (tensor<?x28x28x3xf32>, tensor<3x3x3x2xf32>) -> tensor<?x26x26x2xf32>
      %outputs_10, %control_11 = tf_executor.island wraps "tf.BiasAdd"(%outputs_8, %outputs_4) {data_format = "NHWC", device = ""} : (tensor<?x26x26x2xf32>, tensor<2xf32>) -> tensor<?x26x26x2xf32>
      %outputs_12, %control_13 = tf_executor.island wraps "tf.Relu"(%outputs_10) {device = ""} : (tensor<?x26x26x2xf32>) -> tensor<?x26x26x2xf32>
      %outputs_14, %control_15 = tf_executor.island wraps "tf.MaxPool"(%outputs_12) {data_format = "NHWC", device = "", explicit_paddings = [], ksize = [1, 2, 2, 1], padding = "VALID", strides = [1, 2, 2, 1]} : (tensor<?x26x26x2xf32>) -> tensor<?x13x13x2xf32>
      %outputs_16, %control_17 = tf_executor.island wraps "tf.Conv2D"(%outputs_14, %outputs_2) {data_format = "NHWC", device = "", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = "VALID", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true} : (tensor<?x13x13x2xf32>, tensor<3x3x2x3xf32>) -> tensor<?x11x11x3xf32>
      %outputs_18, %control_19 = tf_executor.island wraps "tf.BiasAdd"(%outputs_16, %outputs_0) {data_format = "NHWC", device = ""} : (tensor<?x11x11x3xf32>, tensor<3xf32>) -> tensor<?x11x11x3xf32>
      %outputs_20, %control_21 = tf_executor.island wraps "tf.Relu"(%outputs_18) {device = ""} : (tensor<?x11x11x3xf32>) -> tensor<?x11x11x3xf32>
      %outputs_22, %control_23 = tf_executor.island wraps "tf.MaxPool"(%outputs_20) {data_format = "NHWC", device = "", explicit_paddings = [], ksize = [1, 2, 2, 1], padding = "VALID", strides = [1, 2, 2, 1]} : (tensor<?x11x11x3xf32>) -> tensor<?x5x5x3xf32>
      %outputs_24, %control_25 = tf_executor.island wraps "tf.Reshape"(%outputs_22, %outputs) {device = ""} : (tensor<?x5x5x3xf32>, tensor<2xi32>) -> tensor<?x75xf32>
      %outputs_26, %control_27 = tf_executor.island wraps "tf.ReadVariableOp"(%arg8) {device = ""} : (tensor<!tf_type.resource>) -> tensor<1xf32>
      %outputs_28, %control_29 = tf_executor.island wraps "tf.ReadVariableOp"(%arg7) {device = ""} : (tensor<!tf_type.resource>) -> tensor<4x1xf32>
      %outputs_30, %control_31 = tf_executor.island wraps "tf.ReadVariableOp"(%arg6) {device = ""} : (tensor<!tf_type.resource>) -> tensor<4xf32>
      %outputs_32, %control_33 = tf_executor.island wraps "tf.ReadVariableOp"(%arg5) {device = ""} : (tensor<!tf_type.resource>) -> tensor<75x4xf32>
      %control_34 = tf_executor.island(%control_7, %control_5, %control_3, %control_1, %control_33, %control_31, %control_29, %control_27) wraps "tf.NoOp"() {device = ""} : () -> ()
      %outputs_35, %control_36 = tf_executor.island wraps "tf.MatMul"(%outputs_24, %outputs_32) {device = "", transpose_a = false, transpose_b = false} : (tensor<?x75xf32>, tensor<75x4xf32>) -> tensor<?x4xf32>
      %outputs_37, %control_38 = tf_executor.island wraps "tf.BiasAdd"(%outputs_35, %outputs_30) {data_format = "NHWC", device = ""} : (tensor<?x4xf32>, tensor<4xf32>) -> tensor<?x4xf32>
      %outputs_39, %control_40 = tf_executor.island wraps "tf.Relu"(%outputs_37) {device = ""} : (tensor<?x4xf32>) -> tensor<?x4xf32>
      %outputs_41, %control_42 = tf_executor.island wraps "tf.MatMul"(%outputs_39, %outputs_28) {device = "", transpose_a = false, transpose_b = false} : (tensor<?x4xf32>, tensor<4x1xf32>) -> tensor<?x1xf32>
      %outputs_43, %control_44 = tf_executor.island wraps "tf.BiasAdd"(%outputs_41, %outputs_26) {data_format = "NHWC", device = ""} : (tensor<?x1xf32>, tensor<1xf32>) -> tensor<?x1xf32>
      %outputs_45, %control_46 = tf_executor.island wraps "tf.Softmax"(%outputs_43) {device = ""} : (tensor<?x1xf32>) -> tensor<?x1xf32>
      %outputs_47, %control_48 = tf_executor.island(%control_34) wraps "tf.Identity"(%outputs_45) {device = ""} : (tensor<?x1xf32>) -> tensor<?x1xf32>
      tf_executor.fetch %outputs_47, %control_7, %control_5, %control_3, %control_1, %control_33, %control_31, %control_29, %control_27 : tensor<?x1xf32>, !tf_executor.control, !tf_executor.control, !tf_executor.control, !tf_executor.control, !tf_executor.control, !tf_executor.control, !tf_executor.control, !tf_executor.control
    }
    return %0 : tensor<?x1xf32>
  }
  func.func private @__inference_conv2d_1_layer_call_and_return_conditional_losses_2720(%arg0: tensor<?x13x13x2xf32> {tf._user_specified_name = "inputs"}, %arg1: tensor<!tf_type.resource> {tf._user_specified_name = "resource"}, %arg2: tensor<!tf_type.resource> {tf._user_specified_name = "resource"}) -> tensor<?x11x11x3xf32> attributes {tf._construction_context = "kEagerRuntime", tf._input_shapes = [#tf_type.shape<?x13x13x2>, #tf_type.shape<>, #tf_type.shape<>], tf.signature.is_stateful} {
    %0 = tf_executor.graph {
      %outputs, %control = tf_executor.island wraps "tf.ReadVariableOp"(%arg2) {device = ""} : (tensor<!tf_type.resource>) -> tensor<3xf32>
      %outputs_0, %control_1 = tf_executor.island wraps "tf.ReadVariableOp"(%arg1) {device = ""} : (tensor<!tf_type.resource>) -> tensor<3x3x2x3xf32>
      %control_2 = tf_executor.island(%control_1, %control) wraps "tf.NoOp"() {device = ""} : () -> ()
      %outputs_3, %control_4 = tf_executor.island wraps "tf.Conv2D"(%arg0, %outputs_0) {data_format = "NHWC", device = "", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = "VALID", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true} : (tensor<?x13x13x2xf32>, tensor<3x3x2x3xf32>) -> tensor<?x11x11x3xf32>
      %outputs_5, %control_6 = tf_executor.island wraps "tf.BiasAdd"(%outputs_3, %outputs) {data_format = "NHWC", device = ""} : (tensor<?x11x11x3xf32>, tensor<3xf32>) -> tensor<?x11x11x3xf32>
      %outputs_7, %control_8 = tf_executor.island wraps "tf.Relu"(%outputs_5) {device = ""} : (tensor<?x11x11x3xf32>) -> tensor<?x11x11x3xf32>
      %outputs_9, %control_10 = tf_executor.island(%control_2) wraps "tf.Identity"(%outputs_7) {device = ""} : (tensor<?x11x11x3xf32>) -> tensor<?x11x11x3xf32>
      tf_executor.fetch %outputs_9, %control_1, %control : tensor<?x11x11x3xf32>, !tf_executor.control, !tf_executor.control
    }
    return %0 : tensor<?x11x11x3xf32>
  }
  func.func private @__inference_conv2d_1_layer_call_and_return_conditional_losses_4790(%arg0: tensor<?x13x13x2xf32> {tf._user_specified_name = "inputs"}, %arg1: tensor<!tf_type.resource> {tf._user_specified_name = "resource"}, %arg2: tensor<!tf_type.resource> {tf._user_specified_name = "resource"}) -> tensor<?x11x11x3xf32> attributes {tf._construction_context = "kEagerRuntime", tf._input_shapes = [#tf_type.shape<?x13x13x2>, #tf_type.shape<>, #tf_type.shape<>], tf.signature.is_stateful} {
    %0 = tf_executor.graph {
      %outputs, %control = tf_executor.island wraps "tf.ReadVariableOp"(%arg2) {device = ""} : (tensor<!tf_type.resource>) -> tensor<3xf32>
      %outputs_0, %control_1 = tf_executor.island wraps "tf.ReadVariableOp"(%arg1) {device = ""} : (tensor<!tf_type.resource>) -> tensor<3x3x2x3xf32>
      %control_2 = tf_executor.island(%control_1, %control) wraps "tf.NoOp"() {device = ""} : () -> ()
      %outputs_3, %control_4 = tf_executor.island wraps "tf.Conv2D"(%arg0, %outputs_0) {data_format = "NHWC", device = "", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = "VALID", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true} : (tensor<?x13x13x2xf32>, tensor<3x3x2x3xf32>) -> tensor<?x11x11x3xf32>
      %outputs_5, %control_6 = tf_executor.island wraps "tf.BiasAdd"(%outputs_3, %outputs) {data_format = "NHWC", device = ""} : (tensor<?x11x11x3xf32>, tensor<3xf32>) -> tensor<?x11x11x3xf32>
      %outputs_7, %control_8 = tf_executor.island wraps "tf.Relu"(%outputs_5) {device = ""} : (tensor<?x11x11x3xf32>) -> tensor<?x11x11x3xf32>
      %outputs_9, %control_10 = tf_executor.island(%control_2) wraps "tf.Identity"(%outputs_7) {device = ""} : (tensor<?x11x11x3xf32>) -> tensor<?x11x11x3xf32>
      tf_executor.fetch %outputs_9, %control_1, %control : tensor<?x11x11x3xf32>, !tf_executor.control, !tf_executor.control
    }
    return %0 : tensor<?x11x11x3xf32>
  }
  func.func private @__inference_conv2d_1_layer_call_fn_4680(%arg0: tensor<?x13x13x2xf32> {tf._user_specified_name = "inputs"}, %arg1: tensor<!tf_type.resource> {tf._user_specified_name = "462"}, %arg2: tensor<!tf_type.resource> {tf._user_specified_name = "464"}) -> tensor<?x?x?x?xf32> attributes {tf._construction_context = "kEagerRuntime", tf._input_shapes = [#tf_type.shape<?x13x13x2>, #tf_type.shape<>, #tf_type.shape<>], tf.signature.is_stateful} {
    %0 = tf_executor.graph {
      %outputs, %control = tf_executor.island wraps "tf.StatefulPartitionedCall"(%arg0, %arg1, %arg2) {_collective_manager_ids = [], _read_only_resource_inputs = [1, 2], config = "", config_proto = "\0A\07\0A\03CPU\10\01\0A\07\0A\03GPU\10\002\02J\008\01\82\01\00", device = "", executor_type = "", f = @__inference_conv2d_1_layer_call_and_return_conditional_losses_2720} : (tensor<?x13x13x2xf32>, tensor<!tf_type.resource>, tensor<!tf_type.resource>) -> tensor<?x?x?x?xf32>
      %control_0 = tf_executor.island(%control) wraps "tf.NoOp"() {device = ""} : () -> ()
      %outputs_1, %control_2 = tf_executor.island(%control_0) wraps "tf.Identity"(%outputs) {device = ""} : (tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>
      tf_executor.fetch %outputs_1, %control : tensor<?x?x?x?xf32>, !tf_executor.control
    }
    return %0 : tensor<?x?x?x?xf32>
  }
  func.func private @__inference_conv2d_layer_call_and_return_conditional_losses_2550(%arg0: tensor<?x28x28x3xf32> {tf._user_specified_name = "inputs"}, %arg1: tensor<!tf_type.resource> {tf._user_specified_name = "resource"}, %arg2: tensor<!tf_type.resource> {tf._user_specified_name = "resource"}) -> tensor<?x26x26x2xf32> attributes {tf._construction_context = "kEagerRuntime", tf._input_shapes = [#tf_type.shape<?x28x28x3>, #tf_type.shape<>, #tf_type.shape<>], tf.signature.is_stateful} {
    %0 = tf_executor.graph {
      %outputs, %control = tf_executor.island wraps "tf.ReadVariableOp"(%arg2) {device = ""} : (tensor<!tf_type.resource>) -> tensor<2xf32>
      %outputs_0, %control_1 = tf_executor.island wraps "tf.ReadVariableOp"(%arg1) {device = ""} : (tensor<!tf_type.resource>) -> tensor<3x3x3x2xf32>
      %control_2 = tf_executor.island(%control_1, %control) wraps "tf.NoOp"() {device = ""} : () -> ()
      %outputs_3, %control_4 = tf_executor.island wraps "tf.Conv2D"(%arg0, %outputs_0) {data_format = "NHWC", device = "", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = "VALID", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true} : (tensor<?x28x28x3xf32>, tensor<3x3x3x2xf32>) -> tensor<?x26x26x2xf32>
      %outputs_5, %control_6 = tf_executor.island wraps "tf.BiasAdd"(%outputs_3, %outputs) {data_format = "NHWC", device = ""} : (tensor<?x26x26x2xf32>, tensor<2xf32>) -> tensor<?x26x26x2xf32>
      %outputs_7, %control_8 = tf_executor.island wraps "tf.Relu"(%outputs_5) {device = ""} : (tensor<?x26x26x2xf32>) -> tensor<?x26x26x2xf32>
      %outputs_9, %control_10 = tf_executor.island(%control_2) wraps "tf.Identity"(%outputs_7) {device = ""} : (tensor<?x26x26x2xf32>) -> tensor<?x26x26x2xf32>
      tf_executor.fetch %outputs_9, %control_1, %control : tensor<?x26x26x2xf32>, !tf_executor.control, !tf_executor.control
    }
    return %0 : tensor<?x26x26x2xf32>
  }
  func.func private @__inference_conv2d_layer_call_and_return_conditional_losses_4490(%arg0: tensor<?x28x28x3xf32> {tf._user_specified_name = "inputs"}, %arg1: tensor<!tf_type.resource> {tf._user_specified_name = "resource"}, %arg2: tensor<!tf_type.resource> {tf._user_specified_name = "resource"}) -> tensor<?x26x26x2xf32> attributes {tf._construction_context = "kEagerRuntime", tf._input_shapes = [#tf_type.shape<?x28x28x3>, #tf_type.shape<>, #tf_type.shape<>], tf.signature.is_stateful} {
    %0 = tf_executor.graph {
      %outputs, %control = tf_executor.island wraps "tf.ReadVariableOp"(%arg2) {device = ""} : (tensor<!tf_type.resource>) -> tensor<2xf32>
      %outputs_0, %control_1 = tf_executor.island wraps "tf.ReadVariableOp"(%arg1) {device = ""} : (tensor<!tf_type.resource>) -> tensor<3x3x3x2xf32>
      %control_2 = tf_executor.island(%control_1, %control) wraps "tf.NoOp"() {device = ""} : () -> ()
      %outputs_3, %control_4 = tf_executor.island wraps "tf.Conv2D"(%arg0, %outputs_0) {data_format = "NHWC", device = "", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = "VALID", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true} : (tensor<?x28x28x3xf32>, tensor<3x3x3x2xf32>) -> tensor<?x26x26x2xf32>
      %outputs_5, %control_6 = tf_executor.island wraps "tf.BiasAdd"(%outputs_3, %outputs) {data_format = "NHWC", device = ""} : (tensor<?x26x26x2xf32>, tensor<2xf32>) -> tensor<?x26x26x2xf32>
      %outputs_7, %control_8 = tf_executor.island wraps "tf.Relu"(%outputs_5) {device = ""} : (tensor<?x26x26x2xf32>) -> tensor<?x26x26x2xf32>
      %outputs_9, %control_10 = tf_executor.island(%control_2) wraps "tf.Identity"(%outputs_7) {device = ""} : (tensor<?x26x26x2xf32>) -> tensor<?x26x26x2xf32>
      tf_executor.fetch %outputs_9, %control_1, %control : tensor<?x26x26x2xf32>, !tf_executor.control, !tf_executor.control
    }
    return %0 : tensor<?x26x26x2xf32>
  }
  func.func private @__inference_conv2d_layer_call_fn_4380(%arg0: tensor<?x28x28x3xf32> {tf._user_specified_name = "inputs"}, %arg1: tensor<!tf_type.resource> {tf._user_specified_name = "432"}, %arg2: tensor<!tf_type.resource> {tf._user_specified_name = "434"}) -> tensor<?x?x?x?xf32> attributes {tf._construction_context = "kEagerRuntime", tf._input_shapes = [#tf_type.shape<?x28x28x3>, #tf_type.shape<>, #tf_type.shape<>], tf.signature.is_stateful} {
    %0 = tf_executor.graph {
      %outputs, %control = tf_executor.island wraps "tf.StatefulPartitionedCall"(%arg0, %arg1, %arg2) {_collective_manager_ids = [], _read_only_resource_inputs = [1, 2], config = "", config_proto = "\0A\07\0A\03CPU\10\01\0A\07\0A\03GPU\10\002\02J\008\01\82\01\00", device = "", executor_type = "", f = @__inference_conv2d_layer_call_and_return_conditional_losses_2550} : (tensor<?x28x28x3xf32>, tensor<!tf_type.resource>, tensor<!tf_type.resource>) -> tensor<?x?x?x?xf32>
      %control_0 = tf_executor.island(%control) wraps "tf.NoOp"() {device = ""} : () -> ()
      %outputs_1, %control_2 = tf_executor.island(%control_0) wraps "tf.Identity"(%outputs) {device = ""} : (tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>
      tf_executor.fetch %outputs_1, %control : tensor<?x?x?x?xf32>, !tf_executor.control
    }
    return %0 : tensor<?x?x?x?xf32>
  }
  func.func private @__inference_dense_1_layer_call_and_return_conditional_losses_3120(%arg0: tensor<?x4xf32> {tf._user_specified_name = "inputs"}, %arg1: tensor<!tf_type.resource> {tf._user_specified_name = "resource"}, %arg2: tensor<!tf_type.resource> {tf._user_specified_name = "resource"}) -> tensor<?x1xf32> attributes {tf._construction_context = "kEagerRuntime", tf._input_shapes = [#tf_type.shape<?x4>, #tf_type.shape<>, #tf_type.shape<>], tf.signature.is_stateful} {
    %0 = tf_executor.graph {
      %outputs, %control = tf_executor.island wraps "tf.ReadVariableOp"(%arg2) {device = ""} : (tensor<!tf_type.resource>) -> tensor<1xf32>
      %outputs_0, %control_1 = tf_executor.island wraps "tf.ReadVariableOp"(%arg1) {device = ""} : (tensor<!tf_type.resource>) -> tensor<4x1xf32>
      %outputs_2, %control_3 = tf_executor.island wraps "tf.MatMul"(%arg0, %outputs_0) {device = "", transpose_a = false, transpose_b = false} : (tensor<?x4xf32>, tensor<4x1xf32>) -> tensor<?x1xf32>
      %outputs_4, %control_5 = tf_executor.island wraps "tf.BiasAdd"(%outputs_2, %outputs) {data_format = "NHWC", device = ""} : (tensor<?x1xf32>, tensor<1xf32>) -> tensor<?x1xf32>
      %outputs_6, %control_7 = tf_executor.island wraps "tf.Softmax"(%outputs_4) {device = ""} : (tensor<?x1xf32>) -> tensor<?x1xf32>
      %control_8 = tf_executor.island(%control_1, %control) wraps "tf.NoOp"() {device = ""} : () -> ()
      %outputs_9, %control_10 = tf_executor.island(%control_8) wraps "tf.Identity"(%outputs_6) {device = ""} : (tensor<?x1xf32>) -> tensor<?x1xf32>
      tf_executor.fetch %outputs_9, %control_1, %control : tensor<?x1xf32>, !tf_executor.control, !tf_executor.control
    }
    return %0 : tensor<?x1xf32>
  }
  func.func private @__inference_dense_1_layer_call_and_return_conditional_losses_5400(%arg0: tensor<?x4xf32> {tf._user_specified_name = "inputs"}, %arg1: tensor<!tf_type.resource> {tf._user_specified_name = "resource"}, %arg2: tensor<!tf_type.resource> {tf._user_specified_name = "resource"}) -> tensor<?x1xf32> attributes {tf._construction_context = "kEagerRuntime", tf._input_shapes = [#tf_type.shape<?x4>, #tf_type.shape<>, #tf_type.shape<>], tf.signature.is_stateful} {
    %0 = tf_executor.graph {
      %outputs, %control = tf_executor.island wraps "tf.ReadVariableOp"(%arg2) {device = ""} : (tensor<!tf_type.resource>) -> tensor<1xf32>
      %outputs_0, %control_1 = tf_executor.island wraps "tf.ReadVariableOp"(%arg1) {device = ""} : (tensor<!tf_type.resource>) -> tensor<4x1xf32>
      %outputs_2, %control_3 = tf_executor.island wraps "tf.MatMul"(%arg0, %outputs_0) {device = "", transpose_a = false, transpose_b = false} : (tensor<?x4xf32>, tensor<4x1xf32>) -> tensor<?x1xf32>
      %outputs_4, %control_5 = tf_executor.island wraps "tf.BiasAdd"(%outputs_2, %outputs) {data_format = "NHWC", device = ""} : (tensor<?x1xf32>, tensor<1xf32>) -> tensor<?x1xf32>
      %outputs_6, %control_7 = tf_executor.island wraps "tf.Softmax"(%outputs_4) {device = ""} : (tensor<?x1xf32>) -> tensor<?x1xf32>
      %control_8 = tf_executor.island(%control_1, %control) wraps "tf.NoOp"() {device = ""} : () -> ()
      %outputs_9, %control_10 = tf_executor.island(%control_8) wraps "tf.Identity"(%outputs_6) {device = ""} : (tensor<?x1xf32>) -> tensor<?x1xf32>
      tf_executor.fetch %outputs_9, %control_1, %control : tensor<?x1xf32>, !tf_executor.control, !tf_executor.control
    }
    return %0 : tensor<?x1xf32>
  }
  func.func private @__inference_dense_1_layer_call_fn_5290(%arg0: tensor<?x4xf32> {tf._user_specified_name = "inputs"}, %arg1: tensor<!tf_type.resource> {tf._user_specified_name = "523"}, %arg2: tensor<!tf_type.resource> {tf._user_specified_name = "525"}) -> tensor<?x?xf32> attributes {tf._construction_context = "kEagerRuntime", tf._input_shapes = [#tf_type.shape<?x4>, #tf_type.shape<>, #tf_type.shape<>], tf.signature.is_stateful} {
    %0 = tf_executor.graph {
      %outputs, %control = tf_executor.island wraps "tf.StatefulPartitionedCall"(%arg0, %arg1, %arg2) {_collective_manager_ids = [], _read_only_resource_inputs = [1, 2], config = "", config_proto = "\0A\07\0A\03CPU\10\01\0A\07\0A\03GPU\10\002\02J\008\01\82\01\00", device = "", executor_type = "", f = @__inference_dense_1_layer_call_and_return_conditional_losses_3120} : (tensor<?x4xf32>, tensor<!tf_type.resource>, tensor<!tf_type.resource>) -> tensor<?x?xf32>
      %control_0 = tf_executor.island(%control) wraps "tf.NoOp"() {device = ""} : () -> ()
      %outputs_1, %control_2 = tf_executor.island(%control_0) wraps "tf.Identity"(%outputs) {device = ""} : (tensor<?x?xf32>) -> tensor<?x?xf32>
      tf_executor.fetch %outputs_1, %control : tensor<?x?xf32>, !tf_executor.control
    }
    return %0 : tensor<?x?xf32>
  }
  func.func private @__inference_dense_layer_call_and_return_conditional_losses_2960(%arg0: tensor<?x75xf32> {tf._user_specified_name = "inputs"}, %arg1: tensor<!tf_type.resource> {tf._user_specified_name = "resource"}, %arg2: tensor<!tf_type.resource> {tf._user_specified_name = "resource"}) -> tensor<?x4xf32> attributes {tf._construction_context = "kEagerRuntime", tf._input_shapes = [#tf_type.shape<?x75>, #tf_type.shape<>, #tf_type.shape<>], tf.signature.is_stateful} {
    %0 = tf_executor.graph {
      %outputs, %control = tf_executor.island wraps "tf.ReadVariableOp"(%arg2) {device = ""} : (tensor<!tf_type.resource>) -> tensor<4xf32>
      %outputs_0, %control_1 = tf_executor.island wraps "tf.ReadVariableOp"(%arg1) {device = ""} : (tensor<!tf_type.resource>) -> tensor<75x4xf32>
      %outputs_2, %control_3 = tf_executor.island wraps "tf.MatMul"(%arg0, %outputs_0) {device = "", transpose_a = false, transpose_b = false} : (tensor<?x75xf32>, tensor<75x4xf32>) -> tensor<?x4xf32>
      %outputs_4, %control_5 = tf_executor.island wraps "tf.BiasAdd"(%outputs_2, %outputs) {data_format = "NHWC", device = ""} : (tensor<?x4xf32>, tensor<4xf32>) -> tensor<?x4xf32>
      %outputs_6, %control_7 = tf_executor.island wraps "tf.Relu"(%outputs_4) {device = ""} : (tensor<?x4xf32>) -> tensor<?x4xf32>
      %control_8 = tf_executor.island(%control_1, %control) wraps "tf.NoOp"() {device = ""} : () -> ()
      %outputs_9, %control_10 = tf_executor.island(%control_8) wraps "tf.Identity"(%outputs_6) {device = ""} : (tensor<?x4xf32>) -> tensor<?x4xf32>
      tf_executor.fetch %outputs_9, %control_1, %control : tensor<?x4xf32>, !tf_executor.control, !tf_executor.control
    }
    return %0 : tensor<?x4xf32>
  }
  func.func private @__inference_dense_layer_call_and_return_conditional_losses_5200(%arg0: tensor<?x75xf32> {tf._user_specified_name = "inputs"}, %arg1: tensor<!tf_type.resource> {tf._user_specified_name = "resource"}, %arg2: tensor<!tf_type.resource> {tf._user_specified_name = "resource"}) -> tensor<?x4xf32> attributes {tf._construction_context = "kEagerRuntime", tf._input_shapes = [#tf_type.shape<?x75>, #tf_type.shape<>, #tf_type.shape<>], tf.signature.is_stateful} {
    %0 = tf_executor.graph {
      %outputs, %control = tf_executor.island wraps "tf.ReadVariableOp"(%arg2) {device = ""} : (tensor<!tf_type.resource>) -> tensor<4xf32>
      %outputs_0, %control_1 = tf_executor.island wraps "tf.ReadVariableOp"(%arg1) {device = ""} : (tensor<!tf_type.resource>) -> tensor<75x4xf32>
      %outputs_2, %control_3 = tf_executor.island wraps "tf.MatMul"(%arg0, %outputs_0) {device = "", transpose_a = false, transpose_b = false} : (tensor<?x75xf32>, tensor<75x4xf32>) -> tensor<?x4xf32>
      %outputs_4, %control_5 = tf_executor.island wraps "tf.BiasAdd"(%outputs_2, %outputs) {data_format = "NHWC", device = ""} : (tensor<?x4xf32>, tensor<4xf32>) -> tensor<?x4xf32>
      %outputs_6, %control_7 = tf_executor.island wraps "tf.Relu"(%outputs_4) {device = ""} : (tensor<?x4xf32>) -> tensor<?x4xf32>
      %control_8 = tf_executor.island(%control_1, %control) wraps "tf.NoOp"() {device = ""} : () -> ()
      %outputs_9, %control_10 = tf_executor.island(%control_8) wraps "tf.Identity"(%outputs_6) {device = ""} : (tensor<?x4xf32>) -> tensor<?x4xf32>
      tf_executor.fetch %outputs_9, %control_1, %control : tensor<?x4xf32>, !tf_executor.control, !tf_executor.control
    }
    return %0 : tensor<?x4xf32>
  }
  func.func private @__inference_dense_layer_call_fn_5090(%arg0: tensor<?x75xf32> {tf._user_specified_name = "inputs"}, %arg1: tensor<!tf_type.resource> {tf._user_specified_name = "503"}, %arg2: tensor<!tf_type.resource> {tf._user_specified_name = "505"}) -> tensor<?x?xf32> attributes {tf._construction_context = "kEagerRuntime", tf._input_shapes = [#tf_type.shape<?x75>, #tf_type.shape<>, #tf_type.shape<>], tf.signature.is_stateful} {
    %0 = tf_executor.graph {
      %outputs, %control = tf_executor.island wraps "tf.StatefulPartitionedCall"(%arg0, %arg1, %arg2) {_collective_manager_ids = [], _read_only_resource_inputs = [1, 2], config = "", config_proto = "\0A\07\0A\03CPU\10\01\0A\07\0A\03GPU\10\002\02J\008\01\82\01\00", device = "", executor_type = "", f = @__inference_dense_layer_call_and_return_conditional_losses_2960} : (tensor<?x75xf32>, tensor<!tf_type.resource>, tensor<!tf_type.resource>) -> tensor<?x?xf32>
      %control_0 = tf_executor.island(%control) wraps "tf.NoOp"() {device = ""} : () -> ()
      %outputs_1, %control_2 = tf_executor.island(%control_0) wraps "tf.Identity"(%outputs) {device = ""} : (tensor<?x?xf32>) -> tensor<?x?xf32>
      tf_executor.fetch %outputs_1, %control : tensor<?x?xf32>, !tf_executor.control
    }
    return %0 : tensor<?x?xf32>
  }
  func.func private @__inference_flatten_layer_call_and_return_conditional_losses_2840(%arg0: tensor<?x5x5x3xf32> {tf._user_specified_name = "inputs"}) -> tensor<?x75xf32> attributes {tf._construction_context = "kEagerRuntime", tf._input_shapes = [#tf_type.shape<?x5x5x3>]} {
    %0 = tf_executor.graph {
      %outputs, %control = tf_executor.island wraps "tf.Const"() {device = "", value = dense<[-1, 75]> : tensor<2xi32>} : () -> tensor<2xi32>
      %outputs_0, %control_1 = tf_executor.island wraps "tf.Reshape"(%arg0, %outputs) {device = ""} : (tensor<?x5x5x3xf32>, tensor<2xi32>) -> tensor<?x75xf32>
      %outputs_2, %control_3 = tf_executor.island wraps "tf.Identity"(%outputs_0) {device = ""} : (tensor<?x75xf32>) -> tensor<?x75xf32>
      tf_executor.fetch %outputs_2 : tensor<?x75xf32>
    }
    return %0 : tensor<?x75xf32>
  }
  func.func private @__inference_flatten_layer_call_and_return_conditional_losses_5000(%arg0: tensor<?x5x5x3xf32> {tf._user_specified_name = "inputs"}) -> tensor<?x75xf32> attributes {tf._construction_context = "kEagerRuntime", tf._input_shapes = [#tf_type.shape<?x5x5x3>]} {
    %0 = tf_executor.graph {
      %outputs, %control = tf_executor.island wraps "tf.Const"() {device = "", value = dense<[-1, 75]> : tensor<2xi32>} : () -> tensor<2xi32>
      %outputs_0, %control_1 = tf_executor.island wraps "tf.Reshape"(%arg0, %outputs) {device = ""} : (tensor<?x5x5x3xf32>, tensor<2xi32>) -> tensor<?x75xf32>
      %outputs_2, %control_3 = tf_executor.island wraps "tf.Identity"(%outputs_0) {device = ""} : (tensor<?x75xf32>) -> tensor<?x75xf32>
      tf_executor.fetch %outputs_2 : tensor<?x75xf32>
    }
    return %0 : tensor<?x75xf32>
  }
  func.func private @__inference_flatten_layer_call_fn_4940(%arg0: tensor<?x5x5x3xf32> {tf._user_specified_name = "inputs"}) -> tensor<?x75xf32> attributes {tf._construction_context = "kEagerRuntime", tf._input_shapes = [#tf_type.shape<?x5x5x3>]} {
    %0 = tf_executor.graph {
      %outputs, %control = tf_executor.island wraps "tf.PartitionedCall"(%arg0) {_collective_manager_ids = [], _read_only_resource_inputs = [], config = "", config_proto = "\0A\07\0A\03CPU\10\01\0A\07\0A\03GPU\10\002\02J\008\01\82\01\00", device = "", executor_type = "", f = @__inference_flatten_layer_call_and_return_conditional_losses_2840} : (tensor<?x5x5x3xf32>) -> tensor<?x75xf32>
      %outputs_0, %control_1 = tf_executor.island wraps "tf.Identity"(%outputs) {device = ""} : (tensor<?x75xf32>) -> tensor<?x75xf32>
      tf_executor.fetch %outputs_0 : tensor<?x75xf32>
    }
    return %0 : tensor<?x75xf32>
  }
  func.func private @__inference_max_pooling2d_1_layer_call_and_return_conditional_losses_2370(%arg0: tensor<?x?x?x?xf32> {tf._user_specified_name = "inputs"}) -> tensor<?x?x?x?xf32> attributes {tf._construction_context = "kEagerRuntime", tf._input_shapes = [#tf_type.shape<?x?x?x?>]} {
    %0 = tf_executor.graph {
      %outputs, %control = tf_executor.island wraps "tf.MaxPool"(%arg0) {data_format = "NHWC", device = "", explicit_paddings = [], ksize = [1, 2, 2, 1], padding = "VALID", strides = [1, 2, 2, 1]} : (tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>
      %outputs_0, %control_1 = tf_executor.island wraps "tf.Identity"(%outputs) {device = ""} : (tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>
      tf_executor.fetch %outputs_0 : tensor<?x?x?x?xf32>
    }
    return %0 : tensor<?x?x?x?xf32>
  }
  func.func private @__inference_max_pooling2d_1_layer_call_and_return_conditional_losses_4890(%arg0: tensor<?x?x?x?xf32> {tf._user_specified_name = "inputs"}) -> tensor<?x?x?x?xf32> attributes {tf._construction_context = "kEagerRuntime", tf._input_shapes = [#tf_type.shape<?x?x?x?>]} {
    %0 = tf_executor.graph {
      %outputs, %control = tf_executor.island wraps "tf.MaxPool"(%arg0) {data_format = "NHWC", device = "", explicit_paddings = [], ksize = [1, 2, 2, 1], padding = "VALID", strides = [1, 2, 2, 1]} : (tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>
      %outputs_0, %control_1 = tf_executor.island wraps "tf.Identity"(%outputs) {device = ""} : (tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>
      tf_executor.fetch %outputs_0 : tensor<?x?x?x?xf32>
    }
    return %0 : tensor<?x?x?x?xf32>
  }
  func.func private @__inference_max_pooling2d_1_layer_call_fn_4840(%arg0: tensor<?x?x?x?xf32> {tf._user_specified_name = "inputs"}) -> tensor<?x?x?x?xf32> attributes {tf._construction_context = "kEagerRuntime", tf._input_shapes = [#tf_type.shape<?x?x?x?>]} {
    %0 = tf_executor.graph {
      %outputs, %control = tf_executor.island wraps "tf.PartitionedCall"(%arg0) {_collective_manager_ids = [], _read_only_resource_inputs = [], config = "", config_proto = "\0A\07\0A\03CPU\10\01\0A\07\0A\03GPU\10\002\02J\008\01\82\01\00", device = "", executor_type = "", f = @__inference_max_pooling2d_1_layer_call_and_return_conditional_losses_2370} : (tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>
      %outputs_0, %control_1 = tf_executor.island wraps "tf.Identity"(%outputs) {device = ""} : (tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>
      tf_executor.fetch %outputs_0 : tensor<?x?x?x?xf32>
    }
    return %0 : tensor<?x?x?x?xf32>
  }
  func.func private @__inference_max_pooling2d_layer_call_and_return_conditional_losses_2270(%arg0: tensor<?x?x?x?xf32> {tf._user_specified_name = "inputs"}) -> tensor<?x?x?x?xf32> attributes {tf._construction_context = "kEagerRuntime", tf._input_shapes = [#tf_type.shape<?x?x?x?>]} {
    %0 = tf_executor.graph {
      %outputs, %control = tf_executor.island wraps "tf.MaxPool"(%arg0) {data_format = "NHWC", device = "", explicit_paddings = [], ksize = [1, 2, 2, 1], padding = "VALID", strides = [1, 2, 2, 1]} : (tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>
      %outputs_0, %control_1 = tf_executor.island wraps "tf.Identity"(%outputs) {device = ""} : (tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>
      tf_executor.fetch %outputs_0 : tensor<?x?x?x?xf32>
    }
    return %0 : tensor<?x?x?x?xf32>
  }
  func.func private @__inference_max_pooling2d_layer_call_and_return_conditional_losses_4590(%arg0: tensor<?x?x?x?xf32> {tf._user_specified_name = "inputs"}) -> tensor<?x?x?x?xf32> attributes {tf._construction_context = "kEagerRuntime", tf._input_shapes = [#tf_type.shape<?x?x?x?>]} {
    %0 = tf_executor.graph {
      %outputs, %control = tf_executor.island wraps "tf.MaxPool"(%arg0) {data_format = "NHWC", device = "", explicit_paddings = [], ksize = [1, 2, 2, 1], padding = "VALID", strides = [1, 2, 2, 1]} : (tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>
      %outputs_0, %control_1 = tf_executor.island wraps "tf.Identity"(%outputs) {device = ""} : (tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>
      tf_executor.fetch %outputs_0 : tensor<?x?x?x?xf32>
    }
    return %0 : tensor<?x?x?x?xf32>
  }
  func.func private @__inference_max_pooling2d_layer_call_fn_4540(%arg0: tensor<?x?x?x?xf32> {tf._user_specified_name = "inputs"}) -> tensor<?x?x?x?xf32> attributes {tf._construction_context = "kEagerRuntime", tf._input_shapes = [#tf_type.shape<?x?x?x?>]} {
    %0 = tf_executor.graph {
      %outputs, %control = tf_executor.island wraps "tf.PartitionedCall"(%arg0) {_collective_manager_ids = [], _read_only_resource_inputs = [], config = "", config_proto = "\0A\07\0A\03CPU\10\01\0A\07\0A\03GPU\10\002\02J\008\01\82\01\00", device = "", executor_type = "", f = @__inference_max_pooling2d_layer_call_and_return_conditional_losses_2270} : (tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>
      %outputs_0, %control_1 = tf_executor.island wraps "tf.Identity"(%outputs) {device = ""} : (tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>
      tf_executor.fetch %outputs_0 : tensor<?x?x?x?xf32>
    }
    return %0 : tensor<?x?x?x?xf32>
  }
  func.func private @__inference_sequential_layer_call_and_return_conditional_losses_3190(%arg0: tensor<?x28x28x3xf32> {tf._user_specified_name = "conv2d_input"}, %arg1: tensor<!tf_type.resource> {tf._user_specified_name = "256"}, %arg2: tensor<!tf_type.resource> {tf._user_specified_name = "258"}, %arg3: tensor<!tf_type.resource> {tf._user_specified_name = "273"}, %arg4: tensor<!tf_type.resource> {tf._user_specified_name = "275"}, %arg5: tensor<!tf_type.resource> {tf._user_specified_name = "297"}, %arg6: tensor<!tf_type.resource> {tf._user_specified_name = "299"}, %arg7: tensor<!tf_type.resource> {tf._user_specified_name = "313"}, %arg8: tensor<!tf_type.resource> {tf._user_specified_name = "315"}) -> tensor<?x?xf32> attributes {tf._construction_context = "kEagerRuntime", tf._input_shapes = [#tf_type.shape<?x28x28x3>, #tf_type.shape<>, #tf_type.shape<>, #tf_type.shape<>, #tf_type.shape<>, #tf_type.shape<>, #tf_type.shape<>, #tf_type.shape<>, #tf_type.shape<>], tf.signature.is_stateful} {
    %0 = tf_executor.graph {
      %outputs, %control = tf_executor.island wraps "tf.StatefulPartitionedCall"(%arg0, %arg1, %arg2) {_collective_manager_ids = [], _read_only_resource_inputs = [1, 2], config = "", config_proto = "\0A\07\0A\03CPU\10\01\0A\07\0A\03GPU\10\002\02J\008\01\82\01\00", device = "", executor_type = "", f = @__inference_conv2d_layer_call_and_return_conditional_losses_2550} : (tensor<?x28x28x3xf32>, tensor<!tf_type.resource>, tensor<!tf_type.resource>) -> tensor<?x?x?x?xf32>
      %outputs_0, %control_1 = tf_executor.island wraps "tf.PartitionedCall"(%outputs) {_collective_manager_ids = [], _read_only_resource_inputs = [], config = "", config_proto = "\0A\07\0A\03CPU\10\01\0A\07\0A\03GPU\10\002\02J\008\01\82\01\00", device = "", executor_type = "", f = @__inference_max_pooling2d_layer_call_and_return_conditional_losses_2270} : (tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>
      %outputs_2, %control_3 = tf_executor.island wraps "tf.StatefulPartitionedCall"(%outputs_0, %arg3, %arg4) {_collective_manager_ids = [], _read_only_resource_inputs = [1, 2], config = "", config_proto = "\0A\07\0A\03CPU\10\01\0A\07\0A\03GPU\10\002\02J\008\01\82\01\00", device = "", executor_type = "", f = @__inference_conv2d_1_layer_call_and_return_conditional_losses_2720} : (tensor<?x?x?x?xf32>, tensor<!tf_type.resource>, tensor<!tf_type.resource>) -> tensor<?x?x?x?xf32>
      %outputs_4, %control_5 = tf_executor.island wraps "tf.PartitionedCall"(%outputs_2) {_collective_manager_ids = [], _read_only_resource_inputs = [], config = "", config_proto = "\0A\07\0A\03CPU\10\01\0A\07\0A\03GPU\10\002\02J\008\01\82\01\00", device = "", executor_type = "", f = @__inference_max_pooling2d_1_layer_call_and_return_conditional_losses_2370} : (tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>
      %outputs_6, %control_7 = tf_executor.island wraps "tf.PartitionedCall"(%outputs_4) {_collective_manager_ids = [], _read_only_resource_inputs = [], config = "", config_proto = "\0A\07\0A\03CPU\10\01\0A\07\0A\03GPU\10\002\02J\008\01\82\01\00", device = "", executor_type = "", f = @__inference_flatten_layer_call_and_return_conditional_losses_2840} : (tensor<?x?x?x?xf32>) -> tensor<?x75xf32>
      %outputs_8, %control_9 = tf_executor.island wraps "tf.StatefulPartitionedCall"(%outputs_6, %arg5, %arg6) {_collective_manager_ids = [], _read_only_resource_inputs = [1, 2], config = "", config_proto = "\0A\07\0A\03CPU\10\01\0A\07\0A\03GPU\10\002\02J\008\01\82\01\00", device = "", executor_type = "", f = @__inference_dense_layer_call_and_return_conditional_losses_2960} : (tensor<?x75xf32>, tensor<!tf_type.resource>, tensor<!tf_type.resource>) -> tensor<?x?xf32>
      %outputs_10, %control_11 = tf_executor.island wraps "tf.StatefulPartitionedCall"(%outputs_8, %arg7, %arg8) {_collective_manager_ids = [], _read_only_resource_inputs = [1, 2], config = "", config_proto = "\0A\07\0A\03CPU\10\01\0A\07\0A\03GPU\10\002\02J\008\01\82\01\00", device = "", executor_type = "", f = @__inference_dense_1_layer_call_and_return_conditional_losses_3120} : (tensor<?x?xf32>, tensor<!tf_type.resource>, tensor<!tf_type.resource>) -> tensor<?x?xf32>
      %control_12 = tf_executor.island(%control, %control_3, %control_9, %control_11) wraps "tf.NoOp"() {device = ""} : () -> ()
      %outputs_13, %control_14 = tf_executor.island(%control_12) wraps "tf.Identity"(%outputs_10) {device = ""} : (tensor<?x?xf32>) -> tensor<?x?xf32>
      tf_executor.fetch %outputs_13, %control, %control_3, %control_9, %control_11 : tensor<?x?xf32>, !tf_executor.control, !tf_executor.control, !tf_executor.control, !tf_executor.control
    }
    return %0 : tensor<?x?xf32>
  }
  func.func private @__inference_sequential_layer_call_and_return_conditional_losses_3460(%arg0: tensor<?x28x28x3xf32> {tf._user_specified_name = "conv2d_input"}, %arg1: tensor<!tf_type.resource> {tf._user_specified_name = "322"}, %arg2: tensor<!tf_type.resource> {tf._user_specified_name = "324"}, %arg3: tensor<!tf_type.resource> {tf._user_specified_name = "328"}, %arg4: tensor<!tf_type.resource> {tf._user_specified_name = "330"}, %arg5: tensor<!tf_type.resource> {tf._user_specified_name = "335"}, %arg6: tensor<!tf_type.resource> {tf._user_specified_name = "337"}, %arg7: tensor<!tf_type.resource> {tf._user_specified_name = "340"}, %arg8: tensor<!tf_type.resource> {tf._user_specified_name = "342"}) -> tensor<?x?xf32> attributes {tf._construction_context = "kEagerRuntime", tf._input_shapes = [#tf_type.shape<?x28x28x3>, #tf_type.shape<>, #tf_type.shape<>, #tf_type.shape<>, #tf_type.shape<>, #tf_type.shape<>, #tf_type.shape<>, #tf_type.shape<>, #tf_type.shape<>], tf.signature.is_stateful} {
    %0 = tf_executor.graph {
      %outputs, %control = tf_executor.island wraps "tf.StatefulPartitionedCall"(%arg0, %arg1, %arg2) {_collective_manager_ids = [], _read_only_resource_inputs = [1, 2], config = "", config_proto = "\0A\07\0A\03CPU\10\01\0A\07\0A\03GPU\10\002\02J\008\01\82\01\00", device = "", executor_type = "", f = @__inference_conv2d_layer_call_and_return_conditional_losses_2550} : (tensor<?x28x28x3xf32>, tensor<!tf_type.resource>, tensor<!tf_type.resource>) -> tensor<?x?x?x?xf32>
      %outputs_0, %control_1 = tf_executor.island wraps "tf.PartitionedCall"(%outputs) {_collective_manager_ids = [], _read_only_resource_inputs = [], config = "", config_proto = "\0A\07\0A\03CPU\10\01\0A\07\0A\03GPU\10\002\02J\008\01\82\01\00", device = "", executor_type = "", f = @__inference_max_pooling2d_layer_call_and_return_conditional_losses_2270} : (tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>
      %outputs_2, %control_3 = tf_executor.island wraps "tf.StatefulPartitionedCall"(%outputs_0, %arg3, %arg4) {_collective_manager_ids = [], _read_only_resource_inputs = [1, 2], config = "", config_proto = "\0A\07\0A\03CPU\10\01\0A\07\0A\03GPU\10\002\02J\008\01\82\01\00", device = "", executor_type = "", f = @__inference_conv2d_1_layer_call_and_return_conditional_losses_2720} : (tensor<?x?x?x?xf32>, tensor<!tf_type.resource>, tensor<!tf_type.resource>) -> tensor<?x?x?x?xf32>
      %outputs_4, %control_5 = tf_executor.island wraps "tf.PartitionedCall"(%outputs_2) {_collective_manager_ids = [], _read_only_resource_inputs = [], config = "", config_proto = "\0A\07\0A\03CPU\10\01\0A\07\0A\03GPU\10\002\02J\008\01\82\01\00", device = "", executor_type = "", f = @__inference_max_pooling2d_1_layer_call_and_return_conditional_losses_2370} : (tensor<?x?x?x?xf32>) -> tensor<?x?x?x?xf32>
      %outputs_6, %control_7 = tf_executor.island wraps "tf.PartitionedCall"(%outputs_4) {_collective_manager_ids = [], _read_only_resource_inputs = [], config = "", config_proto = "\0A\07\0A\03CPU\10\01\0A\07\0A\03GPU\10\002\02J\008\01\82\01\00", device = "", executor_type = "", f = @__inference_flatten_layer_call_and_return_conditional_losses_2840} : (tensor<?x?x?x?xf32>) -> tensor<?x75xf32>
      %outputs_8, %control_9 = tf_executor.island wraps "tf.StatefulPartitionedCall"(%outputs_6, %arg5, %arg6) {_collective_manager_ids = [], _read_only_resource_inputs = [1, 2], config = "", config_proto = "\0A\07\0A\03CPU\10\01\0A\07\0A\03GPU\10\002\02J\008\01\82\01\00", device = "", executor_type = "", f = @__inference_dense_layer_call_and_return_conditional_losses_2960} : (tensor<?x75xf32>, tensor<!tf_type.resource>, tensor<!tf_type.resource>) -> tensor<?x?xf32>
      %outputs_10, %control_11 = tf_executor.island wraps "tf.StatefulPartitionedCall"(%outputs_8, %arg7, %arg8) {_collective_manager_ids = [], _read_only_resource_inputs = [1, 2], config = "", config_proto = "\0A\07\0A\03CPU\10\01\0A\07\0A\03GPU\10\002\02J\008\01\82\01\00", device = "", executor_type = "", f = @__inference_dense_1_layer_call_and_return_conditional_losses_3120} : (tensor<?x?xf32>, tensor<!tf_type.resource>, tensor<!tf_type.resource>) -> tensor<?x?xf32>
      %control_12 = tf_executor.island(%control, %control_3, %control_9, %control_11) wraps "tf.NoOp"() {device = ""} : () -> ()
      %outputs_13, %control_14 = tf_executor.island(%control_12) wraps "tf.Identity"(%outputs_10) {device = ""} : (tensor<?x?xf32>) -> tensor<?x?xf32>
      tf_executor.fetch %outputs_13, %control, %control_3, %control_9, %control_11 : tensor<?x?xf32>, !tf_executor.control, !tf_executor.control, !tf_executor.control, !tf_executor.control
    }
    return %0 : tensor<?x?xf32>
  }
  func.func private @__inference_sequential_layer_call_fn_3670(%arg0: tensor<?x28x28x3xf32> {tf._user_specified_name = "conv2d_input"}, %arg1: tensor<!tf_type.resource> {tf._user_specified_name = "349"}, %arg2: tensor<!tf_type.resource> {tf._user_specified_name = "351"}, %arg3: tensor<!tf_type.resource> {tf._user_specified_name = "353"}, %arg4: tensor<!tf_type.resource> {tf._user_specified_name = "355"}, %arg5: tensor<!tf_type.resource> {tf._user_specified_name = "357"}, %arg6: tensor<!tf_type.resource> {tf._user_specified_name = "359"}, %arg7: tensor<!tf_type.resource> {tf._user_specified_name = "361"}, %arg8: tensor<!tf_type.resource> {tf._user_specified_name = "363"}) -> tensor<?x?xf32> attributes {tf._construction_context = "kEagerRuntime", tf._input_shapes = [#tf_type.shape<?x28x28x3>, #tf_type.shape<>, #tf_type.shape<>, #tf_type.shape<>, #tf_type.shape<>, #tf_type.shape<>, #tf_type.shape<>, #tf_type.shape<>, #tf_type.shape<>], tf.signature.is_stateful} {
    %0 = tf_executor.graph {
      %outputs, %control = tf_executor.island wraps "tf.StatefulPartitionedCall"(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8) {_collective_manager_ids = [], _read_only_resource_inputs = [1, 2, 3, 4, 5, 6, 7, 8], config = "", config_proto = "\0A\07\0A\03CPU\10\01\0A\07\0A\03GPU\10\002\02J\008\01\82\01\00", device = "", executor_type = "", f = @__inference_sequential_layer_call_and_return_conditional_losses_3190} : (tensor<?x28x28x3xf32>, tensor<!tf_type.resource>, tensor<!tf_type.resource>, tensor<!tf_type.resource>, tensor<!tf_type.resource>, tensor<!tf_type.resource>, tensor<!tf_type.resource>, tensor<!tf_type.resource>, tensor<!tf_type.resource>) -> tensor<?x?xf32>
      %control_0 = tf_executor.island(%control) wraps "tf.NoOp"() {device = ""} : () -> ()
      %outputs_1, %control_2 = tf_executor.island(%control_0) wraps "tf.Identity"(%outputs) {device = ""} : (tensor<?x?xf32>) -> tensor<?x?xf32>
      tf_executor.fetch %outputs_1, %control : tensor<?x?xf32>, !tf_executor.control
    }
    return %0 : tensor<?x?xf32>
  }
  func.func private @__inference_sequential_layer_call_fn_3880(%arg0: tensor<?x28x28x3xf32> {tf._user_specified_name = "conv2d_input"}, %arg1: tensor<!tf_type.resource> {tf._user_specified_name = "370"}, %arg2: tensor<!tf_type.resource> {tf._user_specified_name = "372"}, %arg3: tensor<!tf_type.resource> {tf._user_specified_name = "374"}, %arg4: tensor<!tf_type.resource> {tf._user_specified_name = "376"}, %arg5: tensor<!tf_type.resource> {tf._user_specified_name = "378"}, %arg6: tensor<!tf_type.resource> {tf._user_specified_name = "380"}, %arg7: tensor<!tf_type.resource> {tf._user_specified_name = "382"}, %arg8: tensor<!tf_type.resource> {tf._user_specified_name = "384"}) -> tensor<?x?xf32> attributes {tf._construction_context = "kEagerRuntime", tf._input_shapes = [#tf_type.shape<?x28x28x3>, #tf_type.shape<>, #tf_type.shape<>, #tf_type.shape<>, #tf_type.shape<>, #tf_type.shape<>, #tf_type.shape<>, #tf_type.shape<>, #tf_type.shape<>], tf.signature.is_stateful} {
    %0 = tf_executor.graph {
      %outputs, %control = tf_executor.island wraps "tf.StatefulPartitionedCall"(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8) {_collective_manager_ids = [], _read_only_resource_inputs = [1, 2, 3, 4, 5, 6, 7, 8], config = "", config_proto = "\0A\07\0A\03CPU\10\01\0A\07\0A\03GPU\10\002\02J\008\01\82\01\00", device = "", executor_type = "", f = @__inference_sequential_layer_call_and_return_conditional_losses_3460} : (tensor<?x28x28x3xf32>, tensor<!tf_type.resource>, tensor<!tf_type.resource>, tensor<!tf_type.resource>, tensor<!tf_type.resource>, tensor<!tf_type.resource>, tensor<!tf_type.resource>, tensor<!tf_type.resource>, tensor<!tf_type.resource>) -> tensor<?x?xf32>
      %control_0 = tf_executor.island(%control) wraps "tf.NoOp"() {device = ""} : () -> ()
      %outputs_1, %control_2 = tf_executor.island(%control_0) wraps "tf.Identity"(%outputs) {device = ""} : (tensor<?x?xf32>) -> tensor<?x?xf32>
      tf_executor.fetch %outputs_1, %control : tensor<?x?xf32>, !tf_executor.control
    }
    return %0 : tensor<?x?xf32>
  }
}
